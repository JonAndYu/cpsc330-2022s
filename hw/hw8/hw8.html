
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>CPSC 330 - Applied Machine Learning &#8212; CPSC 330 Applied Machine Learning</title>
    
  <link href="../../_static/css/theme.css" rel="stylesheet">
  <link href="../../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../../_static/UBC-CS-logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">CPSC 330 Applied Machine Learning</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Things you should know
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../docs/README.html">
   CPSC 330 Documents
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Lectures
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../lectures/01_intro.html">
   Lecture 1: Course Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../lectures/02_decision-trees.html">
   Lecture 2: Terminology, Baselines, Decision Trees
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../lectures/03_ml-fundamentals.html">
   Lecture 3: Machine Learning Fundamentals
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../lectures/04_kNNs-SVM-RBF.html">
   Lecture 4:
   <span class="math notranslate nohighlight">
    \(k\)
   </span>
   -Nearest Neighbours and SVM RBFs
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../lectures/05_preprocessing-pipelines.html">
   Lecture 5: Preprocessing and
   <code class="docutils literal notranslate">
    <span class="pre">
     sklearn
    </span>
   </code>
   pipelines
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../lectures/06_column-transformer-text-feats.html">
   Lecture 6:
   <code class="docutils literal notranslate">
    <span class="pre">
     sklearn
    </span>
   </code>
   <code class="docutils literal notranslate">
    <span class="pre">
     ColumnTransformer
    </span>
   </code>
   and Text Features
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../lectures/07_linear-models.html">
   Lecture 7: Linear Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../lectures/08_hyperparameter-optimization.html">
   Lecture 8: Hyperparameter Optimization and Optimization Bias
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../lectures/09_classification-metrics.html">
   Lecture 9: Classification Metrics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../lectures/10_regression-metrics.html">
   Lecture 10: Regression Evaluation Metrics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../lectures/11_ensembles.html">
   Lecture 11: Ensembles
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../lectures/12_feat-importances.html">
   Lecture 12: Feature importances
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../lectures/13_feature-engineering-selection.html">
   Lecture 13: Feature engineering and feature selection
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../lectures/14_k-means-clustering.html">
   Lecture 14: Clustering
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../lectures/15_recommender-systems.html">
   Lecture 15: DBSCAN and Recommender Systems
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../lectures/16_natural-language-processing.html">
   Lecture 16: Introduction to natural language processing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../lectures/17_intro_to_computer-vision.html">
   Lecture 17: Multi-class classification and introduction to computer vision
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../lectures/18_time-series.html">
   Lecture 18: Time series
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../lectures/19_survival-analysis.html">
   Lecture 19: Survival analysis
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../lectures/20_ethics.html">
   Lecture 20: Ethics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../lectures/21_communication.html">
   Lecture 21: Communication
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../lectures/22_deployment-conclusion.html">
   Lecture 22: Deployment and conclusion
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Attribution
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../attribution.html">
   Attributions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../LICENSE.html">
   LICENSE
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Varada Kolhatkar, CPSC 330 2021-22<br>Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../_sources/hw/hw8/hw8.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/UBC-CS/cpsc330/master?urlpath=tree/hw/hw8/hw8.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#homework-8-word-embeddings-time-series-and-communication">
   Homework 8: Word embeddings, time series, and communication
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#associated-lectures-lectures-16-18-19-and-ml-communication">
     Associated lectures: Lectures 16, 18, 19, and ML communication
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#table-of-contents">
   Table of Contents
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#instructions">
   Instructions
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#exercise-1-exploring-pre-trained-word-embeddings-a-name-1-a">
   Exercise 1:  Exploring pre-trained word embeddings
   <a name="1">
   </a>
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#word-similarity-using-pre-trained-embeddings">
     1.1 Word similarity using pre-trained embeddings
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#bias-in-embeddings">
     1.2 Bias in embeddings
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#representation-of-all-words-in-english">
     1.3 Representation of all words in English
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#classification-with-pre-trained-embeddings">
     1.4 Classification with pre-trained embeddings
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#exercise-2-exploring-time-series-data-a-name-2-a">
   Exercise 2: Exploring time series data
   <a name="2">
   </a>
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id1">
     2.1
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id2">
     2.2
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id3">
     2.3
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#baseline">
     2.4 Baseline
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#optional-2-5-modeling">
     (Optional) 2.5 Modeling
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#exercise-3-short-answer-questions-a-name-3-a">
   Exercise 3: Short answer questions
   <a name="3">
   </a>
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id4">
     3.1
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id5">
     3.2
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#exercise-4-communication-a-name-4-a">
   Exercise 4: Communication
   <a name="4">
   </a>
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#blog-post">
     4.1 Blog post
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#example-blog-posts">
       Example blog posts
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#a-note-on-plagiarism">
       A note on plagiarism
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id6">
     4.2
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#optional-not-for-marks-4-3">
   (optional, not for marks) 4.3
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#optional-exercise-5-a-name-5-a">
     (Optional) Exercise 5
     <a name="5">
     </a>
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#submission-instructions">
   Submission instructions
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#congratulations-on-finishing-all-homework-assignments-clap-clap">
     Congratulations on finishing all homework assignments! :clap: :clap:
    </a>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>CPSC 330 - Applied Machine Learning</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#homework-8-word-embeddings-time-series-and-communication">
   Homework 8: Word embeddings, time series, and communication
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#associated-lectures-lectures-16-18-19-and-ml-communication">
     Associated lectures: Lectures 16, 18, 19, and ML communication
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#table-of-contents">
   Table of Contents
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#instructions">
   Instructions
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#exercise-1-exploring-pre-trained-word-embeddings-a-name-1-a">
   Exercise 1:  Exploring pre-trained word embeddings
   <a name="1">
   </a>
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#word-similarity-using-pre-trained-embeddings">
     1.1 Word similarity using pre-trained embeddings
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#bias-in-embeddings">
     1.2 Bias in embeddings
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#representation-of-all-words-in-english">
     1.3 Representation of all words in English
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#classification-with-pre-trained-embeddings">
     1.4 Classification with pre-trained embeddings
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#exercise-2-exploring-time-series-data-a-name-2-a">
   Exercise 2: Exploring time series data
   <a name="2">
   </a>
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id1">
     2.1
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id2">
     2.2
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id3">
     2.3
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#baseline">
     2.4 Baseline
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#optional-2-5-modeling">
     (Optional) 2.5 Modeling
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#exercise-3-short-answer-questions-a-name-3-a">
   Exercise 3: Short answer questions
   <a name="3">
   </a>
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id4">
     3.1
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id5">
     3.2
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#exercise-4-communication-a-name-4-a">
   Exercise 4: Communication
   <a name="4">
   </a>
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#blog-post">
     4.1 Blog post
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#example-blog-posts">
       Example blog posts
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#a-note-on-plagiarism">
       A note on plagiarism
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id6">
     4.2
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#optional-not-for-marks-4-3">
   (optional, not for marks) 4.3
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#optional-exercise-5-a-name-5-a">
     (Optional) Exercise 5
     <a name="5">
     </a>
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#submission-instructions">
   Submission instructions
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#congratulations-on-finishing-all-homework-assignments-clap-clap">
     Congratulations on finishing all homework assignments! :clap: :clap:
    </a>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="cpsc-330-applied-machine-learning">
<h1>CPSC 330 - Applied Machine Learning<a class="headerlink" href="#cpsc-330-applied-machine-learning" title="Permalink to this headline">¶</a></h1>
<div class="section" id="homework-8-word-embeddings-time-series-and-communication">
<h2>Homework 8: Word embeddings, time series, and communication<a class="headerlink" href="#homework-8-word-embeddings-time-series-and-communication" title="Permalink to this headline">¶</a></h2>
<div class="section" id="associated-lectures-lectures-16-18-19-and-ml-communication">
<h3>Associated lectures: Lectures 16, 18, 19, and ML communication<a class="headerlink" href="#associated-lectures-lectures-16-18-19-and-ml-communication" title="Permalink to this headline">¶</a></h3>
<p><strong>Due date: December 07, 2021 at 11:59pm</strong></p>
</div>
</div>
<div class="section" id="table-of-contents">
<h2>Table of Contents<a class="headerlink" href="#table-of-contents" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p><a class="reference external" href="#sg">Submission instructions</a> (4%)</p></li>
<li><p><a class="reference external" href="#1">Exercise 1 - Exploring pre-trained word embeddings</a> (24%)</p></li>
<li><p><a class="reference external" href="#2">Exercise 2 - Exploring time series data</a> (16%)</p></li>
<li><p><a class="reference external" href="#4">Exercise 3 - Short answer questions</a> (10%)</p></li>
<li><p><a class="reference external" href="#4">Exercise 4 - Communication</a> (46%)</p></li>
<li><p>(Optional)<a class="reference external" href="#5">Exercise 5 - Course take away</a></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>

<span class="o">%</span><span class="k">matplotlib</span> inline
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">from</span> <span class="nn">sklearn.cluster</span> <span class="kn">import</span> <span class="n">DBSCAN</span><span class="p">,</span> <span class="n">KMeans</span>
<span class="kn">from</span> <span class="nn">sklearn.compose</span> <span class="kn">import</span> <span class="n">ColumnTransformer</span><span class="p">,</span> <span class="n">make_column_transformer</span>
<span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">CountVectorizer</span>
<span class="kn">from</span> <span class="nn">sklearn.impute</span> <span class="kn">import</span> <span class="n">SimpleImputer</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">r2_score</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">GridSearchCV</span><span class="p">,</span>
    <span class="n">RandomizedSearchCV</span><span class="p">,</span>
    <span class="n">cross_validate</span><span class="p">,</span>
    <span class="n">train_test_split</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">Pipeline</span><span class="p">,</span> <span class="n">make_pipeline</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">OneHotEncoder</span><span class="p">,</span> <span class="n">OrdinalEncoder</span><span class="p">,</span> <span class="n">StandardScaler</span>

<span class="n">pd</span><span class="o">.</span><span class="n">set_option</span><span class="p">(</span><span class="s2">&quot;display.max_colwidth&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p><br><br><br><br></p>
</div>
<div class="section" id="instructions">
<h2>Instructions<a class="headerlink" href="#instructions" title="Permalink to this headline">¶</a></h2>
<hr>
rubric={points:4}
<p>Follow the <a class="reference external" href="https://github.com/UBC-CS/cpsc330/blob/master/docs/homework_instructions.md">homework submission instructions</a>.</p>
<p><strong>You may work on this homework in a group and submit your assignment as a group.</strong> Below are some instructions on working as a group.</p>
<ul class="simple">
<li><p>The maximum group size is 3.</p></li>
<li><p>Use group work as an opportunity to collaborate and learn new things from each other.</p></li>
<li><p>Be respectful to each other and make sure you understand all the concepts in the assignment well.</p></li>
<li><p>It’s your responsibility to make sure that the assignment is submitted by one of the group members before the deadline.</p></li>
<li><p>You can find the instructions on how to do group submission on Gradescope <a class="reference external" href="https://help.gradescope.com/article/m5qz2xsnjy-student-add-group-members">here</a>.</p></li>
</ul>
<p><br><br><br><br></p>
</div>
<div class="section" id="exercise-1-exploring-pre-trained-word-embeddings-a-name-1-a">
<h2>Exercise 1:  Exploring pre-trained word embeddings <a name="1"></a><a class="headerlink" href="#exercise-1-exploring-pre-trained-word-embeddings-a-name-1-a" title="Permalink to this headline">¶</a></h2>
<hr>
<p>In lecture 16, we talked about natural language processing (NLP). Using pre-trained word embeddings is very common in NLP. It has been shown that pre-trained word embeddings <a class="reference external" href="http://www.lrec-conf.org/proceedings/lrec2018/pdf/721.pdf">work well on a variety of text classification tasks</a>. These embeddings are created by training a model like Word2Vec on a huge corpus of text such as a dump of Wikipedia or a dump of the web crawl.</p>
<p>A number of pre-trained word embeddings are available out there. Some popular ones are:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://nlp.stanford.edu/projects/glove/">GloVe</a></p>
<ul>
<li><p>trained using <a class="reference external" href="https://nlp.stanford.edu/pubs/glove.pdf">the GloVe algorithm</a></p></li>
<li><p>published by Stanford University</p></li>
</ul>
</li>
<li><p><a class="reference external" href="https://fasttext.cc/docs/en/pretrained-vectors.html">fastText pre-trained embeddings for 294 languages</a></p>
<ul>
<li><p>trained using the fastText algorithm</p></li>
<li><p>published by Facebook</p></li>
</ul>
</li>
</ul>
<p>In this exercise, you will be exploring GloVe Wikipedia pre-trained embeddings. The code below loads pre-trained word vectors trained on Wikipedia. (The vectors are created using an algorithm called GloVe.) To run the code, you’ll need <code class="docutils literal notranslate"><span class="pre">gensim</span></code> package for that in your cpsc330 conda environment, which you can install as follows.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">&gt;</span> <span class="n">conda</span> <span class="n">activate</span> <span class="n">cpsc330</span>
<span class="o">&gt;</span> <span class="n">conda</span> <span class="n">install</span> <span class="o">-</span><span class="n">c</span> <span class="n">anaconda</span> <span class="n">gensim</span>
</pre></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">gensim</span>
<span class="kn">import</span> <span class="nn">gensim.downloader</span>

<span class="nb">print</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">gensim</span><span class="o">.</span><span class="n">downloader</span><span class="o">.</span><span class="n">info</span><span class="p">()[</span><span class="s2">&quot;models&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">keys</span><span class="p">()))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/Users/kvarada/opt/miniconda3/envs/cpsc330/lib/python3.9/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package &lt;https://pypi.org/project/python-Levenshtein/&gt; is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.
  warnings.warn(msg)
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;fasttext-wiki-news-subwords-300&#39;, &#39;conceptnet-numberbatch-17-06-300&#39;, &#39;word2vec-ruscorpora-300&#39;, &#39;word2vec-google-news-300&#39;, &#39;glove-wiki-gigaword-50&#39;, &#39;glove-wiki-gigaword-100&#39;, &#39;glove-wiki-gigaword-200&#39;, &#39;glove-wiki-gigaword-300&#39;, &#39;glove-twitter-25&#39;, &#39;glove-twitter-50&#39;, &#39;glove-twitter-100&#39;, &#39;glove-twitter-200&#39;, &#39;__testing_word2vec-matrix-synopsis&#39;]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># This will take a while to run when you run it for the first time.</span>
<span class="kn">import</span> <span class="nn">gensim.downloader</span> <span class="k">as</span> <span class="nn">api</span>

<span class="n">glove_wiki_vectors</span> <span class="o">=</span> <span class="n">api</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;glove-wiki-gigaword-100&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">KeyboardInterrupt</span><span class="g g-Whitespace">                         </span>Traceback (most recent call last)
<span class="o">/</span><span class="n">var</span><span class="o">/</span><span class="n">folders</span><span class="o">/</span><span class="n">ky</span><span class="o">/</span><span class="mi">533</span><span class="n">nd9l512l5cmmk_kzmzj9m0000gp</span><span class="o">/</span><span class="n">T</span><span class="o">/</span><span class="n">ipykernel_96685</span><span class="o">/</span><span class="mf">1520580851.</span><span class="n">py</span> <span class="ow">in</span> <span class="o">&lt;</span><span class="n">module</span><span class="o">&gt;</span>
<span class="g g-Whitespace">      </span><span class="mi">2</span> <span class="kn">import</span> <span class="nn">gensim.downloader</span> <span class="k">as</span> <span class="nn">api</span>
<span class="g g-Whitespace">      </span><span class="mi">3</span> 
<span class="ne">----&gt; </span><span class="mi">4</span> <span class="n">glove_wiki_vectors</span> <span class="o">=</span> <span class="n">api</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;glove-wiki-gigaword-100&quot;</span><span class="p">)</span>

<span class="nn">~/opt/miniconda3/envs/cpsc330/lib/python3.9/site-packages/gensim/downloader.py</span> in <span class="ni">load</span><span class="nt">(name, return_path)</span>
<span class="g g-Whitespace">    </span><span class="mi">501</span>         <span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">BASE_DIR</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">502</span>         <span class="n">module</span> <span class="o">=</span> <span class="nb">__import__</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
<span class="ne">--&gt; </span><span class="mi">503</span>         <span class="k">return</span> <span class="n">module</span><span class="o">.</span><span class="n">load_data</span><span class="p">()</span>
<span class="g g-Whitespace">    </span><span class="mi">504</span> 
<span class="g g-Whitespace">    </span><span class="mi">505</span> 

<span class="nn">~/gensim-data/glove-wiki-gigaword-100/__init__.py</span> in <span class="ni">load_data</span><span class="nt">()</span>
<span class="g g-Whitespace">      </span><span class="mi">6</span> <span class="k">def</span> <span class="nf">load_data</span><span class="p">():</span>
<span class="g g-Whitespace">      </span><span class="mi">7</span>     <span class="n">path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">base_dir</span><span class="p">,</span> <span class="s1">&#39;glove-wiki-gigaword-100&#39;</span><span class="p">,</span> <span class="s1">&#39;glove-wiki-gigaword-100.gz&#39;</span><span class="p">)</span>
<span class="ne">----&gt; </span><span class="mi">8</span>     <span class="n">model</span> <span class="o">=</span> <span class="n">KeyedVectors</span><span class="o">.</span><span class="n">load_word2vec_format</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>
<span class="g g-Whitespace">      </span><span class="mi">9</span>     <span class="k">return</span> <span class="n">model</span>

<span class="nn">~/opt/miniconda3/envs/cpsc330/lib/python3.9/site-packages/gensim/models/keyedvectors.py</span> in <span class="ni">load_word2vec_format</span><span class="nt">(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header)</span>
<span class="g g-Whitespace">   </span><span class="mi">1628</span> 
<span class="g g-Whitespace">   </span><span class="mi">1629</span>         <span class="s2">&quot;&quot;&quot;</span>
<span class="ne">-&gt; </span><span class="mi">1630</span><span class="s2">         return _load_word2vec_format(</span>
<span class="g g-Whitespace">   </span><span class="mi">1631</span><span class="s2">             cls, fname, fvocab=fvocab, binary=binary, encoding=encoding, unicode_errors=unicode_errors,</span>
<span class="g g-Whitespace">   </span><span class="mi">1632</span><span class="s2">             limit=limit, datatype=datatype, no_header=no_header,</span>

<span class="nn">~/opt/miniconda3/envs/cpsc330/lib/python3.9/site-packages/gensim/models/keyedvectors.py</span> in <span class="ni">_load_word2vec_format</span><span class="nt">(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header, binary_chunk_size)</span>
<span class="g g-Whitespace">   </span><span class="mi">1911</span><span class="s2">             )</span>
<span class="g g-Whitespace">   </span><span class="mi">1912</span><span class="s2">         else:</span>
<span class="ne">-&gt; </span><span class="mi">1913</span><span class="s2">             _word2vec_read_text(fin, kv, counts, vocab_size, vector_size, datatype, unicode_errors, encoding)</span>
<span class="g g-Whitespace">   </span><span class="mi">1914</span><span class="s2">     if kv.vectors.shape[0] != len(kv):</span>
<span class="g g-Whitespace">   </span><span class="mi">1915</span><span class="s2">         logger.info(</span>

<span class="nn">~/opt/miniconda3/envs/cpsc330/lib/python3.9/site-packages/gensim/models/keyedvectors.py</span> in <span class="ni">_word2vec_read_text</span><span class="nt">(fin, kv, counts, vocab_size, vector_size, datatype, unicode_errors, encoding)</span>
<span class="g g-Whitespace">   </span><span class="mi">1813</span><span class="s2"> def _word2vec_read_text(fin, kv, counts, vocab_size, vector_size, datatype, unicode_errors, encoding):</span>
<span class="g g-Whitespace">   </span><span class="mi">1814</span><span class="s2">     for line_no in range(vocab_size):</span>
<span class="ne">-&gt; </span><span class="mi">1815</span><span class="s2">         line = fin.readline()</span>
<span class="g g-Whitespace">   </span><span class="mi">1816</span><span class="s2">         if line == b&#39;&#39;:</span>
<span class="g g-Whitespace">   </span><span class="mi">1817</span><span class="s2">             raise EOFError(&quot;unexpected end of input; is count incorrect or file otherwise damaged?&quot;)</span>

<span class="nn">~/opt/miniconda3/envs/cpsc330/lib/python3.9/gzip.py</span> in <span class="ni">readline</span><span class="nt">(self, size)</span>
<span class="g g-Whitespace">    </span><span class="mi">394</span><span class="s2">         return self.offset</span>
<span class="g g-Whitespace">    </span><span class="mi">395</span><span class="s2"> </span>
<span class="ne">--&gt; </span><span class="mi">396</span><span class="s2">     def readline(self, size=-1):</span>
<span class="g g-Whitespace">    </span><span class="mi">397</span><span class="s2">         self._check_not_closed()</span>
<span class="g g-Whitespace">    </span><span class="mi">398</span><span class="s2">         return self._buffer.readline(size)</span>

<span class="ne">KeyboardInterrupt</span>: 
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">len</span><span class="p">(</span><span class="n">glove_wiki_vectors</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>There are 400,000 word vectors in these pre-trained model.</p>
<p><br><br></p>
<div class="section" id="word-similarity-using-pre-trained-embeddings">
<h3>1.1 Word similarity using pre-trained embeddings<a class="headerlink" href="#word-similarity-using-pre-trained-embeddings" title="Permalink to this headline">¶</a></h3>
<p>rubric={points:4}</p>
<p>Now that we have GloVe Wiki vectors (<code class="docutils literal notranslate"><span class="pre">glove_wiki_vectors</span></code>) loaded, let’s explore the word vectors.</p>
<p><strong>Your tasks:</strong></p>
<ol class="simple">
<li><p>Calculate cosine similarity for the following word pairs (<code class="docutils literal notranslate"><span class="pre">word_pairs</span></code>) using the <a class="reference external" href="https://radimrehurek.com/gensim/models/keyedvectors.html?highlight=similarity#gensim.models.keyedvectors.KeyedVectors.similarity"><code class="docutils literal notranslate"><span class="pre">similarity</span></code></a> method of the model.</p></li>
<li><p>Do the similarities make sense?</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">word_pairs</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">(</span><span class="s2">&quot;coast&quot;</span><span class="p">,</span> <span class="s2">&quot;shore&quot;</span><span class="p">),</span>
    <span class="p">(</span><span class="s2">&quot;clothes&quot;</span><span class="p">,</span> <span class="s2">&quot;closet&quot;</span><span class="p">),</span>
    <span class="p">(</span><span class="s2">&quot;old&quot;</span><span class="p">,</span> <span class="s2">&quot;new&quot;</span><span class="p">),</span>
    <span class="p">(</span><span class="s2">&quot;smart&quot;</span><span class="p">,</span> <span class="s2">&quot;intelligent&quot;</span><span class="p">),</span>
    <span class="p">(</span><span class="s2">&quot;dog&quot;</span><span class="p">,</span> <span class="s2">&quot;cat&quot;</span><span class="p">),</span>
    <span class="p">(</span><span class="s2">&quot;tree&quot;</span><span class="p">,</span> <span class="s2">&quot;lawyer&quot;</span><span class="p">),</span>
<span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p><br><br></p>
</div>
<div class="section" id="bias-in-embeddings">
<h3>1.2 Bias in embeddings<a class="headerlink" href="#bias-in-embeddings" title="Permalink to this headline">¶</a></h3>
<p>rubric={points:10}</p>
<p><strong>Your tasks:</strong></p>
<ol class="simple">
<li><p>In Lecture 16 we saw that our pre-trained word embedding model output an analogy that reinforced a gender stereotype. Give an example of how using such a model could cause harm in the real world.</p></li>
<li><p>Here we are using pre-trained embeddings which are built using Wikipedia data. Explore whether there are any worrisome biases present in these embeddings or not by trying out some examples. You can use the following two methods or other methods of your choice to explore what kind of stereotypes and biases are encoded in these embeddings.</p>
<ul class="simple">
<li><p>You can use the <code class="docutils literal notranslate"><span class="pre">analogy</span></code> function below which gives words analogies.</p></li>
<li><p>You can also use <a class="reference external" href="https://radimrehurek.com/gensim/models/keyedvectors.html?highlight=similarity#gensim.models.keyedvectors.KeyedVectors.similarity">similarity</a> or <a class="reference external" href="https://radimrehurek.com/gensim/models/keyedvectors.html?highlight=distance#gensim.models.keyedvectors.KeyedVectors.distances">distance</a> methods. (An example is shown below.)</p></li>
</ul>
</li>
<li><p>Discuss your observations. Do you observe the gender stereotype we observed in class in these embeddings?</p></li>
</ol>
<blockquote>
<div><p>Note that most of the recent embeddings are de-biased. But you might still observe some biases in them. Also, not all stereotypes present in pre-trained embeddings are necessarily bad. But you should be aware of them when you use them in your models.</p>
</div></blockquote>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">analogy</span><span class="p">(</span><span class="n">word1</span><span class="p">,</span> <span class="n">word2</span><span class="p">,</span> <span class="n">word3</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">glove_wiki_vectors</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns analogy word using the given model.</span>

<span class="sd">    Parameters</span>
<span class="sd">    --------------</span>
<span class="sd">    word1 : (str)</span>
<span class="sd">        word1 in the analogy relation</span>
<span class="sd">    word2 : (str)</span>
<span class="sd">        word2 in the analogy relation</span>
<span class="sd">    word3 : (str)</span>
<span class="sd">        word3 in the analogy relation</span>
<span class="sd">    model :</span>
<span class="sd">        word embedding model</span>

<span class="sd">    Returns</span>
<span class="sd">    ---------------</span>
<span class="sd">        pd.dataframe</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">%s</span><span class="s2"> : </span><span class="si">%s</span><span class="s2"> :: </span><span class="si">%s</span><span class="s2"> : ?&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">word1</span><span class="p">,</span> <span class="n">word2</span><span class="p">,</span> <span class="n">word3</span><span class="p">))</span>
    <span class="n">sim_words</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="n">positive</span><span class="o">=</span><span class="p">[</span><span class="n">word3</span><span class="p">,</span> <span class="n">word2</span><span class="p">],</span> <span class="n">negative</span><span class="o">=</span><span class="p">[</span><span class="n">word1</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">sim_words</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;Analogy word&quot;</span><span class="p">,</span> <span class="s2">&quot;Score&quot;</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<p>An example of using similarity between words to explore biases and stereotypes.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">glove_wiki_vectors</span><span class="o">.</span><span class="n">similarity</span><span class="p">(</span><span class="s2">&quot;white&quot;</span><span class="p">,</span> <span class="s2">&quot;rich&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">glove_wiki_vectors</span><span class="o">.</span><span class="n">similarity</span><span class="p">(</span><span class="s2">&quot;black&quot;</span><span class="p">,</span> <span class="s2">&quot;rich&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p><br><br></p>
</div>
<div class="section" id="representation-of-all-words-in-english">
<h3>1.3 Representation of all words in English<a class="headerlink" href="#representation-of-all-words-in-english" title="Permalink to this headline">¶</a></h3>
<p>rubric={reasoning:2}</p>
<p><strong>Your tasks:</strong></p>
<ol class="simple">
<li><p>The vocabulary size of Wikipedia embeddings is quite large. Do you think it contains <strong>all</strong> words in English language? What would happen if you try to get a word vector that’s unlikely to be present in the vocabulary (e.g., the word “cpsc330”).</p></li>
</ol>
<p><br><br></p>
</div>
<div class="section" id="classification-with-pre-trained-embeddings">
<h3>1.4 Classification with pre-trained embeddings<a class="headerlink" href="#classification-with-pre-trained-embeddings" title="Permalink to this headline">¶</a></h3>
<p>rubric={points:8}</p>
<p>In lecture 16, we saw that you can conveniently get word vectors with <code class="docutils literal notranslate"><span class="pre">spaCy</span></code> with <code class="docutils literal notranslate"><span class="pre">en_core_web_md</span></code> model. In this exercise, you’ll use word embeddings in multi-class text classification task. We will use <a class="reference external" href="https://www.kaggle.com/ritresearch/happydb">HappyDB</a> corpus which contains about 100,000 happy moments classified into 7 categories: <em>affection, exercise, bonding, nature, leisure, achievement, enjoy_the_moment</em>. The data was crowd-sourced via <a class="reference external" href="https://www.mturk.com/">Amazon Mechanical Turk</a>. The ground truth label is not available for all examples, and in this lab, we’ll only use the examples where ground truth is available (~15,000 examples).</p>
<ul class="simple">
<li><p>Download the data from <a class="reference external" href="https://www.kaggle.com/ritresearch/happydb">here</a>.</p></li>
<li><p>Unzip the file and copy it in the lab directory.</p></li>
</ul>
<p>We will be using spaCy in this exercise. If you do not have spaCy in your course environment, here is how you can install it.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">&gt;</span> <span class="n">conda</span> <span class="n">activate</span> <span class="n">cpsc330</span>
<span class="o">&gt;</span> <span class="n">conda</span> <span class="n">install</span> <span class="o">-</span><span class="n">c</span> <span class="n">conda</span><span class="o">-</span><span class="n">forge</span> <span class="n">spacy</span>
</pre></div>
</div>
<ul class="simple">
<li><p>You also need to download the language model which contains all the pre-trained models. For that run the following in your course <code class="docutils literal notranslate"><span class="pre">conda</span></code> environment.</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">python</span> <span class="o">-</span><span class="n">m</span> <span class="n">spacy</span> <span class="n">download</span> <span class="n">en_core_web_md</span>
</pre></div>
</div>
<p>The code below reads the data CSV (assuming that it’s present in the current directory as <em>cleaned_hm.csv</em>),  cleans it up a bit, and splits it into train and test splits.</p>
<p><strong>Your tasks:</strong></p>
<ol class="simple">
<li><p>Train logistic regression with bag-of-words features and show classification report on the test set.</p></li>
<li><p>Train logistic regression with average embedding representation extracted using spaCy and show classification report on the test set. (You can find an example of extracting average embedding features using spaCy in <a class="reference external" href="https://ubc-cs.github.io/cpsc330/lectures/16_natural-language-processing.html#sentiment-classification-using-average-embeddings">lecture 16</a>.)</p></li>
<li><p>Discuss your results. Which model is performing well. Which model would be more interpretable?</p></li>
<li><p>Are you observing any benefits of transfer learning here? Briefly discuss.</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;cleaned_hm.csv&quot;</span><span class="p">,</span> <span class="n">index_col</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">sample_df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">dropna</span><span class="p">()</span>
<span class="n">sample_df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sample_df</span> <span class="o">=</span> <span class="n">sample_df</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span>
    <span class="n">columns</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;cleaned_hm&quot;</span><span class="p">:</span> <span class="s2">&quot;moment&quot;</span><span class="p">,</span> <span class="s2">&quot;ground_truth_category&quot;</span><span class="p">:</span> <span class="s2">&quot;target&quot;</span><span class="p">}</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">train_df</span><span class="p">,</span> <span class="n">test_df</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">sample_df</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span> <span class="o">=</span> <span class="n">train_df</span><span class="p">[</span><span class="s2">&quot;moment&quot;</span><span class="p">],</span> <span class="n">train_df</span><span class="p">[</span><span class="s2">&quot;target&quot;</span><span class="p">]</span>
<span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">test_df</span><span class="p">[</span><span class="s2">&quot;moment&quot;</span><span class="p">],</span> <span class="n">test_df</span><span class="p">[</span><span class="s2">&quot;target&quot;</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">spacy</span>

<span class="n">nlp</span> <span class="o">=</span> <span class="n">spacy</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;en_core_web_md&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p><br><br><br><br></p>
</div>
</div>
<div class="section" id="exercise-2-exploring-time-series-data-a-name-2-a">
<h2>Exercise 2: Exploring time series data <a name="2"></a><a class="headerlink" href="#exercise-2-exploring-time-series-data-a-name-2-a" title="Permalink to this headline">¶</a></h2>
<hr>
<p>In this exercise we’ll be looking at a <a class="reference external" href="https://www.kaggle.com/neuromusic/avocado-prices">dataset of avocado prices</a>. You should start by downloading the dataset.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;avocado.csv&quot;</span><span class="p">,</span> <span class="n">parse_dates</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;Date&quot;</span><span class="p">],</span> <span class="n">index_col</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df</span><span class="p">[</span><span class="s2">&quot;Date&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df</span><span class="p">[</span><span class="s2">&quot;Date&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>It looks like the data ranges from the start of 2015 to March 2018 (~3 years ago), for a total of 3.25 years or so. Let’s split the data so that we have a 6 months of test data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">split_date</span> <span class="o">=</span> <span class="s2">&quot;20170925&quot;</span>
<span class="n">train_df</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s2">&quot;Date&quot;</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="n">split_date</span><span class="p">]</span>
<span class="n">test_df</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s2">&quot;Date&quot;</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">split_date</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_df</span><span class="p">)</span> <span class="o">+</span> <span class="nb">len</span><span class="p">(</span><span class="n">test_df</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="id1">
<h3>2.1<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h3>
<p>rubric={points:4}</p>
<p>In the Rain is Australia dataset from lecture, we had different measurements for each Location. What about this dataset: for which categorical feature(s), if any, do we have separate measurements? Justify your answer by referencing the dataset.</p>
<p><br><br></p>
</div>
<div class="section" id="id2">
<h3>2.2<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h3>
<p>rubric={points:4}</p>
<p>In the Rain in Australia dataset, the measurements were generally equally spaced but with some exceptions. How about with this dataset? Justify your answer by referencing the dataset.</p>
<p><br><br></p>
</div>
<div class="section" id="id3">
<h3>2.3<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h3>
<p>rubric={points:4}</p>
<p>In the Rain is Australia dataset, each location was a different place in Australia. For this dataset, look at the names of the regions. Do you think the regions are all distinct, or are there overlapping regions? Justify your answer by referencing the data.</p>
<p><br><br></p>
<p>We will use the entire dataset despite any location-based weirdness uncovered in the previous part.</p>
<p>We would like to forecast the avocado price, which is the <code class="docutils literal notranslate"><span class="pre">AveragePrice</span></code> column. The function below is adapted from Lecture 18, with some improvements.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">create_lag_feature</span><span class="p">(</span>
    <span class="n">df</span><span class="p">,</span> <span class="n">orig_feature</span><span class="p">,</span> <span class="n">lag</span><span class="p">,</span> <span class="n">groupby</span><span class="p">,</span> <span class="n">new_feature_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">clip</span><span class="o">=</span><span class="kc">False</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Creates a new feature that&#39;s a lagged version of an existing one.</span>

<span class="sd">    NOTE: assumes df is already sorted by the time columns and has unique indices.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    df : pandas.core.frame.DataFrame</span>
<span class="sd">        The dataset.</span>
<span class="sd">    orig_feature : str</span>
<span class="sd">        The column name of the feature we&#39;re copying</span>
<span class="sd">    lag : int</span>
<span class="sd">        The lag; negative lag means values from the past, positive lag means values from the future</span>
<span class="sd">    groupby : list</span>
<span class="sd">        Column(s) to group by in case df contains multiple time series</span>
<span class="sd">    new_feature_name : str</span>
<span class="sd">        Override the default name of the newly created column</span>
<span class="sd">    clip : bool</span>
<span class="sd">        If True, remove rows with a NaN values for the new feature</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    pandas.core.frame.DataFrame</span>
<span class="sd">        A new dataframe with the additional column added.</span>

<span class="sd">    TODO: could/should simplify this function by using `df.shift()`</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="n">new_feature_name</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">lag</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">new_feature_name</span> <span class="o">=</span> <span class="s2">&quot;</span><span class="si">%s</span><span class="s2">_lag</span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">orig_feature</span><span class="p">,</span> <span class="o">-</span><span class="n">lag</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">new_feature_name</span> <span class="o">=</span> <span class="s2">&quot;</span><span class="si">%s</span><span class="s2">_ahead</span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">orig_feature</span><span class="p">,</span> <span class="n">lag</span><span class="p">)</span>

    <span class="n">new_df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="o">**</span><span class="p">{</span><span class="n">new_feature_name</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">})</span>
    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">group</span> <span class="ow">in</span> <span class="n">new_df</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="n">groupby</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">lag</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>  <span class="c1"># take values from the past</span>
            <span class="n">new_df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">group</span><span class="o">.</span><span class="n">index</span><span class="p">[</span><span class="o">-</span><span class="n">lag</span><span class="p">:],</span> <span class="n">new_feature_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">group</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:</span><span class="n">lag</span><span class="p">][</span>
                <span class="n">orig_feature</span>
            <span class="p">]</span><span class="o">.</span><span class="n">values</span>
        <span class="k">else</span><span class="p">:</span>  <span class="c1"># take values from the future</span>
            <span class="n">new_df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">group</span><span class="o">.</span><span class="n">index</span><span class="p">[:</span><span class="o">-</span><span class="n">lag</span><span class="p">],</span> <span class="n">new_feature_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">group</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">lag</span><span class="p">:][</span>
                <span class="n">orig_feature</span>
            <span class="p">]</span><span class="o">.</span><span class="n">values</span>

    <span class="k">if</span> <span class="n">clip</span><span class="p">:</span>
        <span class="n">new_df</span> <span class="o">=</span> <span class="n">new_df</span><span class="o">.</span><span class="n">dropna</span><span class="p">(</span><span class="n">subset</span><span class="o">=</span><span class="p">[</span><span class="n">new_feature_name</span><span class="p">])</span>

    <span class="k">return</span> <span class="n">new_df</span>
</pre></div>
</div>
</div>
</div>
<p>We first sort our dataframe properly:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df_sort</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;region&quot;</span><span class="p">,</span> <span class="s2">&quot;type&quot;</span><span class="p">,</span> <span class="s2">&quot;Date&quot;</span><span class="p">])</span><span class="o">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">df_sort</span>
</pre></div>
</div>
</div>
</div>
<p>We then call <code class="docutils literal notranslate"><span class="pre">create_lag_feature</span></code>. This creates a new column in the dataset <code class="docutils literal notranslate"><span class="pre">AveragePriceNextWeek</span></code>, which is the following week’s <code class="docutils literal notranslate"><span class="pre">AveragePrice</span></code>. We have set <code class="docutils literal notranslate"><span class="pre">clip=True</span></code> which means it will remove rows where the target would be missing.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df_hastarget</span> <span class="o">=</span> <span class="n">create_lag_feature</span><span class="p">(</span>
    <span class="n">df_sort</span><span class="p">,</span> <span class="s2">&quot;AveragePrice&quot;</span><span class="p">,</span> <span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;region&quot;</span><span class="p">,</span> <span class="s2">&quot;type&quot;</span><span class="p">],</span> <span class="s2">&quot;AveragePriceNextWeek&quot;</span><span class="p">,</span> <span class="n">clip</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>
<span class="n">df_hastarget</span>
</pre></div>
</div>
</div>
</div>
<p>I will now split the data:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">train_df</span> <span class="o">=</span> <span class="n">df_hastarget</span><span class="p">[</span><span class="n">df_hastarget</span><span class="p">[</span><span class="s2">&quot;Date&quot;</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="n">split_date</span><span class="p">]</span>
<span class="n">test_df</span> <span class="o">=</span> <span class="n">df_hastarget</span><span class="p">[</span><span class="n">df_hastarget</span><span class="p">[</span><span class="s2">&quot;Date&quot;</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">split_date</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p><br><br></p>
</div>
<div class="section" id="baseline">
<h3>2.4 Baseline<a class="headerlink" href="#baseline" title="Permalink to this headline">¶</a></h3>
<p>rubric={points:4}</p>
<p>Let’s try a baseline. Previously we used <code class="docutils literal notranslate"><span class="pre">DummyClassifier</span></code> or <code class="docutils literal notranslate"><span class="pre">DummyRegressor</span></code> as a baseline. This time, we’ll do something else as a baseline: we’ll assume the price stays the same from this week to next week. So, we’ll set our prediction of “AveragePriceNextWeek” exactly equal to “AveragePrice”, assuming no change. That is kind of like saying, “If it’s raining today then I’m guessing it will be raining tomorrow”. This simplistic approach will not get a great score but it’s a good starting point for reference. If our model does worse that this, it must not be very good.</p>
<p>Using this baseline approach, what <span class="math notranslate nohighlight">\(R^2\)</span> do you get?</p>
<p><br><br></p>
</div>
<div class="section" id="optional-2-5-modeling">
<h3>(Optional) 2.5 Modeling<a class="headerlink" href="#optional-2-5-modeling" title="Permalink to this headline">¶</a></h3>
<p>rubric={points:2}</p>
<p>Now that the baseline is done, let’s build some models to forecast the average avocado price a week later. Experiment with a few approachs for encoding the date. Justify the decisions you make. Which approach worked best? Report your test score and briefly discuss your results.</p>
<blockquote>
<div><p>Because we only have 2 splits here, we need to be a bit wary of overfitting on the test set. Try not to test on it a ridiculous number of times. If you are interested in some proper ways of dealing with this, see for example sklearn’s <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.TimeSeriesSplit.html">TimeSeriesSplit</a>, which is like cross-validation for time series data.</p>
</div></blockquote>
<p><br><br><br><br></p>
</div>
</div>
<div class="section" id="exercise-3-short-answer-questions-a-name-3-a">
<h2>Exercise 3: Short answer questions <a name="3"></a><a class="headerlink" href="#exercise-3-short-answer-questions-a-name-3-a" title="Permalink to this headline">¶</a></h2>
<p>Each question is worth 2 points.</p>
<div class="section" id="id4">
<h3>3.1<a class="headerlink" href="#id4" title="Permalink to this headline">¶</a></h3>
<p>rubric={points:4}</p>
<p>The following questions pertain to Lecture 18 on time series data:</p>
<ol class="simple">
<li><p>Sometimes a time series has missing time points or, worse, time points that are unequally spaced in general. Give an example of a real world situation where the time series data would have unequally spaced time points.</p></li>
<li><p>In class we discussed two approaches to using temporal information: encoding the date as one or more features, and creating lagged versions of features. Which of these (one/other/both/neither) two approaches would struggle with unequally spaced time points? Briefly justify your answer.</p></li>
</ol>
<p><br><br></p>
</div>
<div class="section" id="id5">
<h3>3.2<a class="headerlink" href="#id5" title="Permalink to this headline">¶</a></h3>
<p>rubric={points:6}</p>
<p>The following questions pertain to Lecture 19 on survival analysis. We’ll consider the use case of customer churn analysis.</p>
<ol class="simple">
<li><p>What is the problem with simply labeling customers are “churned” or “not churned” and using standard supervised learning techniques, as we did in hw5?</p></li>
<li><p>Consider customer A who just joined last week vs. customer B who has been with the service for a year. Who do you expect will leave the service first: probably customer A, probably customer B, or we don’t have enough information to answer?</p></li>
<li><p>If a customer’s survival function is almost flat during a certain period, how do we interpret that?</p></li>
</ol>
<p><br><br><br><br></p>
</div>
</div>
<div class="section" id="exercise-4-communication-a-name-4-a">
<h2>Exercise 4: Communication <a name="4"></a><a class="headerlink" href="#exercise-4-communication-a-name-4-a" title="Permalink to this headline">¶</a></h2>
<hr><div class="section" id="blog-post">
<h3>4.1 Blog post<a class="headerlink" href="#blog-post" title="Permalink to this headline">¶</a></h3>
<p>rubric={points:40}</p>
<p>Write up your analysis from hw6 or any other assignment or your side project on machine learning in a “blog post” or report format. It’s fine if you just write it here in this notebook. Alternatively, you can publish your blog post publicly and include a link here. (See exercise 4.3.) The target audience for your blog post is someone like yourself right before you took this course. They don’t necessarily have ML knowledge, but they have a solid foundation in technical matters. The post should focus on explaining <strong>your results and what you did</strong> in a way that’s understandable to such a person, <strong>not</strong> a lesson trying to teach someone about machine learning. Again: focus on the results and why they are interesting; avoid pedagogical content.</p>
<p>Your post must include the following elements (not necessarily in this order):</p>
<ul class="simple">
<li><p>Description of the problem/decision.</p></li>
<li><p>Description of the dataset (the raw data and/or some EDA).</p></li>
<li><p>Description of the model.</p></li>
<li><p>Description your results, both quantitatively and qualitatively. Make sure to refer to the original problem/decision.</p></li>
<li><p>A section on caveats, describing at least 3 reasons why your results might be incorrect, misleading, overconfident, or otherwise problematic. Make reference to your specific dataset, model, approach, etc. To check that your reasons are specific enough, make sure they would not make sense, if left unchanged, to most students’ submissions; for example, do not just say “overfitting” without explaining why you might be worried about overfitting in your specific case.</p></li>
<li><p>At least 3 visualizations. These visualizations must be embedded/interwoven into the text, not pasted at the end. The text must refer directly to each visualization. For example “as shown below” or “the figure demonstrates” or “take a look at Figure 1”, etc. It is <strong>not</strong> sufficient to put a visualization in without referring to it directly.</p></li>
</ul>
<p>A reasonable length for your entire post would be <strong>800 words</strong>. The maximum allowed is <strong>1000 words</strong>.</p>
<div class="section" id="example-blog-posts">
<h4>Example blog posts<a class="headerlink" href="#example-blog-posts" title="Permalink to this headline">¶</a></h4>
<p>Here are some examples of applied ML blog posts that you may find useful as inspiration. The target audiences of these posts aren’t necessarily the same as yours, and these posts are longer than yours, but they are well-structured and engaging. You are <strong>not required to read these</strong> posts as part of this assignment - they are here only as examples if you’d find that useful.</p>
<p>From the UBC Master of Data Science blog, written by a past student:</p>
<ul class="simple">
<li><p>https://ubc-mds.github.io/2019-07-26-predicting-customer-probabilities/</p></li>
</ul>
<p>This next one uses R instead of Python, but that might be good in a way, as you can see what it’s like for a reader that doesn’t understand the code itself (the target audience for your post here):</p>
<ul class="simple">
<li><p>https://rpubs.com/RosieB/taylorswiftlyricanalysis</p></li>
</ul>
<p>Finally, here are a couple interviews with winners from Kaggle competitions. The format isn’t quite the same as a blog post, but you might find them interesting/relevant:</p>
<ul class="simple">
<li><p>https://medium.com/kaggle-blog/instacart-market-basket-analysis-feda2700cded</p></li>
<li><p>https://medium.com/kaggle-blog/winner-interview-with-shivam-bansal-data-science-for-good-challenge-city-of-los-angeles-3294c0ed1fb2</p></li>
</ul>
</div>
<div class="section" id="a-note-on-plagiarism">
<h4>A note on plagiarism<a class="headerlink" href="#a-note-on-plagiarism" title="Permalink to this headline">¶</a></h4>
<p>You may <strong>NOT</strong> include text or visualizations that were not written/created by you. If you are in any doubt as to what constitutes plagiarism, please just ask. For more information see the <a class="reference external" href="http://www.calendar.ubc.ca/vancouver/index.cfm?tree=3,54,111,959">UBC Academic Misconduct policies</a>. Please don’t copy this from somewhere 🙏. If you can’t do it.</p>
<p><br><br></p>
</div>
</div>
<div class="section" id="id6">
<h3>4.2<a class="headerlink" href="#id6" title="Permalink to this headline">¶</a></h3>
<p>rubric={points:6}</p>
<p>Describe one effective communication technique that you used in your post, or an aspect of the post that you are particularly satisfied with.</p>
<p>Max 3 sentences</p>
<p><br><br></p>
</div>
</div>
<div class="section" id="optional-not-for-marks-4-3">
<h2>(optional, not for marks) 4.3<a class="headerlink" href="#optional-not-for-marks-4-3" title="Permalink to this headline">¶</a></h2>
<p>Publish your blog post from 4.1 publicly using a tool like Hugo, or somewhere like medium.com, and paste a link here. Be sure to pick a tool in which code and code output look reasonable. This link could be a useful line on your resume!</p>
<p><br><br><br><br></p>
<div class="section" id="optional-exercise-5-a-name-5-a">
<h3>(Optional) Exercise 5 <a name="5"></a><a class="headerlink" href="#optional-exercise-5-a-name-5-a" title="Permalink to this headline">¶</a></h3>
<p>rubric={points:1}</p>
<p><strong>Your tasks:</strong></p>
<p>What is your biggest takeaway from this course?</p>
<blockquote>
<div><p>I’m looking forward to read your answers.</p>
</div></blockquote>
<p><br><br><br><br></p>
</div>
</div>
<div class="section" id="submission-instructions">
<h2>Submission instructions<a class="headerlink" href="#submission-instructions" title="Permalink to this headline">¶</a></h2>
<p><strong>PLEASE READ:</strong> When you are ready to submit your assignment do the following:</p>
<ol class="simple">
<li><p>Run all cells in your notebook to make sure there are no errors by doing <code class="docutils literal notranslate"><span class="pre">Kernel</span> <span class="pre">-&gt;</span> <span class="pre">Restart</span> <span class="pre">Kernel</span> <span class="pre">and</span> <span class="pre">Clear</span> <span class="pre">All</span> <span class="pre">Outputs</span></code> and then <code class="docutils literal notranslate"><span class="pre">Run</span> <span class="pre">-&gt;</span> <span class="pre">Run</span> <span class="pre">All</span> <span class="pre">Cells</span></code>.</p></li>
<li><p>Notebooks with cell execution numbers out of order or not starting from “1” will have marks deducted. Notebooks without the output displayed may not be graded at all (because we need to see the output in order to grade your work).</p></li>
<li><p>Upload the assignment using Gradescope’s drag and drop tool. Check out this <a class="reference external" href="https://lthub.ubc.ca/guides/gradescope-student-guide/">Gradescope Student Guide</a> if you need help with Gradescope submission.</p></li>
</ol>
<div class="section" id="congratulations-on-finishing-all-homework-assignments-clap-clap">
<h3>Congratulations on finishing all homework assignments! :clap: :clap:<a class="headerlink" href="#congratulations-on-finishing-all-homework-assignments-clap-clap" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">Image</span>

<span class="n">Image</span><span class="p">(</span><span class="s2">&quot;eva-congrats.png&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "conda-env-cpsc330-py"
        },
        kernelOptions: {
            kernelName: "conda-env-cpsc330-py",
            path: "./hw/hw8"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'conda-env-cpsc330-py'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By Varada Kolhatkar<br/>
    
        &copy; Copyright 2021.<br/>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="../../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>