
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Lecture 16: Introduction to natural language processing &#8212; CPSC 330 Applied Machine Learning</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet" />
  <link href="../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.e8e5499552300ddf5d7adccae7cc3b70.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      <img src="../_static/UBC-CS-logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">CPSC 330 Applied Machine Learning</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <div class="bd-toc-item active">
        <p class="caption">
 <span class="caption-text">
  Things you should know
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../docs/README.html">
   CPSC 330 Documents
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Lectures
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="01_intro.html">
   Lecture 1: Course Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="02_decision-trees.html">
   Lecture 2: Terminology, Baselines, Decision Trees
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="03_ml-fundamentals.html">
   Lecture 3: Machine Learning Fundamentals
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="04_kNNs-SVM-RBF.html">
   Lecture 4:
   <span class="math notranslate nohighlight">
    \(k\)
   </span>
   -Nearest Neighbours and SVM RBFs
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="05_preprocessing-pipelines.html">
   Lecture 5: Preprocessing and
   <code class="docutils literal notranslate">
    <span class="pre">
     sklearn
    </span>
   </code>
   pipelines
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="06_column-transformer-text-feats.html">
   Lecture 6:
   <code class="docutils literal notranslate">
    <span class="pre">
     sklearn
    </span>
   </code>
   <code class="docutils literal notranslate">
    <span class="pre">
     ColumnTransformer
    </span>
   </code>
   and Text Features
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="07_linear-models.html">
   Lecture 7: Linear Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="08_hyperparameter-optimization.html">
   Lecture 8: Hyperparameter Optimization and Optimization Bias
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="09_classification-metrics.html">
   Lecture 9: Classification Metrics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="10_regression-metrics.html">
   Lecture 10: Regression Evaluation Metrics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="11_ensembles.html">
   Lecture 11: Ensembles
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="14_k-means-clustering.html">
   Lecture 14: Clustering
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="15_recommender-systems.html">
   Lecture 15: DBSCAN and Recommender Systems
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Attribution
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../attribution.html">
   Attributions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../LICENSE.html">
   LICENSE
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Varada Kolhatkar, CPSC 330 2021-22<br>Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/lectures/16_natural-language-processing.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/UBC-CS/cpsc330/master?urlpath=tree/lectures/16_natural-language-processing.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#what-is-natural-language-processing-nlp">
   What is Natural Language Processing (NLP)?
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id1">
     What is Natural Language Processing (NLP)?
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#how-often-do-you-search-everyday">
       How often do you search everyday?
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id2">
     What is Natural Language Processing (NLP)?
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#everyday-nlp-applications">
     Everyday NLP applications
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#nlp-in-news">
     NLP in news
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#why-is-nlp-hard">
     Why is NLP hard?
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#example-lexical-ambiguity">
   Example: Lexical ambiguity
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#example-referential-ambiguity">
   Example: Referential ambiguity
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#ambiguous-news-headlines">
     Ambiguous news headlines
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#overall-goal">
     Overall goal
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#word-embeddings">
   Word Embeddings
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#why-do-we-care-about-word-representation">
     Why do we care about word representation?
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#word-meaning">
     Word meaning
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#word-meaning-ml-and-nlp-view">
     Word meaning: ML and NLP view
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#word-representations">
   Word representations
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#activity-1-brainstorm-ways-to-represent-words-2-mins">
     Activity 1:  Brainstorm ways to represent words (~2 mins)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#need-a-representation-that-captures-relationships-between-words">
     Need a representation that captures relationships between words.
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#distributional-hypothesis">
     Distributional hypothesis
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#vector-space-model">
     Vector space model
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#term-term-co-occurrence-matrix">
     Term-term co-occurrence matrix
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#visualizing-word-vectors-and-similarity">
     Visualizing word vectors and similarity
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id3">
     Visualizing word vectors and similarity
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#sparse-vs-dense-word-vectors">
     Sparse vs. dense word vectors
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#alternative">
     Alternative
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#word2vec">
     Word2Vec
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#activity-2-try-out-word-similarity-with-the-code-above-4-mins">
       Activity 2: Try out word similarity with the code above (~4 mins)
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#other-popular-methods-to-get-embeddings">
     Other popular methods to get embeddings
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#fasttext">
     fastText
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#glove-global-vectors-for-word-representation">
     GloVe: Global Vectors for Word Representation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#pre-trained-embeddings">
     Pre-trained embeddings
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id4">
     Pre-trained embeddings
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#finding-similar-words">
     Finding similar words
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#success-of-word2vec">
     Success of Word2Vec
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#examples-of-semantic-and-syntactic-relationships">
     Examples of semantic and syntactic relationships
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#implicit-biases-and-stereotypes-in-word-embeddings">
     Implicit biases and stereotypes in word embeddings
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#representing-documents-using-word-embeddings">
     Representing documents using word embeddings
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#averaging-embeddings">
     Averaging embeddings
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#average-embeddings-with-spacy">
     Average embeddings with spaCy
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#similarity-between-documents">
     Similarity between documents
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#airline-sentiment-analysis-using-average-embedding-representation">
     Airline sentiment analysis using average embedding representation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#bag-of-words-representation-for-sentiment-analysis">
     Bag-of-words representation for sentiment analysis
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#sentiment-analysis-with-average-embedding-representation">
     Sentiment analysis with average embedding representation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#sentiment-classification-using-average-embeddings">
     Sentiment classification using average embeddings
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#spacy-s-pre-trained-embeddings">
     <code class="docutils literal notranslate">
      <span class="pre">
       spaCy
      </span>
     </code>
     ’s pre-trained embeddings
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#topic-modeling">
   Topic modeling
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#topic-modeling-motivation">
     Topic modeling motivation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#example-a-corpus-of-news-articles">
     Example: A corpus of news articles
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#example-a-corpus-of-food-magazines">
     Example: A corpus of food magazines
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#a-corpus-of-scientific-articles">
     A corpus of scientific articles
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id5">
     Topic modeling motivation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id6">
     Topic modeling
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#topic-modeling-input-and-output">
     Topic modeling: Input and output
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#topic-modeling-example">
     Topic modeling: Example
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id7">
     Topic modeling: Example
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id8">
     Topic modeling: Input and output
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#topic-modeling-some-applications">
     Topic modeling: Some applications
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#topic-modeling-toy-example">
     Topic modeling toy example
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#topic-modeling-with-sklearn">
     Topic modeling with
     <code class="docutils literal notranslate">
      <span class="pre">
       sklearn
      </span>
     </code>
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#basic-text-preprocessing">
   Basic text preprocessing
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#introduction">
     Introduction
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#tokenization">
     Tokenization
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#tokenization-sentence-segmentation">
     Tokenization: sentence segmentation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#sentence-segmentation">
     Sentence segmentation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#word-tokenization">
     Word tokenization
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id9">
     Word tokenization
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#word-segmentation">
     Word segmentation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#types-and-tokens">
     Types and tokens
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#exercise-for-you">
     Exercise for you
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#other-commonly-used-preprocessing-steps">
     Other commonly used preprocessing steps
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#punctuation-and-stopword-removal">
     Punctuation and stopword removal
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#lemmatization">
     Lemmatization
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#stemming">
     Stemming
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#other-tools-for-preprocessing">
     Other tools for preprocessing
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#spacy">
     spaCy
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#other-typical-nlp-tasks">
     Other typical NLP tasks
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#extracting-named-entities-using-spacy">
     Extracting named-entities using spaCy
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#dependency-parsing-using-spacy">
     Dependency parsing using spaCy
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#many-other-things-possible">
     Many other things possible
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#summary">
   Summary
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <p><img alt="" src="../_images/330-banner.png" /></p>
<div class="section" id="lecture-16-introduction-to-natural-language-processing">
<h1>Lecture 16: Introduction to natural language processing<a class="headerlink" href="#lecture-16-introduction-to-natural-language-processing" title="Permalink to this headline">¶</a></h1>
<p>UBC 2020-21</p>
<p>Instructor: Varada Kolhatkar</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">## Imports</span>

<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">re</span>
<span class="kn">import</span> <span class="nn">string</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">Counter</span><span class="p">,</span> <span class="n">defaultdict</span>

<span class="kn">import</span> <span class="nn">IPython</span>
<span class="kn">import</span> <span class="nn">nltk</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">numpy.random</span> <span class="k">as</span> <span class="nn">npr</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">HTML</span>
<span class="kn">from</span> <span class="nn">ipywidgets</span> <span class="kn">import</span> <span class="n">interactive</span>
<span class="kn">from</span> <span class="nn">nltk.corpus</span> <span class="kn">import</span> <span class="n">stopwords</span>
<span class="kn">from</span> <span class="nn">nltk.tokenize</span> <span class="kn">import</span> <span class="n">sent_tokenize</span><span class="p">,</span> <span class="n">word_tokenize</span>
<span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">CountVectorizer</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">make_pipeline</span>
</pre></div>
</div>
</div>
</div>
<p><br><br></p>
<div class="section" id="what-is-natural-language-processing-nlp">
<h2>What is Natural Language Processing (NLP)?<a class="headerlink" href="#what-is-natural-language-processing-nlp" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>What should a search engine return when asked the following question?</p></li>
</ul>
<center>
<img src="img/lexical_ambiguity.png" width="1000" height="1000">
</center>    <div class="section" id="id1">
<h3>What is Natural Language Processing (NLP)?<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h3>
<div class="section" id="how-often-do-you-search-everyday">
<h4>How often do you search everyday?<a class="headerlink" href="#how-often-do-you-search-everyday" title="Permalink to this headline">¶</a></h4>
<center>
<img src="img/Google_search.png" width="900" height="900">
</center></div>
</div>
<div class="section" id="id2">
<h3>What is Natural Language Processing (NLP)?<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h3>
<center>
<img src="img/WhatisNLP.png" width="800" height="800">
</center>    </div>
<div class="section" id="everyday-nlp-applications">
<h3>Everyday NLP applications<a class="headerlink" href="#everyday-nlp-applications" title="Permalink to this headline">¶</a></h3>
<center>
<img src="img/annotation-image.png" height="1200" width="1200">
</center></div>
<div class="section" id="nlp-in-news">
<h3>NLP in news<a class="headerlink" href="#nlp-in-news" title="Permalink to this headline">¶</a></h3>
<p>Often you’ll NLP in news. Some examples:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://abcnews.go.com/GMA/Wellness/suicide-prevention-boost-artificial-intelligence-exclusive/story?id=76541481">How suicide prevention is getting a boost from artificial intelligence</a></p></li>
<li><p><a class="reference external" href="https://www.nytimes.com/2020/11/24/science/artificial-intelligence-ai-gpt3.html">Meet GPT-3. It Has Learned to Code (and Blog and Argue).</a></p></li>
<li><p><a class="reference external" href="https://www.nytimes.com/2020/07/29/opinion/gpt-3-ai-automation.html">How Do You Know a Human Wrote This?</a></p></li>
<li><p>…</p></li>
</ul>
</div>
<div class="section" id="why-is-nlp-hard">
<h3>Why is NLP hard?<a class="headerlink" href="#why-is-nlp-hard" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Language is complex and subtle.</p></li>
<li><p>Language is ambiguous at different levels.</p></li>
<li><p>Language understanding involves common-sense knowledge and real-world reasoning.</p></li>
<li><p>All the problems related to representation and reasoning in artificial intelligence arise in this domain.</p></li>
</ul>
</div>
</div>
<div class="section" id="example-lexical-ambiguity">
<h2>Example: Lexical ambiguity<a class="headerlink" href="#example-lexical-ambiguity" title="Permalink to this headline">¶</a></h2>
<p><br><br></p>
<a class="reference internal image-reference" href="../_images/lexical_ambiguity.png"><img alt="../_images/lexical_ambiguity.png" src="../_images/lexical_ambiguity.png" style="width: 1000px; height: 1000px;" /></a>
</div>
<div class="section" id="example-referential-ambiguity">
<h2>Example: Referential ambiguity<a class="headerlink" href="#example-referential-ambiguity" title="Permalink to this headline">¶</a></h2>
<p><br><br></p>
<a class="reference internal image-reference" href="../_images/referential_ambiguity.png"><img alt="../_images/referential_ambiguity.png" src="../_images/referential_ambiguity.png" style="width: 1000px; height: 1000px;" /></a>
<div class="section" id="ambiguous-news-headlines">
<h3><a class="reference external" href="http://www.fun-with-words.com/ambiguous_headlines.html">Ambiguous news headlines</a><a class="headerlink" href="#ambiguous-news-headlines" title="Permalink to this headline">¶</a></h3>
<blockquote>
PROSTITUTES APPEAL TO POPE
</blockquote>    
<ul class="simple">
<li><p><strong>appeal to</strong> means make a serious or urgent request or be attractive or interesting?</p></li>
</ul>
<blockquote>
KICKING BABY CONSIDERED TO BE HEALTHY    
</blockquote> 
<ul class="simple">
<li><p><strong>kicking</strong> is used as an adjective or a verb?</p></li>
</ul>
<blockquote>
MILK DRINKERS ARE TURNING TO POWDER
</blockquote>
<ul class="simple">
<li><p><strong>turning</strong> means becoming or take up?</p></li>
</ul>
</div>
<div class="section" id="overall-goal">
<h3>Overall goal<a class="headerlink" href="#overall-goal" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Give you a quick introduction to you of this important field in artificial intelligence which extensively used machine learning.</p></li>
</ul>
<center>
<img src="img/NLP_in_industry.png" width="900" height="800">
</center><p>Today’s plan</p>
<ul class="simple">
<li><p>Word embeddings</p></li>
<li><p>Sentence embeddings</p></li>
<li><p>Topic modeling</p></li>
<li><p>Basic text preprocessing</p></li>
</ul>
</div>
</div>
<div class="section" id="word-embeddings">
<h2>Word Embeddings<a class="headerlink" href="#word-embeddings" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>The idea is to create <strong>embeddings of words</strong> so that distances among words in the vector space indicate the relationship between them.</p></li>
</ul>
<a class="reference internal image-reference" href="../_images/t-SNE_word_embeddings.png"><img alt="../_images/t-SNE_word_embeddings.png" src="../_images/t-SNE_word_embeddings.png" style="width: 900px; height: 900px;" /></a>
<p>(Attribution: Jurafsky and Martin 3rd edition)</p>
<div class="section" id="why-do-we-care-about-word-representation">
<h3>Why do we care about word representation?<a class="headerlink" href="#why-do-we-care-about-word-representation" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>So far we have been talking about sentence or document representation.</p></li>
<li><p>Now we are going one step back and talking about word representation.</p></li>
<li><p>Although word representation cannot be effectively used in text classification tasks such as sentiment analysis using tradition ML models, it’s good to know about word embeddings because they are so widely used.</p></li>
<li><p>They are quite useful in more advanced machine learning models such as recurrent neural networks.</p></li>
</ul>
</div>
<div class="section" id="word-meaning">
<h3>Word meaning<a class="headerlink" href="#word-meaning" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>A favourite topic of philosophers for centuries.</p></li>
<li><p>An example from legal domain: <a class="reference external" href="https://www.scc-csc.ca/case-dossier/info/sum-som-eng.aspx?cas=36258">Are hockey gloves gloves or “articles of plastics”?</a></p></li>
</ul>
<blockquote>
Canada (A.G.) v. Igloo Vikski Inc. was a tariff code case that made its way to the SCC (Supreme Court of Canada). The case disputed the definition of hockey gloves as either gloves or as "articles of plastics."
</blockquote>
<a class="reference internal image-reference" href="../_images/hockey_gloves_case.png"><img alt="../_images/hockey_gloves_case.png" src="../_images/hockey_gloves_case.png" style="width: 900px; height: 900px;" /></a>
</div>
<div class="section" id="word-meaning-ml-and-nlp-view">
<h3>Word meaning: ML and NLP view<a class="headerlink" href="#word-meaning-ml-and-nlp-view" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Modeling word meaning that allows us to</p>
<ul>
<li><p>draw useful inferences to solve meaning-related problems</p></li>
<li><p>find relationship between words,</p>
<ul>
<li><p>E.g., which words are similar, which ones have positive or negative connotations</p></li>
</ul>
</li>
</ul>
</li>
</ul>
<p><br><br><br><br></p>
</div>
</div>
<div class="section" id="word-representations">
<h2>Word representations<a class="headerlink" href="#word-representations" title="Permalink to this headline">¶</a></h2>
<div class="section" id="activity-1-brainstorm-ways-to-represent-words-2-mins">
<h3>Activity 1:  Brainstorm ways to represent words (~2 mins)<a class="headerlink" href="#activity-1-brainstorm-ways-to-represent-words-2-mins" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Suppose you are building a question answering system and you are given the following question and three candidate answers.</p></li>
<li><p>What kind of relationship between words would we like our representation to capture in order to arrive at the correct answer?</p></li>
</ul>
<blockquote>       
<p style="font-size:30px"><b>Question:</b> How <b>tall</b> is Machu Picchu?</p>
    <p style="font-size:30px"><b>Candidate 1:</b> Machu Picchu is 13.164 degrees south of the equator.</p>    
<p style="font-size:30px"><b>Candidate 2:</b> The official height of Machu Picchu is 2,430 m.</p>
<p style="font-size:30px"><b>Candidate 3:</b> Machu Picchu is 80 kilometres (50 miles) northwest of Cusco.</p>    
</blockquote> 
</div>
<div class="section" id="need-a-representation-that-captures-relationships-between-words">
<h3>Need a representation that captures relationships between words.<a class="headerlink" href="#need-a-representation-that-captures-relationships-between-words" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>We will be looking at two such representations.</p>
<ol class="simple">
<li><p>Sparse representation with <strong>term-term co-occurrence matrix</strong></p></li>
<li><p>Dense representation with <strong>Word2Vec skip-gram model</strong></p></li>
</ol>
</li>
<li><p>Both are based on two ideas: <strong>distributional hypothesis</strong> and <strong>vector space model</strong>.</p></li>
</ul>
</div>
<div class="section" id="distributional-hypothesis">
<h3>Distributional hypothesis<a class="headerlink" href="#distributional-hypothesis" title="Permalink to this headline">¶</a></h3>
<blockquote> 
    <p>You shall know a word by the company it keeps.</p>
    <footer>Firth, 1957</footer>        
</blockquote>
<blockquote> 
If A and B have almost identical environments we say that they are synonyms.
<footer>Harris, 1954</footer>    
</blockquote>    
<p>Example:</p>
<ul class="simple">
<li><p>Her <strong>child</strong> loves to play in the playground.</p></li>
<li><p>Her <strong>kid</strong> loves to play in the playground.</p></li>
</ul>
</div>
<div class="section" id="vector-space-model">
<h3>Vector space model<a class="headerlink" href="#vector-space-model" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Model the meaning of a word by placing it into a vector space.</p></li>
<li><p>A standard way to represent meaning in NLP</p></li>
<li><p>Distances among words in the vector space indicate the relationship between them.</p></li>
</ul>
<a class="reference internal image-reference" href="../_images/t-SNE_word_embeddings.png"><img alt="../_images/t-SNE_word_embeddings.png" src="../_images/t-SNE_word_embeddings.png" style="width: 900px; height: 900px;" /></a>
<p>(Attribution: Jurafsky and Martin 3rd edition)</p>
</div>
<div class="section" id="term-term-co-occurrence-matrix">
<h3>Term-term co-occurrence matrix<a class="headerlink" href="#term-term-co-occurrence-matrix" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>So far we have been talking about documents and we created document-term co-occurrence matrix (e.g., bag-of-words representation of text).</p></li>
<li><p>We can also do this with words. The idea is to go through a corpus of text, keeping a count of all of the words that appear in context of each word (within a window).</p></li>
<li><p>An example:</p></li>
</ul>
<a class="reference internal image-reference" href="../_images/term-term_comat.png"><img alt="../_images/term-term_comat.png" src="../_images/term-term_comat.png" style="width: 600px; height: 600px;" /></a>
<p>(Credit: Jurafsky and Martin 3rd edition)</p>
</div>
<div class="section" id="visualizing-word-vectors-and-similarity">
<h3>Visualizing word vectors and similarity<a class="headerlink" href="#visualizing-word-vectors-and-similarity" title="Permalink to this headline">¶</a></h3>
<a class="reference internal image-reference" href="../_images/word_vectors_and_angles.png"><img alt="../_images/word_vectors_and_angles.png" src="../_images/word_vectors_and_angles.png" style="width: 800px; height: 800px;" /></a>
<p>(Credit: Jurafsky and Martin 3rd edition)</p>
<ul class="simple">
<li><p>The similarity is calculated using dot products between word vectors.</p>
<ul>
<li><p>Example: <span class="math notranslate nohighlight">\(\vec{\text{digital}}.\vec{\text{information}} = 0 \times 1 + 1\times 6 = 6\)</span></p></li>
<li><p>Higher the dot product more similar the words.</p></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="id3">
<h3>Visualizing word vectors and similarity<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h3>
<a class="reference internal image-reference" href="../_images/word_vectors_and_angles.png"><img alt="../_images/word_vectors_and_angles.png" src="../_images/word_vectors_and_angles.png" style="width: 600px; height: 600px;" /></a>
<p>(Credit: Jurafsky and Martin 3rd edition)</p>
<ul class="simple">
<li><p>The similarity is calculated using dot products between word vectors.</p>
<ul>
<li><p>Example: <span class="math notranslate nohighlight">\(\vec{\text{digital}}.\vec{\text{information}} = 0 \times 1 + 1\times 6 = 6\)</span></p></li>
<li><p>Higher the dot product more similar the words.</p></li>
</ul>
</li>
<li><p>We can also calculate a normalized version of dot products.
$<span class="math notranslate nohighlight">\(similarity_{cosine}(w_1,w_2) = \frac{w_1.w_2}{\left\lVert w_1\right\rVert_2 \left\lVert w_2\right\rVert_2}\)</span>$</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">### Let&#39;s build term-term co-occurrence matrix for our text.</span>
<span class="n">corpus</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;How tall is Machu Picchu?&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Machu Picchu is 13.164 degrees south of the equator.&quot;</span><span class="p">,</span>
    <span class="s2">&quot;The official height of Machu Picchu is 2,430 m.&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Machu Picchu is 80 kilometres (50 miles) northwest of Cusco.&quot;</span><span class="p">,</span>
    <span class="s2">&quot;It is 80 kilometres (50 miles) northwest of Cusco, on the crest of the mountain Machu Picchu, located about 2,430 metres (7,970 feet) above mean sea level, over 1,000 metres (3,300 ft) lower than Cusco, which has an elevation of 3,400 metres (11,200 ft).&quot;</span><span class="p">,</span>
<span class="p">]</span>
<span class="n">pp</span> <span class="o">=</span> <span class="n">MyPreprocessor</span><span class="p">()</span>
<span class="n">pp_corpus</span> <span class="o">=</span> <span class="n">pp</span><span class="o">.</span><span class="n">preprocess_corpus</span><span class="p">(</span><span class="n">corpus</span><span class="p">)</span>
<span class="n">cm</span> <span class="o">=</span> <span class="n">CooccurrenceMatrix</span><span class="p">(</span><span class="n">pp_corpus</span><span class="p">)</span>
<span class="n">vocab</span><span class="p">,</span> <span class="n">comat</span> <span class="o">=</span> <span class="n">cm</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">()</span>
<span class="n">words</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">key</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">vocab</span><span class="o">.</span><span class="n">items</span><span class="p">(),</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">item</span><span class="p">:</span> <span class="p">(</span><span class="n">item</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">item</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
<span class="p">]</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">comat</span><span class="o">.</span><span class="n">todense</span><span class="p">(),</span> <span class="n">columns</span><span class="o">=</span><span class="n">words</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">words</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int8</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">NameError</span><span class="g g-Whitespace">                                 </span>Traceback (most recent call last)
<span class="o">/</span><span class="n">var</span><span class="o">/</span><span class="n">folders</span><span class="o">/</span><span class="mi">80</span><span class="o">/</span><span class="n">kr9rkqfj4w78h49djkz8yy9r0000gp</span><span class="o">/</span><span class="n">T</span><span class="o">/</span><span class="n">ipykernel_38418</span><span class="o">/</span><span class="mf">1772445134.</span><span class="n">py</span> <span class="ow">in</span> <span class="o">&lt;</span><span class="n">module</span><span class="o">&gt;</span>
<span class="g g-Whitespace">      </span><span class="mi">7</span>     <span class="s2">&quot;It is 80 kilometres (50 miles) northwest of Cusco, on the crest of the mountain Machu Picchu, located about 2,430 metres (7,970 feet) above mean sea level, over 1,000 metres (3,300 ft) lower than Cusco, which has an elevation of 3,400 metres (11,200 ft).&quot;</span><span class="p">,</span>
<span class="g g-Whitespace">      </span><span class="mi">8</span> <span class="p">]</span>
<span class="ne">----&gt; </span><span class="mi">9</span> <span class="n">pp</span> <span class="o">=</span> <span class="n">MyPreprocessor</span><span class="p">()</span>
<span class="g g-Whitespace">     </span><span class="mi">10</span> <span class="n">pp_corpus</span> <span class="o">=</span> <span class="n">pp</span><span class="o">.</span><span class="n">preprocess_corpus</span><span class="p">(</span><span class="n">corpus</span><span class="p">)</span>
<span class="g g-Whitespace">     </span><span class="mi">11</span> <span class="n">cm</span> <span class="o">=</span> <span class="n">CooccurrenceMatrix</span><span class="p">(</span><span class="n">pp_corpus</span><span class="p">)</span>

<span class="ne">NameError</span>: name &#39;MyPreprocessor&#39; is not defined
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.metrics.pairwise</span> <span class="kn">import</span> <span class="n">cosine_similarity</span>


<span class="k">def</span> <span class="nf">similarity</span><span class="p">(</span><span class="n">word1</span><span class="p">,</span> <span class="n">word2</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns similarity score between word1 and word2</span>
<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    word1 -- (str)</span>
<span class="sd">        The first word</span>
<span class="sd">    word2 -- (str)</span>
<span class="sd">        The second word</span>

<span class="sd">    Returns</span>
<span class="sd">    --------</span>
<span class="sd">    None. Prints the similarity score between word1 and word2.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">vec1</span> <span class="o">=</span> <span class="n">cm</span><span class="o">.</span><span class="n">get_word_vector</span><span class="p">(</span><span class="n">word1</span><span class="p">)</span><span class="o">.</span><span class="n">todense</span><span class="p">()</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
    <span class="n">vec2</span> <span class="o">=</span> <span class="n">cm</span><span class="o">.</span><span class="n">get_word_vector</span><span class="p">(</span><span class="n">word2</span><span class="p">)</span><span class="o">.</span><span class="n">todense</span><span class="p">()</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
    <span class="n">v1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">vec1</span><span class="p">))</span>
    <span class="n">v2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">vec2</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span>
        <span class="s2">&quot;The dot product between </span><span class="si">%s</span><span class="s2"> and </span><span class="si">%s</span><span class="s2"> is </span><span class="si">%0.2f</span><span class="s2"> and cosine similarity is </span><span class="si">%0.2f</span><span class="s2">&quot;</span>
        <span class="o">%</span> <span class="p">(</span><span class="n">word1</span><span class="p">,</span> <span class="n">word2</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">v1</span><span class="p">,</span> <span class="n">v2</span><span class="p">),</span> <span class="n">cosine_similarity</span><span class="p">(</span><span class="n">vec1</span><span class="p">,</span> <span class="n">vec2</span><span class="p">))</span>
    <span class="p">)</span>

<span class="n">similarity</span><span class="p">(</span><span class="s2">&quot;tall&quot;</span><span class="p">,</span> <span class="s2">&quot;height&quot;</span><span class="p">)</span>
<span class="n">similarity</span><span class="p">(</span><span class="s2">&quot;tall&quot;</span><span class="p">,</span> <span class="s2">&quot;official&quot;</span><span class="p">)</span>

<span class="c1">### Not very reliable similarity scores because we used only 4 sentences.</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The dot product between tall and height is 2.00 and cosine similarity is 0.71
The dot product between tall and official is 2.00 and cosine similarity is 0.82
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>We are getting non-zero dot products now and we are able to capture some similarities between words now.</p></li>
<li><p>That said similarities do not make much sense in the toy example above because we’re using a tiny corpus.</p></li>
<li><p>To find meaningful patterns of similarities between words, we need a large corpus.</p></li>
<li><p>Let’s try a bit larger corpus and check whether the similarities make sense.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">wikipedia</span>
<span class="kn">from</span> <span class="nn">nltk.tokenize</span> <span class="kn">import</span> <span class="n">sent_tokenize</span><span class="p">,</span> <span class="n">word_tokenize</span>

<span class="n">corpus</span> <span class="o">=</span> <span class="p">[]</span>

<span class="n">queries</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;Machu Picchu&quot;</span><span class="p">,</span> <span class="s2">&quot;human stature&quot;</span><span class="p">,</span> <span class="s2">&quot;Everest&quot;</span><span class="p">]</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">queries</span><span class="p">)):</span>
    <span class="n">sents</span> <span class="o">=</span> <span class="n">sent_tokenize</span><span class="p">(</span><span class="n">wikipedia</span><span class="o">.</span><span class="n">page</span><span class="p">(</span><span class="n">queries</span><span class="p">[</span><span class="n">i</span><span class="p">])</span><span class="o">.</span><span class="n">content</span><span class="p">)</span>
    <span class="n">corpus</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">sents</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Number of sentences in the corpus: &quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">sents</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Number of sentences in the corpus:  686
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pp</span> <span class="o">=</span> <span class="n">MyPreprocessor</span><span class="p">()</span>
<span class="n">pp_corpus</span> <span class="o">=</span> <span class="n">pp</span><span class="o">.</span><span class="n">preprocess_corpus</span><span class="p">(</span><span class="n">corpus</span><span class="p">)</span>
<span class="n">cm</span> <span class="o">=</span> <span class="n">CooccurrenceMatrix</span><span class="p">(</span><span class="n">pp_corpus</span><span class="p">)</span>
<span class="n">vocab</span><span class="p">,</span> <span class="n">comat</span> <span class="o">=</span> <span class="n">cm</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">()</span>
<span class="n">words</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">key</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">vocab</span><span class="o">.</span><span class="n">items</span><span class="p">(),</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">item</span><span class="p">:</span> <span class="p">(</span><span class="n">item</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">item</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
<span class="p">]</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">comat</span><span class="o">.</span><span class="n">todense</span><span class="p">(),</span> <span class="n">columns</span><span class="o">=</span><span class="n">words</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">words</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int8</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>machu</th>
      <th>picchu</th>
      <th>15th-century</th>
      <th>inca</th>
      <th>citadel</th>
      <th>located</th>
      <th>eastern</th>
      <th>cordillera</th>
      <th>southern</th>
      <th>peru</th>
      <th>...</th>
      <th>panorama</th>
      <th>drawing</th>
      <th>imaging</th>
      <th>panoramas</th>
      <th>interactive</th>
      <th>summitpost</th>
      <th>format</th>
      <th>quicktime</th>
      <th>virtual</th>
      <th>info-graphic</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>machu</th>
      <td>0</td>
      <td>89</td>
      <td>1</td>
      <td>6</td>
      <td>2</td>
      <td>3</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>4</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>picchu</th>
      <td>89</td>
      <td>0</td>
      <td>1</td>
      <td>6</td>
      <td>4</td>
      <td>2</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>3</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>15th-century</th>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>inca</th>
      <td>6</td>
      <td>6</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>citadel</th>
      <td>2</td>
      <td>4</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 5615 columns</p>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">similarity</span><span class="p">(</span><span class="s2">&quot;tall&quot;</span><span class="p">,</span> <span class="s2">&quot;height&quot;</span><span class="p">)</span>
<span class="n">similarity</span><span class="p">(</span><span class="s2">&quot;tall&quot;</span><span class="p">,</span> <span class="s2">&quot;official&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The dot product between tall and height is 108.00 and cosine similarity is 0.19
The dot product between tall and official is 0.00 and cosine similarity is 0.00
</pre></div>
</div>
</div>
</div>
<p><br><br></p>
</div>
<div class="section" id="sparse-vs-dense-word-vectors">
<h3>Sparse vs. dense word vectors<a class="headerlink" href="#sparse-vs-dense-word-vectors" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Term-term co-occurrence matrices are long and sparse.</p>
<ul>
<li><p>length |V| is usually large (e.g., &gt; 50,000)</p></li>
<li><p>most elements are zero</p></li>
</ul>
</li>
<li><p>OK because there are efficient ways to deal with sparse matrices.</p></li>
</ul>
</div>
<div class="section" id="alternative">
<h3>Alternative<a class="headerlink" href="#alternative" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Learn short (~100 to 1000 dimensions) and dense vectors.</p></li>
<li><p>Short vectors may be easier to train with ML models (less weights to train).</p></li>
<li><p>They may generalize better.</p></li>
<li><p>In practice they work much better!</p></li>
</ul>
</div>
<div class="section" id="word2vec">
<h3>Word2Vec<a class="headerlink" href="#word2vec" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>A family of algorithms to create dense word embeddings</p></li>
</ul>
<a class="reference internal image-reference" href="../_images/word2vec.png"><img alt="../_images/word2vec.png" src="../_images/word2vec.png" style="width: 1000px; height: 1000px;" /></a>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">conda</span> <span class="n">install</span> <span class="o">-</span><span class="n">c</span> <span class="n">anaconda</span> <span class="n">gensim</span>
</pre></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">spacy</span>
<span class="n">nlp</span> <span class="o">=</span> <span class="n">spacy</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;en_core_web_md&quot;</span><span class="p">)</span>  <span class="c1"># make sure to use larger package!</span>
<span class="n">doc1</span> <span class="o">=</span> <span class="n">nlp</span><span class="p">(</span><span class="s2">&quot;I like salty fries and hamburgers.&quot;</span><span class="p">)</span>
<span class="n">doc2</span> <span class="o">=</span> <span class="n">nlp</span><span class="p">(</span><span class="s2">&quot;Fast food tastes very good.&quot;</span><span class="p">)</span>
<span class="c1"># Similarity of two documents</span>
<span class="nb">print</span><span class="p">(</span><span class="n">doc1</span><span class="p">,</span> <span class="s2">&quot;&lt;-&gt;&quot;</span><span class="p">,</span> <span class="n">doc2</span><span class="p">,</span> <span class="n">doc1</span><span class="o">.</span><span class="n">similarity</span><span class="p">(</span><span class="n">doc2</span><span class="p">))</span>
<span class="c1"># Similarity of tokens and spans</span>
<span class="n">french_fries</span> <span class="o">=</span> <span class="n">doc1</span><span class="p">[</span><span class="mi">2</span><span class="p">:</span><span class="mi">4</span><span class="p">]</span>
<span class="n">burgers</span> <span class="o">=</span> <span class="n">doc1</span><span class="p">[</span><span class="mi">5</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">french_fries</span><span class="p">,</span> <span class="s2">&quot;&lt;-&gt;&quot;</span><span class="p">,</span> <span class="n">burgers</span><span class="p">,</span> <span class="n">french_fries</span><span class="o">.</span><span class="n">similarity</span><span class="p">(</span><span class="n">burgers</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>I like salty fries and hamburgers. &lt;-&gt; Fast food tastes very good. 0.77994864211694
salty fries &lt;-&gt; hamburgers 0.7304624
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">word_vector</span> <span class="o">=</span> <span class="n">nlp</span><span class="p">(</span><span class="n">computer</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Load Google&#39;s pre-trained Word2Vec model.</span>
<span class="c1"># You can download them from here:</span>
<span class="c1"># https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz</span>
<span class="kn">from</span> <span class="nn">gensim.models</span> <span class="kn">import</span> <span class="n">KeyedVectors</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">KeyedVectors</span><span class="o">.</span><span class="n">load_word2vec_format</span><span class="p">(</span>
    <span class="s2">&quot;data/GoogleNews-vectors-negative300.bin&quot;</span><span class="p">,</span> <span class="n">binary</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Size of vocabulary: &quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">vocab</span><span class="p">))</span>
<span class="n">word_pairs</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">(</span><span class="s2">&quot;height&quot;</span><span class="p">,</span> <span class="s2">&quot;tall&quot;</span><span class="p">),</span>
    <span class="p">(</span><span class="s2">&quot;pineapple&quot;</span><span class="p">,</span> <span class="s2">&quot;mango&quot;</span><span class="p">),</span>
    <span class="p">(</span><span class="s2">&quot;pineapple&quot;</span><span class="p">,</span> <span class="s2">&quot;juice&quot;</span><span class="p">),</span>
    <span class="p">(</span><span class="s2">&quot;sun&quot;</span><span class="p">,</span> <span class="s2">&quot;robot&quot;</span><span class="p">),</span>
    <span class="p">(</span><span class="s2">&quot;GPU&quot;</span><span class="p">,</span> <span class="s2">&quot;lion&quot;</span><span class="p">),</span>
<span class="p">]</span>
<span class="k">for</span> <span class="n">pair</span> <span class="ow">in</span> <span class="n">word_pairs</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span>
        <span class="s2">&quot;The similarity between </span><span class="si">%s</span><span class="s2"> and </span><span class="si">%s</span><span class="s2"> is </span><span class="si">%0.3f</span><span class="s2">&quot;</span>
        <span class="o">%</span> <span class="p">(</span><span class="n">pair</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">pair</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">model</span><span class="o">.</span><span class="n">similarity</span><span class="p">(</span><span class="n">pair</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">pair</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
    <span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">AttributeError</span><span class="g g-Whitespace">                            </span>Traceback (most recent call last)
<span class="o">/</span><span class="n">var</span><span class="o">/</span><span class="n">folders</span><span class="o">/</span><span class="mi">80</span><span class="o">/</span><span class="n">kr9rkqfj4w78h49djkz8yy9r0000gp</span><span class="o">/</span><span class="n">T</span><span class="o">/</span><span class="n">ipykernel_15278</span><span class="o">/</span><span class="mf">3295700205.</span><span class="n">py</span> <span class="ow">in</span> <span class="o">&lt;</span><span class="n">module</span><span class="o">&gt;</span>
<span class="ne">----&gt; </span><span class="mi">1</span> <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Size of vocabulary: &quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">vocab</span><span class="p">))</span>
<span class="g g-Whitespace">      </span><span class="mi">2</span> <span class="n">word_pairs</span> <span class="o">=</span> <span class="p">[</span>
<span class="g g-Whitespace">      </span><span class="mi">3</span>     <span class="p">(</span><span class="s2">&quot;height&quot;</span><span class="p">,</span> <span class="s2">&quot;tall&quot;</span><span class="p">),</span>
<span class="g g-Whitespace">      </span><span class="mi">4</span>     <span class="p">(</span><span class="s2">&quot;pineapple&quot;</span><span class="p">,</span> <span class="s2">&quot;mango&quot;</span><span class="p">),</span>
<span class="g g-Whitespace">      </span><span class="mi">5</span>     <span class="p">(</span><span class="s2">&quot;pineapple&quot;</span><span class="p">,</span> <span class="s2">&quot;juice&quot;</span><span class="p">),</span>

<span class="nn">~/opt/miniconda3/envs/cpsc330/lib/python3.9/site-packages/gensim/models/keyedvectors.py</span> in <span class="ni">vocab</span><span class="nt">(self)</span>
<span class="g g-Whitespace">    </span><span class="mi">643</span>     <span class="nd">@property</span>
<span class="g g-Whitespace">    </span><span class="mi">644</span>     <span class="k">def</span> <span class="nf">vocab</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="ne">--&gt; </span><span class="mi">645</span>         <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">646</span>             <span class="s2">&quot;The vocab attribute was removed from KeyedVector in Gensim 4.0.0.</span><span class="se">\n</span><span class="s2">&quot;</span>
<span class="g g-Whitespace">    </span><span class="mi">647</span>             <span class="s2">&quot;Use KeyedVector&#39;s .key_to_index dict, .index_to_key list, and methods &quot;</span>

<span class="ne">AttributeError</span>: The vocab attribute was removed from KeyedVector in Gensim 4.0.0.
<span class="n">Use</span> <span class="n">KeyedVector</span><span class="s1">&#39;s .key_to_index dict, .index_to_key list, and methods .get_vecattr(key, attr) and .set_vecattr(key, attr, new_val) instead.</span>
<span class="n">See</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">github</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">RaRe</span><span class="o">-</span><span class="n">Technologies</span><span class="o">/</span><span class="n">gensim</span><span class="o">/</span><span class="n">wiki</span><span class="o">/</span><span class="n">Migrating</span><span class="o">-</span><span class="n">from</span><span class="o">-</span><span class="n">Gensim</span><span class="o">-</span><span class="mf">3.</span><span class="n">x</span><span class="o">-</span><span class="n">to</span><span class="o">-</span><span class="mi">4</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="s2">&quot;UBC&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[(&#39;UVic&#39;, 0.7886475920677185),
 (&#39;SFU&#39;, 0.7588528394699097),
 (&#39;Simon_Fraser&#39;, 0.7356574535369873),
 (&#39;UFV&#39;, 0.6880435943603516),
 (&#39;VIU&#39;, 0.6778583526611328),
 (&#39;Kwantlen&#39;, 0.6771429181098938),
 (&#39;UBCO&#39;, 0.6734487414360046),
 (&#39;UPEI&#39;, 0.6731126308441162),
 (&#39;UBC_Okanagan&#39;, 0.6709133982658386),
 (&#39;Lakehead_University&#39;, 0.6622507572174072)]
</pre></div>
</div>
</div>
</div>
<div class="section" id="activity-2-try-out-word-similarity-with-the-code-above-4-mins">
<h4>Activity 2: Try out word similarity with the code above (~4 mins)<a class="headerlink" href="#activity-2-try-out-word-similarity-with-the-code-above-4-mins" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li><p>Take a moment here and try out some words to find most similar words. To get you started here are some words: <em>Vancouver, bread, Computer_Science</em></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Let&#39;s build a word2vec model on our tiny machu picchu corpus</span>
<span class="c1"># Just for demonstration. Won&#39;t give any meaningful relationships because</span>
<span class="c1"># of the size of our corpus.</span>
<span class="kn">import</span> <span class="nn">gensim</span>
<span class="kn">from</span> <span class="nn">gensim.models</span> <span class="kn">import</span> <span class="n">Word2Vec</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">Word2Vec</span><span class="p">(</span><span class="n">pp_corpus</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">window</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">min_count</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># How does a learned dense word vector look like?</span>
<span class="n">model</span><span class="o">.</span><span class="n">wv</span><span class="p">[</span><span class="s2">&quot;tall&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">:</span><span class="mi">10</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">NameError</span><span class="g g-Whitespace">                                 </span>Traceback (most recent call last)
<span class="o">/</span><span class="n">var</span><span class="o">/</span><span class="n">folders</span><span class="o">/</span><span class="mi">80</span><span class="o">/</span><span class="n">kr9rkqfj4w78h49djkz8yy9r0000gp</span><span class="o">/</span><span class="n">T</span><span class="o">/</span><span class="n">ipykernel_15278</span><span class="o">/</span><span class="mf">109332215.</span><span class="n">py</span> <span class="ow">in</span> <span class="o">&lt;</span><span class="n">module</span><span class="o">&gt;</span>
<span class="g g-Whitespace">      </span><span class="mi">5</span> <span class="kn">from</span> <span class="nn">gensim.models</span> <span class="kn">import</span> <span class="n">Word2Vec</span>
<span class="g g-Whitespace">      </span><span class="mi">6</span> 
<span class="ne">----&gt; </span><span class="mi">7</span> <span class="n">model</span> <span class="o">=</span> <span class="n">Word2Vec</span><span class="p">(</span><span class="n">pp_corpus</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">window</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">min_count</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="g g-Whitespace">      </span><span class="mi">8</span> 
<span class="g g-Whitespace">      </span><span class="mi">9</span> <span class="c1"># How does a learned dense word vector look like?</span>

<span class="ne">NameError</span>: name &#39;pp_corpus&#39; is not defined
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="other-popular-methods-to-get-embeddings">
<h3>Other popular methods to get embeddings<a class="headerlink" href="#other-popular-methods-to-get-embeddings" title="Permalink to this headline">¶</a></h3>
</div>
<div class="section" id="fasttext">
<h3><a class="reference external" href="https://fasttext.cc/">fastText</a><a class="headerlink" href="#fasttext" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>NLP library by Facebook research</p></li>
<li><p>Includes an algorithm which is an extension to Word2Vec</p></li>
<li><p>Helps deal with unknown words elegantly</p></li>
<li><p>Breaks words into several n-gram subwords</p></li>
<li><p>Example: trigram sub-words for <em>berry</em> are <em>ber</em>, <em>err</em>, <em>rry</em>)</p></li>
<li><p>Embedding(<em>berry</em>) = embedding(<em>ber</em>) + embedding(<em>err</em>) + embedding(rry)</p></li>
</ul>
</div>
<div class="section" id="glove-global-vectors-for-word-representation">
<h3><a class="reference external" href="https://nlp.stanford.edu/projects/glove/">GloVe: Global Vectors for Word Representation</a><a class="headerlink" href="#glove-global-vectors-for-word-representation" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Starts with the co-occurrence matrix</p>
<ul>
<li><p>Co-occurrence can be interpreted as an indicator of semantic proximity of words</p></li>
</ul>
</li>
<li><p>Takes advantage of global count statistics</p></li>
<li><p>Predicts co-occurrence ratios</p></li>
<li><p>Loss based on word frequency</p></li>
</ul>
</div>
<div class="section" id="pre-trained-embeddings">
<h3>Pre-trained embeddings<a class="headerlink" href="#pre-trained-embeddings" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Training embeddings is computationally expensive</p></li>
<li><p>For typical corpora, the vocabulary size is greater than 100,000.</p></li>
<li><p>If the size of embeddings is 300, the number of parameters of the model is <span class="math notranslate nohighlight">\(2 \times 30,000,000\)</span>.</p></li>
<li><p>So people have trained embeddings on huge corpora and made them available.</p></li>
</ul>
</div>
<div class="section" id="id4">
<h3>Pre-trained embeddings<a class="headerlink" href="#id4" title="Permalink to this headline">¶</a></h3>
<p>A number of pre-trained word embeddings are available. The most popular ones are:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://code.google.com/archive/p/word2vec/">Word2Vec</a></p>
<ul>
<li><p>trained on several corpora using the word2vec algorithm</p></li>
</ul>
</li>
<li><p><a class="reference external" href="https://wikipedia2vec.github.io/wikipedia2vec/pretrained/">wikipedia2vec</a></p>
<ul>
<li><p>pretrained embeddings for 12 languages</p></li>
</ul>
</li>
<li><p><a class="reference external" href="https://nlp.stanford.edu/projects/glove/">GloVe</a></p>
<ul>
<li><p>trained using <a class="reference external" href="https://nlp.stanford.edu/pubs/glove.pdf">the GloVe algorithm</a></p></li>
<li><p>published by Stanford University</p></li>
</ul>
</li>
<li><p><a class="reference external" href="https://fasttext.cc/docs/en/pretrained-vectors.html">fastText pre-trained embeddings for 294 languages</a></p>
<ul>
<li><p>trained using <a class="reference external" href="http://aclweb.org/anthology/Q17-1010">the fastText algorithm</a></p></li>
<li><p>published by Facebook</p></li>
</ul>
</li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Load Google&#39;s pre-trained Word2Vec model.</span>
<span class="c1"># wget -c &quot;https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz&quot;</span>
<span class="kn">from</span> <span class="nn">gensim.models</span> <span class="kn">import</span> <span class="n">KeyedVectors</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">KeyedVectors</span><span class="o">.</span><span class="n">load_word2vec_format</span><span class="p">(</span>
    <span class="s2">&quot;data/GoogleNews-vectors-negative300.bin&quot;</span><span class="p">,</span> <span class="n">binary</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Size of vocabulary: &quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">vocab</span><span class="p">))</span>
<span class="n">word_pairs</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">(</span><span class="s2">&quot;height&quot;</span><span class="p">,</span> <span class="s2">&quot;tall&quot;</span><span class="p">),</span>
    <span class="p">(</span><span class="s2">&quot;pineapple&quot;</span><span class="p">,</span> <span class="s2">&quot;mango&quot;</span><span class="p">),</span>
    <span class="p">(</span><span class="s2">&quot;pineapple&quot;</span><span class="p">,</span> <span class="s2">&quot;juice&quot;</span><span class="p">),</span>
    <span class="p">(</span><span class="s2">&quot;sun&quot;</span><span class="p">,</span> <span class="s2">&quot;robot&quot;</span><span class="p">),</span>
    <span class="p">(</span><span class="s2">&quot;GPU&quot;</span><span class="p">,</span> <span class="s2">&quot;lion&quot;</span><span class="p">),</span>
<span class="p">]</span>
<span class="k">for</span> <span class="n">pair</span> <span class="ow">in</span> <span class="n">word_pairs</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span>
        <span class="s2">&quot;The similarity between </span><span class="si">%s</span><span class="s2"> and </span><span class="si">%s</span><span class="s2"> is </span><span class="si">%0.3f</span><span class="s2">&quot;</span>
        <span class="o">%</span> <span class="p">(</span><span class="n">pair</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">pair</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">model</span><span class="o">.</span><span class="n">similarity</span><span class="p">(</span><span class="n">pair</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">pair</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
    <span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">AttributeError</span><span class="g g-Whitespace">                            </span>Traceback (most recent call last)
<span class="o">/</span><span class="n">var</span><span class="o">/</span><span class="n">folders</span><span class="o">/</span><span class="mi">80</span><span class="o">/</span><span class="n">kr9rkqfj4w78h49djkz8yy9r0000gp</span><span class="o">/</span><span class="n">T</span><span class="o">/</span><span class="n">ipykernel_15278</span><span class="o">/</span><span class="mf">3295700205.</span><span class="n">py</span> <span class="ow">in</span> <span class="o">&lt;</span><span class="n">module</span><span class="o">&gt;</span>
<span class="ne">----&gt; </span><span class="mi">1</span> <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Size of vocabulary: &quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">vocab</span><span class="p">))</span>
<span class="g g-Whitespace">      </span><span class="mi">2</span> <span class="n">word_pairs</span> <span class="o">=</span> <span class="p">[</span>
<span class="g g-Whitespace">      </span><span class="mi">3</span>     <span class="p">(</span><span class="s2">&quot;height&quot;</span><span class="p">,</span> <span class="s2">&quot;tall&quot;</span><span class="p">),</span>
<span class="g g-Whitespace">      </span><span class="mi">4</span>     <span class="p">(</span><span class="s2">&quot;pineapple&quot;</span><span class="p">,</span> <span class="s2">&quot;mango&quot;</span><span class="p">),</span>
<span class="g g-Whitespace">      </span><span class="mi">5</span>     <span class="p">(</span><span class="s2">&quot;pineapple&quot;</span><span class="p">,</span> <span class="s2">&quot;juice&quot;</span><span class="p">),</span>

<span class="nn">~/opt/miniconda3/envs/cpsc330/lib/python3.9/site-packages/gensim/models/keyedvectors.py</span> in <span class="ni">vocab</span><span class="nt">(self)</span>
<span class="g g-Whitespace">    </span><span class="mi">643</span>     <span class="nd">@property</span>
<span class="g g-Whitespace">    </span><span class="mi">644</span>     <span class="k">def</span> <span class="nf">vocab</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="ne">--&gt; </span><span class="mi">645</span>         <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">646</span>             <span class="s2">&quot;The vocab attribute was removed from KeyedVector in Gensim 4.0.0.</span><span class="se">\n</span><span class="s2">&quot;</span>
<span class="g g-Whitespace">    </span><span class="mi">647</span>             <span class="s2">&quot;Use KeyedVector&#39;s .key_to_index dict, .index_to_key list, and methods &quot;</span>

<span class="ne">AttributeError</span>: The vocab attribute was removed from KeyedVector in Gensim 4.0.0.
<span class="n">Use</span> <span class="n">KeyedVector</span><span class="s1">&#39;s .key_to_index dict, .index_to_key list, and methods .get_vecattr(key, attr) and .set_vecattr(key, attr, new_val) instead.</span>
<span class="n">See</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">github</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">RaRe</span><span class="o">-</span><span class="n">Technologies</span><span class="o">/</span><span class="n">gensim</span><span class="o">/</span><span class="n">wiki</span><span class="o">/</span><span class="n">Migrating</span><span class="o">-</span><span class="n">from</span><span class="o">-</span><span class="n">Gensim</span><span class="o">-</span><span class="mf">3.</span><span class="n">x</span><span class="o">-</span><span class="n">to</span><span class="o">-</span><span class="mi">4</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="finding-similar-words">
<h3>Finding similar words<a class="headerlink" href="#finding-similar-words" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Given word <span class="math notranslate nohighlight">\(w\)</span>, search in the vector space for the word closest to <span class="math notranslate nohighlight">\(w\)</span> as measured by cosine distance.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="s2">&quot;UBC&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Captures different contracted forms and mispelled occurrences</span>
<span class="n">model</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="s2">&quot;information&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="success-of-word2vec">
<h3>Success of Word2Vec<a class="headerlink" href="#success-of-word2vec" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Able to capture complex relationships between words.</p></li>
<li><p>Example: What is the word that is similar to <strong>WOMAN</strong> in the same sense as <strong>KING</strong> is similar to <strong>MAN</strong>?</p></li>
<li><p>Perform a simple algebraic operations with the vector representation of words.
<span class="math notranslate nohighlight">\(\vec{X} = \vec{\text{KING}} − \vec{\text{MAN}} + \vec{\text{WOMAN}}\)</span></p></li>
<li><p>Search in the vector space for the word closest to <span class="math notranslate nohighlight">\(\vec{X}\)</span> measured by cosine distance.</p></li>
</ul>
<a class="reference internal image-reference" href="../_images/word_analogies1.png"><img alt="../_images/word_analogies1.png" src="../_images/word_analogies1.png" style="width: 500px; height: 500px;" /></a>
<p>(Credit: Mikolov et al. 2013)</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">analogy</span><span class="p">(</span><span class="n">word1</span><span class="p">,</span> <span class="n">word2</span><span class="p">,</span> <span class="n">word3</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns analogy word using the given model.</span>

<span class="sd">    Parameters</span>
<span class="sd">    --------------</span>
<span class="sd">    word1 : (str)</span>
<span class="sd">        word1 in the analogy relation</span>
<span class="sd">    word2 : (str)</span>
<span class="sd">        word2 in the analogy relation</span>
<span class="sd">    word3 : (str)</span>
<span class="sd">        word3 in the analogy relation</span>
<span class="sd">    model :</span>
<span class="sd">        word embedding model</span>

<span class="sd">    Returns</span>
<span class="sd">    ---------------</span>
<span class="sd">        pd.dataframe</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">%s</span><span class="s2"> : </span><span class="si">%s</span><span class="s2"> :: </span><span class="si">%s</span><span class="s2"> : ?&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">word1</span><span class="p">,</span> <span class="n">word2</span><span class="p">,</span> <span class="n">word3</span><span class="p">))</span>
    <span class="n">sim_words</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="n">positive</span><span class="o">=</span><span class="p">[</span><span class="n">word3</span><span class="p">,</span> <span class="n">word2</span><span class="p">],</span> <span class="n">negative</span><span class="o">=</span><span class="p">[</span><span class="n">word1</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">sim_words</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;Analogy word&quot;</span><span class="p">,</span> <span class="s2">&quot;Score&quot;</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">analogy</span><span class="p">(</span><span class="s2">&quot;man&quot;</span><span class="p">,</span> <span class="s2">&quot;king&quot;</span><span class="p">,</span> <span class="s2">&quot;woman&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>man : king :: woman : ?
</pre></div>
</div>
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Analogy word</th>
      <th>Score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>queen</td>
      <td>0.711819</td>
    </tr>
    <tr>
      <th>1</th>
      <td>monarch</td>
      <td>0.618967</td>
    </tr>
    <tr>
      <th>2</th>
      <td>princess</td>
      <td>0.590243</td>
    </tr>
    <tr>
      <th>3</th>
      <td>crown_prince</td>
      <td>0.549946</td>
    </tr>
    <tr>
      <th>4</th>
      <td>prince</td>
      <td>0.537732</td>
    </tr>
    <tr>
      <th>5</th>
      <td>kings</td>
      <td>0.523684</td>
    </tr>
    <tr>
      <th>6</th>
      <td>Queen_Consort</td>
      <td>0.523595</td>
    </tr>
    <tr>
      <th>7</th>
      <td>queens</td>
      <td>0.518113</td>
    </tr>
    <tr>
      <th>8</th>
      <td>sultan</td>
      <td>0.509859</td>
    </tr>
    <tr>
      <th>9</th>
      <td>monarchy</td>
      <td>0.508741</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">analogy</span><span class="p">(</span><span class="s2">&quot;Montreal&quot;</span><span class="p">,</span> <span class="s2">&quot;Canadiens&quot;</span><span class="p">,</span> <span class="s2">&quot;Vancouver&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Montreal : Canadiens :: Vancouver : ?
</pre></div>
</div>
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Analogy word</th>
      <th>Score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Canucks</td>
      <td>0.821327</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Vancouver_Canucks</td>
      <td>0.750401</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Calgary_Flames</td>
      <td>0.705470</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Leafs</td>
      <td>0.695783</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Maple_Leafs</td>
      <td>0.691617</td>
    </tr>
    <tr>
      <th>5</th>
      <td>Thrashers</td>
      <td>0.687504</td>
    </tr>
    <tr>
      <th>6</th>
      <td>Avs</td>
      <td>0.681716</td>
    </tr>
    <tr>
      <th>7</th>
      <td>Sabres</td>
      <td>0.665307</td>
    </tr>
    <tr>
      <th>8</th>
      <td>Blackhawks</td>
      <td>0.664625</td>
    </tr>
    <tr>
      <th>9</th>
      <td>Habs</td>
      <td>0.661023</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">### Recall the title of today&#39;s lesson</span>
<span class="n">analogy</span><span class="p">(</span><span class="s2">&quot;Toronto&quot;</span><span class="p">,</span> <span class="s2">&quot;UofT&quot;</span><span class="p">,</span> <span class="s2">&quot;Vancouver&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Toronto : UofT :: Vancouver : ?
</pre></div>
</div>
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Analogy word</th>
      <th>Score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>SFU</td>
      <td>0.579245</td>
    </tr>
    <tr>
      <th>1</th>
      <td>UVic</td>
      <td>0.576921</td>
    </tr>
    <tr>
      <th>2</th>
      <td>UBC</td>
      <td>0.571431</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Simon_Fraser</td>
      <td>0.543464</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Langara_College</td>
      <td>0.541347</td>
    </tr>
    <tr>
      <th>5</th>
      <td>UVIC</td>
      <td>0.520495</td>
    </tr>
    <tr>
      <th>6</th>
      <td>Grant_MacEwan</td>
      <td>0.517273</td>
    </tr>
    <tr>
      <th>7</th>
      <td>UFV</td>
      <td>0.514150</td>
    </tr>
    <tr>
      <th>8</th>
      <td>Ubyssey</td>
      <td>0.510421</td>
    </tr>
    <tr>
      <th>9</th>
      <td>Kwantlen</td>
      <td>0.503807</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
</div>
<div class="section" id="examples-of-semantic-and-syntactic-relationships">
<h3>Examples of semantic and syntactic relationships<a class="headerlink" href="#examples-of-semantic-and-syntactic-relationships" title="Permalink to this headline">¶</a></h3>
<a class="reference internal image-reference" href="../_images/word_analogies2.png"><img alt="../_images/word_analogies2.png" src="../_images/word_analogies2.png" style="width: 800px; height: 800px;" /></a>
<p>(Credit: Mikolov 2013)</p>
</div>
<div class="section" id="implicit-biases-and-stereotypes-in-word-embeddings">
<h3>Implicit biases and stereotypes in word embeddings<a class="headerlink" href="#implicit-biases-and-stereotypes-in-word-embeddings" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Reflect gender stereotypes present in broader society.</p></li>
<li><p>They may also amplify these stereotypes because of their widespread usage.</p></li>
<li><p>See the paper <a class="reference external" href="http://papers.nips.cc/paper/6228-man-is-to-computer-programmer-as-woman-is-to-homemaker-debiasing-word-embeddings.pdf">Man is to Computer Programmer as Woman is to …</a>.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">analogy</span><span class="p">(</span><span class="s2">&quot;man&quot;</span><span class="p">,</span> <span class="s2">&quot;computer_programmer&quot;</span><span class="p">,</span> <span class="s2">&quot;woman&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">NameError</span><span class="g g-Whitespace">                                 </span>Traceback (most recent call last)
<span class="o">/</span><span class="n">var</span><span class="o">/</span><span class="n">folders</span><span class="o">/</span><span class="mi">80</span><span class="o">/</span><span class="n">kr9rkqfj4w78h49djkz8yy9r0000gp</span><span class="o">/</span><span class="n">T</span><span class="o">/</span><span class="n">ipykernel_15278</span><span class="o">/</span><span class="mf">3289983101.</span><span class="n">py</span> <span class="ow">in</span> <span class="o">&lt;</span><span class="n">module</span><span class="o">&gt;</span>
<span class="ne">----&gt; </span><span class="mi">1</span> <span class="n">analogy</span><span class="p">(</span><span class="s2">&quot;man&quot;</span><span class="p">,</span> <span class="s2">&quot;computer_programmer&quot;</span><span class="p">,</span> <span class="s2">&quot;woman&quot;</span><span class="p">)</span>

<span class="ne">NameError</span>: name &#39;analogy&#39; is not defined
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="representing-documents-using-word-embeddings">
<h3>Representing documents using word embeddings<a class="headerlink" href="#representing-documents-using-word-embeddings" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Assuming that we have reasonable representations of words.</p></li>
<li><p>How do we represent meaning of paragraphs or documents?</p></li>
<li><p>Two simple approaches</p>
<ul>
<li><p>Averaging embeddings</p></li>
<li><p>Concatenating embeddings</p></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="averaging-embeddings">
<h3>Averaging embeddings<a class="headerlink" href="#averaging-embeddings" title="Permalink to this headline">¶</a></h3>
<blockquote>
All empty promises
</blockquote>
<p><span class="math notranslate nohighlight">\((embedding(all) + embedding(empty) + embedding(promise))/3\)</span></p>
</div>
<div class="section" id="average-embeddings-with-spacy">
<h3>Average embeddings with spaCy<a class="headerlink" href="#average-embeddings-with-spacy" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>We can do this conveniently with <a class="reference external" href="https://spacy.io/usage/linguistic-features#vectors-similarity">spaCy</a>.</p></li>
<li><p>We need <code class="docutils literal notranslate"><span class="pre">en_core_web_md</span></code> model to access word vectors.</p></li>
<li><p>You can download the model by going to command line and in your course <code class="docutils literal notranslate"><span class="pre">conda</span></code> environment and download <code class="docutils literal notranslate"><span class="pre">en_core_web_md</span></code> as follows.</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">conda</span> <span class="n">activate</span> <span class="n">cpsc330</span>
<span class="n">python</span> <span class="o">-</span><span class="n">m</span> <span class="n">spacy</span> <span class="n">download</span> <span class="n">en_core_web_md</span>
</pre></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">spacy</span>

<span class="n">nlp</span> <span class="o">=</span> <span class="n">spacy</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;en_core_web_md&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We can access word vectors for individual words in <code class="docutils literal notranslate"><span class="pre">spaCy</span></code> as follows.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">nlp</span><span class="p">(</span><span class="s2">&quot;empty&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">vector</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">10</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([ 1.0127  , -0.20527 , -0.24555 , -0.076636,  0.11981 ,  0.21642 ,
       -0.55584 ,  0.020206,  0.39778 ,  1.7218  ], dtype=float32)
</pre></div>
</div>
</div>
</div>
<p>We can get average embeddings for a sentence or a document in <code class="docutils literal notranslate"><span class="pre">spaCy</span></code> as follows:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">s</span> <span class="o">=</span> <span class="s2">&quot;All empty promises&quot;</span>
<span class="n">doc</span> <span class="o">=</span> <span class="n">nlp</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
<span class="n">avg_sent_emb</span> <span class="o">=</span> <span class="n">doc</span><span class="o">.</span><span class="n">vector</span>
<span class="nb">print</span><span class="p">(</span><span class="n">avg_sent_emb</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Word vector for: </span><span class="si">{}</span><span class="se">\n</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">((</span><span class="n">s</span><span class="p">),</span> <span class="p">(</span><span class="n">avg_sent_emb</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">10</span><span class="p">])))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(300,)
Word vector for: All empty promises
[ 0.28903252 -0.09684668 -0.11497    -0.26554868  0.01983    -0.111594
 -0.10229333  0.126915    0.17705734  2.1837332 ]
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="similarity-between-documents">
<h3>Similarity between documents<a class="headerlink" href="#similarity-between-documents" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>We can also get similarity between documents as follows.</p></li>
<li><p>Note that this is based on average embeddings of each sentence.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">doc1</span> <span class="o">=</span> <span class="n">nlp</span><span class="p">(</span><span class="s2">&quot;Deep learning is very popular these days.&quot;</span><span class="p">)</span>
<span class="n">doc2</span> <span class="o">=</span> <span class="n">nlp</span><span class="p">(</span><span class="s2">&quot;Machine learning is dominated by neural networks.&quot;</span><span class="p">)</span>
<span class="n">doc3</span> <span class="o">=</span> <span class="n">nlp</span><span class="p">(</span><span class="s2">&quot;A home-made fresh bread is baking in the oven.&quot;</span><span class="p">)</span>

<span class="c1"># Similarity of two documents</span>
<span class="nb">print</span><span class="p">(</span><span class="n">doc1</span><span class="p">,</span> <span class="s2">&quot;&lt;-&gt;&quot;</span><span class="p">,</span> <span class="n">doc2</span><span class="p">,</span> <span class="n">doc1</span><span class="o">.</span><span class="n">similarity</span><span class="p">(</span><span class="n">doc2</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">doc2</span><span class="p">,</span> <span class="s2">&quot;&lt;-&gt;&quot;</span><span class="p">,</span> <span class="n">doc3</span><span class="p">,</span> <span class="n">doc2</span><span class="o">.</span><span class="n">similarity</span><span class="p">(</span><span class="n">doc3</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Deep learning is very popular these days. &lt;-&gt; Machine learning is dominated by neural networks. 0.7564516644025884
Machine learning is dominated by neural networks. &lt;-&gt; A home-made fresh bread is baking in the oven. 0.5939755332427483
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Do these scores make sense?</p></li>
<li><p>There are no common words, but we are still able to identify that doc1 and doc2 are more similar that doc2 and doc3.</p></li>
<li><p>You can use such average embedding representation in text classification tasks.</p></li>
</ul>
</div>
<div class="section" id="airline-sentiment-analysis-using-average-embedding-representation">
<h3>Airline sentiment analysis using average embedding representation<a class="headerlink" href="#airline-sentiment-analysis-using-average-embedding-representation" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Let’s try average embedding representation for airline sentiment analysis.</p></li>
<li><p>We used this dataset last week so you should already have it in the data directory. If not you can download it <a class="reference external" href="https://www.kaggle.com/jaskarancr/airline-sentiment-dataset">here</a>.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;data/Airline-Sentiment-2-w-AA.csv&quot;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s2">&quot;ISO-8859-1&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_validate</span><span class="p">,</span> <span class="n">train_test_split</span>

<span class="n">train_df</span><span class="p">,</span> <span class="n">test_df</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span> <span class="o">=</span> <span class="n">train_df</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">],</span> <span class="n">train_df</span><span class="p">[</span><span class="s2">&quot;airline_sentiment&quot;</span><span class="p">]</span>
<span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">test_df</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">],</span> <span class="n">test_df</span><span class="p">[</span><span class="s2">&quot;airline_sentiment&quot;</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">train_df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>_unit_id</th>
      <th>_golden</th>
      <th>_unit_state</th>
      <th>_trusted_judgments</th>
      <th>_last_judgment_at</th>
      <th>airline_sentiment</th>
      <th>airline_sentiment:confidence</th>
      <th>negativereason</th>
      <th>negativereason:confidence</th>
      <th>airline</th>
      <th>airline_sentiment_gold</th>
      <th>name</th>
      <th>negativereason_gold</th>
      <th>retweet_count</th>
      <th>text</th>
      <th>tweet_coord</th>
      <th>tweet_created</th>
      <th>tweet_id</th>
      <th>tweet_location</th>
      <th>user_timezone</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>5789</th>
      <td>681455792</td>
      <td>False</td>
      <td>finalized</td>
      <td>3</td>
      <td>2/25/15 4:21</td>
      <td>negative</td>
      <td>1.0</td>
      <td>Can't Tell</td>
      <td>0.6667</td>
      <td>Southwest</td>
      <td>NaN</td>
      <td>mrssuperdimmock</td>
      <td>NaN</td>
      <td>0</td>
      <td>@SouthwestAir link doesn't work</td>
      <td>NaN</td>
      <td>2/19/15 18:53</td>
      <td>5.686040e+17</td>
      <td>Lake Arrowhead, CA</td>
      <td>Pacific Time (US &amp; Canada)</td>
    </tr>
    <tr>
      <th>8918</th>
      <td>681459957</td>
      <td>False</td>
      <td>finalized</td>
      <td>3</td>
      <td>2/25/15 9:45</td>
      <td>neutral</td>
      <td>1.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>Delta</td>
      <td>NaN</td>
      <td>labeles</td>
      <td>NaN</td>
      <td>0</td>
      <td>@JetBlue okayyyy. But I had huge irons on way ...</td>
      <td>NaN</td>
      <td>2/17/15 10:18</td>
      <td>5.677500e+17</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>11688</th>
      <td>681462990</td>
      <td>False</td>
      <td>finalized</td>
      <td>3</td>
      <td>2/25/15 9:53</td>
      <td>negative</td>
      <td>1.0</td>
      <td>Customer Service Issue</td>
      <td>0.6727</td>
      <td>US Airways</td>
      <td>NaN</td>
      <td>DropMeAnywhere</td>
      <td>NaN</td>
      <td>0</td>
      <td>@USAirways They're all reservations numbers an...</td>
      <td>[0.0, 0.0]</td>
      <td>2/17/15 14:50</td>
      <td>5.678190e+17</td>
      <td>Here, There and Everywhere</td>
      <td>Arizona</td>
    </tr>
    <tr>
      <th>413</th>
      <td>681448905</td>
      <td>False</td>
      <td>finalized</td>
      <td>3</td>
      <td>2/25/15 10:10</td>
      <td>neutral</td>
      <td>1.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>Virgin America</td>
      <td>NaN</td>
      <td>jsamaudio</td>
      <td>NaN</td>
      <td>0</td>
      <td>@VirginAmerica no A's channel this year?</td>
      <td>NaN</td>
      <td>2/18/15 12:25</td>
      <td>5.681440e+17</td>
      <td>St. Francis (Calif.)</td>
      <td>Pacific Time (US &amp; Canada)</td>
    </tr>
    <tr>
      <th>4135</th>
      <td>681454122</td>
      <td>False</td>
      <td>finalized</td>
      <td>3</td>
      <td>2/25/15 10:08</td>
      <td>negative</td>
      <td>1.0</td>
      <td>Bad Flight</td>
      <td>0.3544</td>
      <td>United</td>
      <td>NaN</td>
      <td>CajunSQL</td>
      <td>NaN</td>
      <td>0</td>
      <td>@united missed it.  Incoming on time, then Sat...</td>
      <td>NaN</td>
      <td>2/17/15 14:20</td>
      <td>5.678110e+17</td>
      <td>Baton Rouge, LA</td>
      <td>NaN</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
</div>
<div class="section" id="bag-of-words-representation-for-sentiment-analysis">
<h3>Bag-of-words representation for sentiment analysis<a class="headerlink" href="#bag-of-words-representation-for-sentiment-analysis" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pipe</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span>
    <span class="n">CountVectorizer</span><span class="p">(</span><span class="n">stop_words</span><span class="o">=</span><span class="s2">&quot;english&quot;</span><span class="p">),</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">max_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
<span class="p">)</span>
<span class="n">pipe</span><span class="o">.</span><span class="n">named_steps</span><span class="p">[</span><span class="s2">&quot;countvectorizer&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">X_train_transformed</span> <span class="o">=</span> <span class="n">pipe</span><span class="o">.</span><span class="n">named_steps</span><span class="p">[</span><span class="s2">&quot;countvectorizer&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Data matrix shape:&quot;</span><span class="p">,</span> <span class="n">X_train_transformed</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">pipe</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Data matrix shape: (11712, 13064)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Train accuracy </span><span class="si">{:.2f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">pipe</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Test accuracy </span><span class="si">{:.2f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">pipe</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Train accuracy 0.94
Test accuracy 0.80
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="sentiment-analysis-with-average-embedding-representation">
<h3>Sentiment analysis with average embedding representation<a class="headerlink" href="#sentiment-analysis-with-average-embedding-representation" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Let’s see how can we get word vectors using <code class="docutils literal notranslate"><span class="pre">spaCy</span></code>.</p></li>
<li><p>Let’s create average embedding representation for each example.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_train_embeddings</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">([</span><span class="n">text</span><span class="o">.</span><span class="n">vector</span> <span class="k">for</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">nlp</span><span class="o">.</span><span class="n">pipe</span><span class="p">(</span><span class="n">X_train</span><span class="p">)])</span>
<span class="n">X_test_embeddings</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">([</span><span class="n">text</span><span class="o">.</span><span class="n">vector</span> <span class="k">for</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">nlp</span><span class="o">.</span><span class="n">pipe</span><span class="p">(</span><span class="n">X_test</span><span class="p">)])</span>
</pre></div>
</div>
</div>
</div>
<p>We have reduced dimensionality from 13,064 to 300!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_train_embeddings</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(11712, 300)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_train_embeddings</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
      <th>6</th>
      <th>7</th>
      <th>8</th>
      <th>9</th>
      <th>...</th>
      <th>290</th>
      <th>291</th>
      <th>292</th>
      <th>293</th>
      <th>294</th>
      <th>295</th>
      <th>296</th>
      <th>297</th>
      <th>298</th>
      <th>299</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>-0.102596</td>
      <td>0.089241</td>
      <td>-0.236022</td>
      <td>0.030403</td>
      <td>-0.183757</td>
      <td>0.188970</td>
      <td>-0.057322</td>
      <td>-0.194976</td>
      <td>-0.025315</td>
      <td>1.887160</td>
      <td>...</td>
      <td>-0.186171</td>
      <td>-0.001768</td>
      <td>0.088228</td>
      <td>0.095176</td>
      <td>0.195018</td>
      <td>-0.085129</td>
      <td>-0.113560</td>
      <td>-0.108034</td>
      <td>0.095647</td>
      <td>0.274220</td>
    </tr>
    <tr>
      <th>1</th>
      <td>-0.063396</td>
      <td>0.124589</td>
      <td>-0.122292</td>
      <td>-0.189419</td>
      <td>-0.101237</td>
      <td>0.002607</td>
      <td>0.032192</td>
      <td>-0.151461</td>
      <td>-0.055412</td>
      <td>1.796026</td>
      <td>...</td>
      <td>-0.183419</td>
      <td>-0.016063</td>
      <td>-0.005495</td>
      <td>-0.045449</td>
      <td>0.092108</td>
      <td>0.052184</td>
      <td>0.099207</td>
      <td>0.011452</td>
      <td>0.004731</td>
      <td>0.065721</td>
    </tr>
    <tr>
      <th>2</th>
      <td>-0.064436</td>
      <td>0.148900</td>
      <td>-0.170613</td>
      <td>-0.155509</td>
      <td>0.216841</td>
      <td>0.002881</td>
      <td>0.078774</td>
      <td>-0.123730</td>
      <td>-0.041174</td>
      <td>2.200783</td>
      <td>...</td>
      <td>-0.276544</td>
      <td>0.022344</td>
      <td>-0.029640</td>
      <td>0.003638</td>
      <td>0.070680</td>
      <td>-0.000402</td>
      <td>-0.029196</td>
      <td>-0.033838</td>
      <td>0.040326</td>
      <td>0.118550</td>
    </tr>
    <tr>
      <th>3</th>
      <td>-0.029201</td>
      <td>0.253583</td>
      <td>-0.020073</td>
      <td>0.035106</td>
      <td>0.027043</td>
      <td>-0.122413</td>
      <td>0.076435</td>
      <td>-0.253022</td>
      <td>-0.044745</td>
      <td>1.879360</td>
      <td>...</td>
      <td>-0.070406</td>
      <td>-0.012429</td>
      <td>-0.075428</td>
      <td>-0.106797</td>
      <td>0.218251</td>
      <td>-0.218904</td>
      <td>-0.120035</td>
      <td>-0.085280</td>
      <td>0.002369</td>
      <td>0.034781</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.077888</td>
      <td>0.213935</td>
      <td>-0.108230</td>
      <td>0.056640</td>
      <td>0.071779</td>
      <td>0.035684</td>
      <td>-0.034920</td>
      <td>-0.154641</td>
      <td>-0.004615</td>
      <td>1.916412</td>
      <td>...</td>
      <td>-0.063743</td>
      <td>0.002112</td>
      <td>-0.026977</td>
      <td>-0.102208</td>
      <td>0.037013</td>
      <td>-0.101183</td>
      <td>-0.003163</td>
      <td>0.025034</td>
      <td>-0.087998</td>
      <td>0.123148</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 300 columns</p>
</div></div></div>
</div>
</div>
<div class="section" id="sentiment-classification-using-average-embeddings">
<h3>Sentiment classification using average embeddings<a class="headerlink" href="#sentiment-classification-using-average-embeddings" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>What are the train and test accuracies with average word embedding representation?</p></li>
<li><p>The accuracy is a bit better with less overfitting.</p></li>
<li><p>Note that we are using <strong>transfer learning</strong> here.</p></li>
<li><p>The embeddings are trained on a completely different corpus.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lgr</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">max_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
<span class="n">lgr</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_embeddings</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Train accuracy </span><span class="si">{:.2f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">lgr</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train_embeddings</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Test accuracy </span><span class="si">{:.2f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">lgr</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test_embeddings</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Train accuracy 0.81
Test accuracy 0.81
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="spacy-s-pre-trained-embeddings">
<h3><code class="docutils literal notranslate"><span class="pre">spaCy</span></code>’s pre-trained embeddings<a class="headerlink" href="#spacy-s-pre-trained-embeddings" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">spaCy</span></code>’s pre-trained embeddings are trained on <a class="reference external" href="https://catalog.ldc.upenn.edu/LDC2013T19">OntoNotes corpus</a>.</p></li>
<li><p>This corpus has a collection of different styles of texts such as telephone conversations, newswire, newsgroups, broadcast news, broadcast conversation, weblogs, religious texts.</p></li>
<li><p>If you are working with a specific style (e.g., tweets or healthcare) you may want to use different pre-trained embeddings. In that case you’ll have to get average embeddings on your own.</p></li>
<li><p>You can check out the function I’m providing you in the notebook  for this.</p></li>
</ul>
<ul class="simple">
<li><p>Since, representing documents is so essential for text classification tasks, there are more advanced methods for document representation.</p></li>
<li><p>In homework 7, you also explore sentence embedding representation.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sentence_transformers</span> <span class="kn">import</span> <span class="n">SentenceTransformer</span>

<span class="n">embedder</span> <span class="o">=</span> <span class="n">SentenceTransformer</span><span class="p">(</span><span class="s2">&quot;paraphrase-distilroberta-base-v1&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">emb_sents</span> <span class="o">=</span> <span class="n">embedder</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="s2">&quot;all empty promises&quot;</span><span class="p">)</span>
<span class="n">emb_sents</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(768,)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">emb_train</span> <span class="o">=</span> <span class="n">embedder</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">train_df</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">())</span>
<span class="n">emb_train_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">emb_train</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">train_df</span><span class="o">.</span><span class="n">index</span><span class="p">)</span>
<span class="n">emb_train_df</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
      <th>6</th>
      <th>7</th>
      <th>8</th>
      <th>9</th>
      <th>...</th>
      <th>758</th>
      <th>759</th>
      <th>760</th>
      <th>761</th>
      <th>762</th>
      <th>763</th>
      <th>764</th>
      <th>765</th>
      <th>766</th>
      <th>767</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>5789</th>
      <td>-0.120494</td>
      <td>0.250262</td>
      <td>-0.022795</td>
      <td>-0.116368</td>
      <td>0.078650</td>
      <td>0.037357</td>
      <td>-0.251341</td>
      <td>0.321429</td>
      <td>-0.143984</td>
      <td>-0.123487</td>
      <td>...</td>
      <td>0.199151</td>
      <td>-0.150143</td>
      <td>0.167078</td>
      <td>-0.407671</td>
      <td>-0.066161</td>
      <td>0.049514</td>
      <td>0.019385</td>
      <td>-0.357601</td>
      <td>0.125996</td>
      <td>0.381073</td>
    </tr>
    <tr>
      <th>8918</th>
      <td>-0.182954</td>
      <td>0.118282</td>
      <td>0.066341</td>
      <td>-0.136099</td>
      <td>0.094947</td>
      <td>-0.121303</td>
      <td>0.069233</td>
      <td>-0.097500</td>
      <td>0.025739</td>
      <td>-0.367980</td>
      <td>...</td>
      <td>0.113612</td>
      <td>0.114661</td>
      <td>0.049926</td>
      <td>0.256736</td>
      <td>-0.118687</td>
      <td>-0.190720</td>
      <td>0.011986</td>
      <td>-0.141883</td>
      <td>-0.230142</td>
      <td>0.024899</td>
    </tr>
    <tr>
      <th>11688</th>
      <td>-0.032988</td>
      <td>0.630251</td>
      <td>-0.079516</td>
      <td>0.148981</td>
      <td>0.194709</td>
      <td>-0.226264</td>
      <td>-0.043630</td>
      <td>0.217398</td>
      <td>-0.010716</td>
      <td>0.069644</td>
      <td>...</td>
      <td>0.676791</td>
      <td>0.244484</td>
      <td>0.051042</td>
      <td>0.064099</td>
      <td>-0.146945</td>
      <td>0.090878</td>
      <td>-0.090059</td>
      <td>0.077212</td>
      <td>-0.209226</td>
      <td>0.308773</td>
    </tr>
    <tr>
      <th>413</th>
      <td>-0.119259</td>
      <td>0.172168</td>
      <td>0.098698</td>
      <td>0.319859</td>
      <td>0.415475</td>
      <td>0.248360</td>
      <td>-0.025923</td>
      <td>0.385350</td>
      <td>0.066414</td>
      <td>-0.334289</td>
      <td>...</td>
      <td>-0.128482</td>
      <td>-0.232446</td>
      <td>-0.077805</td>
      <td>0.181328</td>
      <td>0.123244</td>
      <td>-0.143693</td>
      <td>0.660456</td>
      <td>-0.048714</td>
      <td>0.204774</td>
      <td>0.163496</td>
    </tr>
    <tr>
      <th>4135</th>
      <td>0.094240</td>
      <td>0.360193</td>
      <td>0.213747</td>
      <td>0.363690</td>
      <td>0.275521</td>
      <td>0.134936</td>
      <td>-0.276319</td>
      <td>0.009336</td>
      <td>-0.021523</td>
      <td>-0.258992</td>
      <td>...</td>
      <td>0.474885</td>
      <td>0.242125</td>
      <td>0.294532</td>
      <td>0.279014</td>
      <td>0.037831</td>
      <td>0.089761</td>
      <td>-0.548748</td>
      <td>-0.049258</td>
      <td>0.154525</td>
      <td>0.141268</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>5218</th>
      <td>-0.204409</td>
      <td>-0.145290</td>
      <td>-0.064201</td>
      <td>0.213571</td>
      <td>-0.140225</td>
      <td>0.338555</td>
      <td>-0.148578</td>
      <td>0.224516</td>
      <td>-0.042963</td>
      <td>0.075930</td>
      <td>...</td>
      <td>-0.161949</td>
      <td>0.040582</td>
      <td>0.003971</td>
      <td>-0.152549</td>
      <td>-0.582907</td>
      <td>-0.126527</td>
      <td>0.060502</td>
      <td>-0.111495</td>
      <td>-0.097492</td>
      <td>0.199321</td>
    </tr>
    <tr>
      <th>12252</th>
      <td>0.108408</td>
      <td>0.438293</td>
      <td>0.216812</td>
      <td>-0.349289</td>
      <td>0.422689</td>
      <td>0.377761</td>
      <td>0.045198</td>
      <td>-0.034096</td>
      <td>0.427570</td>
      <td>-0.328272</td>
      <td>...</td>
      <td>0.257849</td>
      <td>-0.032362</td>
      <td>-0.275003</td>
      <td>0.080452</td>
      <td>-0.078975</td>
      <td>-0.049972</td>
      <td>-0.009762</td>
      <td>-0.314754</td>
      <td>-0.020774</td>
      <td>0.268777</td>
    </tr>
    <tr>
      <th>1346</th>
      <td>0.068411</td>
      <td>0.017591</td>
      <td>0.236154</td>
      <td>0.221446</td>
      <td>-0.103567</td>
      <td>0.055510</td>
      <td>0.062909</td>
      <td>0.067425</td>
      <td>-0.003504</td>
      <td>-0.157758</td>
      <td>...</td>
      <td>0.007711</td>
      <td>0.323297</td>
      <td>0.334638</td>
      <td>0.367042</td>
      <td>-0.068821</td>
      <td>0.063667</td>
      <td>-0.329991</td>
      <td>0.232331</td>
      <td>-0.184768</td>
      <td>-0.000683</td>
    </tr>
    <tr>
      <th>11646</th>
      <td>-0.091488</td>
      <td>-0.155709</td>
      <td>0.032391</td>
      <td>0.018313</td>
      <td>0.524998</td>
      <td>0.563933</td>
      <td>-0.080984</td>
      <td>0.097983</td>
      <td>-0.535285</td>
      <td>-0.377195</td>
      <td>...</td>
      <td>0.428014</td>
      <td>-0.144572</td>
      <td>0.045296</td>
      <td>-0.107935</td>
      <td>-0.135673</td>
      <td>-0.290019</td>
      <td>-0.137200</td>
      <td>-0.503395</td>
      <td>-0.042567</td>
      <td>-0.282591</td>
    </tr>
    <tr>
      <th>3582</th>
      <td>0.185626</td>
      <td>0.092904</td>
      <td>0.097085</td>
      <td>-0.174650</td>
      <td>-0.193584</td>
      <td>0.047294</td>
      <td>0.098216</td>
      <td>0.332670</td>
      <td>0.163098</td>
      <td>-0.135102</td>
      <td>...</td>
      <td>0.078530</td>
      <td>-0.030177</td>
      <td>0.391598</td>
      <td>0.073519</td>
      <td>-0.454038</td>
      <td>-0.244358</td>
      <td>-0.790682</td>
      <td>-0.607009</td>
      <td>-0.255162</td>
      <td>0.029779</td>
    </tr>
  </tbody>
</table>
<p>11712 rows × 768 columns</p>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">emb_test</span> <span class="o">=</span> <span class="n">embedder</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">test_df</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">())</span>
<span class="n">emb_test_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">emb_test</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">test_df</span><span class="o">.</span><span class="n">index</span><span class="p">)</span>
<span class="n">emb_test_df</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
      <th>6</th>
      <th>7</th>
      <th>8</th>
      <th>9</th>
      <th>...</th>
      <th>758</th>
      <th>759</th>
      <th>760</th>
      <th>761</th>
      <th>762</th>
      <th>763</th>
      <th>764</th>
      <th>765</th>
      <th>766</th>
      <th>767</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1671</th>
      <td>-0.002864</td>
      <td>0.217326</td>
      <td>0.124350</td>
      <td>-0.082548</td>
      <td>0.709688</td>
      <td>-0.582442</td>
      <td>0.257897</td>
      <td>0.169356</td>
      <td>0.248880</td>
      <td>-0.266686</td>
      <td>...</td>
      <td>0.501767</td>
      <td>0.095387</td>
      <td>0.340173</td>
      <td>0.087452</td>
      <td>-0.368359</td>
      <td>0.276195</td>
      <td>0.238676</td>
      <td>-0.219546</td>
      <td>0.066603</td>
      <td>0.256149</td>
    </tr>
    <tr>
      <th>10951</th>
      <td>-0.141048</td>
      <td>0.137934</td>
      <td>0.131319</td>
      <td>0.194774</td>
      <td>0.868205</td>
      <td>0.078791</td>
      <td>-0.131656</td>
      <td>0.036243</td>
      <td>-0.215749</td>
      <td>-0.291946</td>
      <td>...</td>
      <td>-0.056256</td>
      <td>-0.056040</td>
      <td>0.147341</td>
      <td>0.189665</td>
      <td>-0.357366</td>
      <td>0.061799</td>
      <td>-0.161922</td>
      <td>-0.278956</td>
      <td>-0.173722</td>
      <td>0.065324</td>
    </tr>
    <tr>
      <th>5382</th>
      <td>-0.252943</td>
      <td>0.527507</td>
      <td>-0.065608</td>
      <td>0.013467</td>
      <td>0.207989</td>
      <td>0.003881</td>
      <td>-0.066281</td>
      <td>0.253166</td>
      <td>0.021039</td>
      <td>0.290956</td>
      <td>...</td>
      <td>0.180685</td>
      <td>-0.042605</td>
      <td>-0.173794</td>
      <td>-0.079129</td>
      <td>-0.169160</td>
      <td>0.001317</td>
      <td>-0.142593</td>
      <td>-0.070816</td>
      <td>-0.208826</td>
      <td>0.400736</td>
    </tr>
    <tr>
      <th>3954</th>
      <td>0.054318</td>
      <td>0.096738</td>
      <td>0.113037</td>
      <td>0.032039</td>
      <td>0.493064</td>
      <td>-0.641102</td>
      <td>0.078760</td>
      <td>0.402187</td>
      <td>0.189743</td>
      <td>-0.089538</td>
      <td>...</td>
      <td>0.123879</td>
      <td>-0.285019</td>
      <td>-0.297771</td>
      <td>0.557171</td>
      <td>0.076168</td>
      <td>-0.029826</td>
      <td>-0.076095</td>
      <td>0.225454</td>
      <td>0.002134</td>
      <td>0.235429</td>
    </tr>
    <tr>
      <th>11193</th>
      <td>-0.065858</td>
      <td>0.223270</td>
      <td>0.507333</td>
      <td>0.266193</td>
      <td>0.104696</td>
      <td>-0.219555</td>
      <td>0.146247</td>
      <td>0.315650</td>
      <td>-0.126193</td>
      <td>-0.435462</td>
      <td>...</td>
      <td>0.163994</td>
      <td>0.207813</td>
      <td>-0.001871</td>
      <td>0.109391</td>
      <td>-0.166779</td>
      <td>-0.249199</td>
      <td>-0.525419</td>
      <td>-0.413066</td>
      <td>0.119939</td>
      <td>0.064297</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>5861</th>
      <td>0.077512</td>
      <td>0.322276</td>
      <td>0.026697</td>
      <td>-0.111392</td>
      <td>0.174208</td>
      <td>0.235201</td>
      <td>0.053888</td>
      <td>0.244941</td>
      <td>0.181625</td>
      <td>-0.226870</td>
      <td>...</td>
      <td>0.149843</td>
      <td>0.311337</td>
      <td>0.045975</td>
      <td>-0.572319</td>
      <td>-0.068256</td>
      <td>0.217745</td>
      <td>-0.056509</td>
      <td>-0.355174</td>
      <td>-0.028610</td>
      <td>0.090676</td>
    </tr>
    <tr>
      <th>3627</th>
      <td>-0.173311</td>
      <td>-0.023604</td>
      <td>0.190388</td>
      <td>-0.136543</td>
      <td>-0.360269</td>
      <td>-0.444687</td>
      <td>0.056311</td>
      <td>0.291941</td>
      <td>-0.399719</td>
      <td>-0.167930</td>
      <td>...</td>
      <td>0.042209</td>
      <td>-0.161905</td>
      <td>-0.040535</td>
      <td>-0.050516</td>
      <td>-0.252020</td>
      <td>-0.133981</td>
      <td>0.155001</td>
      <td>-0.154482</td>
      <td>-0.060201</td>
      <td>-0.126555</td>
    </tr>
    <tr>
      <th>12559</th>
      <td>-0.124636</td>
      <td>-0.101799</td>
      <td>0.129061</td>
      <td>0.636908</td>
      <td>0.681090</td>
      <td>0.399300</td>
      <td>-0.078321</td>
      <td>0.221824</td>
      <td>-0.277218</td>
      <td>-0.178589</td>
      <td>...</td>
      <td>0.022364</td>
      <td>-0.109274</td>
      <td>-0.073540</td>
      <td>-0.153336</td>
      <td>-0.123705</td>
      <td>-0.238896</td>
      <td>0.296447</td>
      <td>-0.116798</td>
      <td>0.115076</td>
      <td>-0.345925</td>
    </tr>
    <tr>
      <th>8123</th>
      <td>0.063508</td>
      <td>0.332506</td>
      <td>0.119605</td>
      <td>-0.001362</td>
      <td>-0.161801</td>
      <td>-0.082302</td>
      <td>-0.025883</td>
      <td>0.048027</td>
      <td>0.126974</td>
      <td>-0.159802</td>
      <td>...</td>
      <td>0.002221</td>
      <td>-0.093885</td>
      <td>0.430285</td>
      <td>-0.088562</td>
      <td>0.321488</td>
      <td>0.447437</td>
      <td>0.292395</td>
      <td>-0.188566</td>
      <td>-0.272767</td>
      <td>0.126173</td>
    </tr>
    <tr>
      <th>210</th>
      <td>0.015537</td>
      <td>0.425568</td>
      <td>0.350672</td>
      <td>0.113120</td>
      <td>-0.128615</td>
      <td>0.098112</td>
      <td>0.222081</td>
      <td>0.101654</td>
      <td>0.224073</td>
      <td>-0.341075</td>
      <td>...</td>
      <td>0.100983</td>
      <td>-0.008055</td>
      <td>0.202025</td>
      <td>0.029846</td>
      <td>-0.019182</td>
      <td>0.107063</td>
      <td>0.002301</td>
      <td>0.038213</td>
      <td>-0.139270</td>
      <td>-0.007586</td>
    </tr>
  </tbody>
</table>
<p>2928 rows × 768 columns</p>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lgr</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">max_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
<span class="n">lgr</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">emb_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Train accuracy </span><span class="si">{:.2f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">lgr</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">emb_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Test accuracy </span><span class="si">{:.2f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">lgr</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">emb_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Train accuracy 0.87
Test accuracy 0.83
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Some improvement over bag of words and average embedding representations!</p></li>
<li><p>But much slower …</p></li>
</ul>
<p><br><br><br><br></p>
</div>
</div>
<div class="section" id="topic-modeling">
<h2>Topic modeling<a class="headerlink" href="#topic-modeling" title="Permalink to this headline">¶</a></h2>
<div class="section" id="topic-modeling-motivation">
<h3>Topic modeling motivation<a class="headerlink" href="#topic-modeling-motivation" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Suppose you have a large collection of documents on a variety of topics.</p></li>
</ul>
</div>
<div class="section" id="example-a-corpus-of-news-articles">
<h3>Example: A corpus of news articles<a class="headerlink" href="#example-a-corpus-of-news-articles" title="Permalink to this headline">¶</a></h3>
<!-- <center> -->
<img src="img/TM_NYT_articles.png" height="2000" width="2000"> 
<!-- </center> --></div>
<div class="section" id="example-a-corpus-of-food-magazines">
<h3>Example: A corpus of food magazines<a class="headerlink" href="#example-a-corpus-of-food-magazines" title="Permalink to this headline">¶</a></h3>
<!-- <center> -->
<img src="img/TM_food_magazines.png" height="2000" width="2000"> 
<!-- </center> --></div>
<div class="section" id="a-corpus-of-scientific-articles">
<h3>A corpus of scientific articles<a class="headerlink" href="#a-corpus-of-scientific-articles" title="Permalink to this headline">¶</a></h3>
<center>
<img src="img/TM_science_articles.png" height="2000" width="2000"> 
</center>
<p>(Credit: <a class="reference external" href="http://www.cs.columbia.edu/~blei/talks/Blei_Science_2008.pdf">Dave Blei’s presentation</a>)</p>
</div>
<div class="section" id="id5">
<h3>Topic modeling motivation<a class="headerlink" href="#id5" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Humans are pretty good at reading and understanding a document and answering questions such as</p>
<ul>
<li><p>What is it about?</p></li>
<li><p>Which documents is it related to?</p></li>
</ul>
</li>
<li><p>But for a large collection of documents it would take years to read all documents and organize and categorize them so that they are easy to search.</p></li>
<li><p>You need an automated way</p>
<ul>
<li><p>to get an idea of what’s going on in the data or</p></li>
<li><p>to pull documents related to a certain topic</p></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="id6">
<h3>Topic modeling<a class="headerlink" href="#id6" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Topic modeling gives you an ability to summarize the major themes in a large collection of documents (corpus).</p>
<ul>
<li><p>Example: The major themes in a collection of news articles could be</p>
<ul>
<li><p><strong>politics</strong></p></li>
<li><p><strong>entertainment</strong></p></li>
<li><p><strong>sports</strong></p></li>
<li><p><strong>technology</strong></p></li>
<li><p>…</p></li>
</ul>
</li>
</ul>
</li>
<li><p>A common tool to solve such problems is unsupervised ML methods.</p></li>
<li><p>Given the hyperparameter <span class="math notranslate nohighlight">\(K\)</span>, the idea of topic modeling is to describe the data using <span class="math notranslate nohighlight">\(K\)</span> “topics”</p></li>
</ul>
</div>
<div class="section" id="topic-modeling-input-and-output">
<h3>Topic modeling: Input and output<a class="headerlink" href="#topic-modeling-input-and-output" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Input</p>
<ul>
<li><p>A large collection of documents</p></li>
<li><p>A value for the hyperparameter <span class="math notranslate nohighlight">\(K\)</span> (e.g., <span class="math notranslate nohighlight">\(K = 3\)</span>)</p></li>
</ul>
</li>
<li><p>Output</p>
<ol class="simple">
<li><p>Topic-words association</p>
<ul>
<li><p>For each topic, what words describe that topic?</p></li>
</ul>
</li>
<li><p>Document-topics association</p>
<ul>
<li><p>For each document, what topics are expressed by the document?</p></li>
</ul>
</li>
</ol>
</li>
</ul>
</div>
<div class="section" id="topic-modeling-example">
<h3>Topic modeling: Example<a class="headerlink" href="#topic-modeling-example" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Topic-words association</p>
<ul>
<li><p>For each topic, what words describe that topic?</p></li>
<li><p>A topic is a mixture of words.</p></li>
</ul>
</li>
</ul>
<!-- <center> -->
<img src="img/topic_modeling_word_topics.png" height="1000" width="1000"> 
<!-- </center>     --></div>
<div class="section" id="id7">
<h3>Topic modeling: Example<a class="headerlink" href="#id7" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Document-topics association</p>
<ul>
<li><p>For each document, what topics are expressed by the document?</p></li>
<li><p>A document is a mixture of topics.</p></li>
</ul>
</li>
</ul>
<!-- <center>     -->
<img src="img/topic_modeling_doc_topics.png" height="800" width="800"> 
<!-- </center>     --></div>
<div class="section" id="id8">
<h3>Topic modeling: Input and output<a class="headerlink" href="#id8" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Input</p>
<ul>
<li><p>A large collection of documents</p></li>
<li><p>A value for the hyperparameter <span class="math notranslate nohighlight">\(K\)</span> (e.g., <span class="math notranslate nohighlight">\(K = 3\)</span>)</p></li>
</ul>
</li>
<li><p>Output</p>
<ul>
<li><p>For each topic, what words describe that topic?</p></li>
<li><p>For each document, what topics are expressed by the document?</p></li>
</ul>
</li>
</ul>
<!-- <center> -->
<img src="img/topic_modeling_output.png" height="1000" width="1000"> 
<!-- </center>     --></div>
<div class="section" id="topic-modeling-some-applications">
<h3>Topic modeling: Some applications<a class="headerlink" href="#topic-modeling-some-applications" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Topic modeling is a great EDA tool to get a sense of what’s going on in a large corpus.</p></li>
<li><p>Some examples</p>
<ul>
<li><p>If you want to pull documents related to a particular lawsuit.</p></li>
<li><p>You want to examine people’s sentiment towards a particular candidate and/or political party and so you want to pull tweets or Facebook posts related to election.</p></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="topic-modeling-toy-example">
<h3>Topic modeling toy example<a class="headerlink" href="#topic-modeling-toy-example" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">toy_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;data/toy_lda_data.csv&quot;</span><span class="p">)</span>
<span class="n">toy_df</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>doc_id</th>
      <th>text</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>famous fashion model</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>fashion model pattern</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>fashion model probabilistic topic model confer...</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>famous fashion model</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>fresh fashion model</td>
    </tr>
    <tr>
      <th>5</th>
      <td>6</td>
      <td>famous fashion model</td>
    </tr>
    <tr>
      <th>6</th>
      <td>7</td>
      <td>famous fashion model</td>
    </tr>
    <tr>
      <th>7</th>
      <td>8</td>
      <td>famous fashion model</td>
    </tr>
    <tr>
      <th>8</th>
      <td>9</td>
      <td>famous fashion model</td>
    </tr>
    <tr>
      <th>9</th>
      <td>10</td>
      <td>creative fashion model</td>
    </tr>
    <tr>
      <th>10</th>
      <td>11</td>
      <td>famous fashion model</td>
    </tr>
    <tr>
      <th>11</th>
      <td>12</td>
      <td>famous fashion model</td>
    </tr>
    <tr>
      <th>12</th>
      <td>13</td>
      <td>fashion model probabilistic topic model confer...</td>
    </tr>
    <tr>
      <th>13</th>
      <td>14</td>
      <td>probabilistic topic model</td>
    </tr>
    <tr>
      <th>14</th>
      <td>15</td>
      <td>probabilistic model pattern</td>
    </tr>
    <tr>
      <th>15</th>
      <td>16</td>
      <td>probabilistic topic model</td>
    </tr>
    <tr>
      <th>16</th>
      <td>17</td>
      <td>probabilistic topic model</td>
    </tr>
    <tr>
      <th>17</th>
      <td>18</td>
      <td>probabilistic topic model</td>
    </tr>
    <tr>
      <th>18</th>
      <td>19</td>
      <td>probabilistic topic model</td>
    </tr>
    <tr>
      <th>19</th>
      <td>20</td>
      <td>probabilistic topic model</td>
    </tr>
    <tr>
      <th>20</th>
      <td>21</td>
      <td>probabilistic topic model</td>
    </tr>
    <tr>
      <th>21</th>
      <td>22</td>
      <td>fashion model probabilistic topic model confer...</td>
    </tr>
    <tr>
      <th>22</th>
      <td>23</td>
      <td>apple kiwi nutrition</td>
    </tr>
    <tr>
      <th>23</th>
      <td>24</td>
      <td>kiwi health nutrition</td>
    </tr>
    <tr>
      <th>24</th>
      <td>25</td>
      <td>fresh apple health</td>
    </tr>
    <tr>
      <th>25</th>
      <td>26</td>
      <td>probabilistic topic model</td>
    </tr>
    <tr>
      <th>26</th>
      <td>27</td>
      <td>creative health nutrition</td>
    </tr>
    <tr>
      <th>27</th>
      <td>28</td>
      <td>probabilistic topic model</td>
    </tr>
    <tr>
      <th>28</th>
      <td>29</td>
      <td>probabilistic topic model</td>
    </tr>
    <tr>
      <th>29</th>
      <td>30</td>
      <td>hidden markov model probabilistic</td>
    </tr>
    <tr>
      <th>30</th>
      <td>31</td>
      <td>probabilistic topic model</td>
    </tr>
    <tr>
      <th>31</th>
      <td>32</td>
      <td>probabilistic topic model</td>
    </tr>
    <tr>
      <th>32</th>
      <td>33</td>
      <td>apple kiwi nutrition</td>
    </tr>
    <tr>
      <th>33</th>
      <td>34</td>
      <td>apple kiwi health</td>
    </tr>
    <tr>
      <th>34</th>
      <td>35</td>
      <td>apple kiwi nutrition</td>
    </tr>
    <tr>
      <th>35</th>
      <td>36</td>
      <td>fresh kiwi health</td>
    </tr>
    <tr>
      <th>36</th>
      <td>37</td>
      <td>apple kiwi nutrition</td>
    </tr>
    <tr>
      <th>37</th>
      <td>38</td>
      <td>apple kiwi nutrition</td>
    </tr>
    <tr>
      <th>38</th>
      <td>39</td>
      <td>apple kiwi nutrition</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">corpus</span> <span class="o">=</span> <span class="p">[</span><span class="n">doc</span><span class="o">.</span><span class="n">split</span><span class="p">()</span> <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">toy_df</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()]</span>
<span class="c1"># Create a vocabulary for the lda model</span>
<span class="n">dictionary</span> <span class="o">=</span> <span class="n">corpora</span><span class="o">.</span><span class="n">Dictionary</span><span class="p">(</span><span class="n">corpus</span><span class="p">)</span>
<span class="c1"># Convert our corpus into document-term matrix for Lda</span>
<span class="n">doc_term_matrix</span> <span class="o">=</span> <span class="p">[</span><span class="n">dictionary</span><span class="o">.</span><span class="n">doc2bow</span><span class="p">(</span><span class="n">doc</span><span class="p">)</span> <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">corpus</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">gensim.models</span> <span class="kn">import</span> <span class="n">LdaModel</span>

<span class="c1"># Train an lda model</span>
<span class="n">lda</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">LdaModel</span><span class="p">(</span>
    <span class="n">corpus</span><span class="o">=</span><span class="n">doc_term_matrix</span><span class="p">,</span>
    <span class="n">id2word</span><span class="o">=</span><span class="n">dictionary</span><span class="p">,</span>
    <span class="n">num_topics</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
    <span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">,</span>
    <span class="n">passes</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">### Examine the topics in our LDA model</span>
<span class="n">lda</span><span class="o">.</span><span class="n">print_topics</span><span class="p">(</span><span class="n">num_words</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[(0,
  &#39;0.303*&quot;model&quot; + 0.296*&quot;probabilistic&quot; + 0.261*&quot;topic&quot; + 0.040*&quot;pattern&quot;&#39;),
 (1, &#39;0.245*&quot;kiwi&quot; + 0.219*&quot;apple&quot; + 0.218*&quot;nutrition&quot; + 0.140*&quot;health&quot;&#39;),
 (2, &#39;0.308*&quot;fashion&quot; + 0.307*&quot;model&quot; + 0.180*&quot;famous&quot; + 0.071*&quot;conference&quot;&#39;)]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">### Examine the topic distribution for a document</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Document: &quot;</span><span class="p">,</span> <span class="n">corpus</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">lda</span><span class="p">[</span><span class="n">doc_term_matrix</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;topic id&quot;</span><span class="p">,</span> <span class="s2">&quot;probability&quot;</span><span class="p">])</span>
<span class="n">df</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="s2">&quot;probability&quot;</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Document:  [&#39;famous&#39;, &#39;fashion&#39;, &#39;model&#39;]
</pre></div>
</div>
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>topic id</th>
      <th>probability</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>2</th>
      <td>2</td>
      <td>0.828760</td>
    </tr>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>0.087849</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>0.083391</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>You can also visualize the topics using <code class="docutils literal notranslate"><span class="pre">pyLDAvis</span></code>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">pip</span> <span class="n">install</span> <span class="n">pyLDAvis</span>

</pre></div>
</div>
<blockquote>
<div><p>Do not install it using <code class="docutils literal notranslate"><span class="pre">conda</span></code>. They have made some changes in the recent version and <code class="docutils literal notranslate"><span class="pre">conda</span></code> build is not available for this version yet.</p>
</div></blockquote>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># # Visualize the topics</span>
<span class="c1"># vis = gensimvis.prepare(lda, doc_term_matrix, dictionary, sort_topics=False)</span>
<span class="c1"># vis</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="topic-modeling-with-sklearn">
<h3>Topic modeling with <code class="docutils literal notranslate"><span class="pre">sklearn</span></code><a class="headerlink" href="#topic-modeling-with-sklearn" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>We are using <code class="docutils literal notranslate"><span class="pre">Gensim</span></code> LDA so that we’ll be able to use <code class="docutils literal notranslate"><span class="pre">CoherenceModel</span></code> to evaluate topic model later.</p></li>
<li><p>But we can also train an LDA model with <code class="docutils literal notranslate"><span class="pre">sklearn</span></code>.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">CountVectorizer</span>

<span class="n">vec</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">()</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">vec</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">wiki_df</span><span class="p">[</span><span class="s2">&quot;text_pp&quot;</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">LatentDirichletAllocation</span>

<span class="n">n_topics</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">lda</span> <span class="o">=</span> <span class="n">LatentDirichletAllocation</span><span class="p">(</span>
    <span class="n">n_components</span><span class="o">=</span><span class="n">n_topics</span><span class="p">,</span> <span class="n">learning_method</span><span class="o">=</span><span class="s2">&quot;batch&quot;</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span>
<span class="p">)</span>
<span class="n">document_topics</span> <span class="o">=</span> <span class="n">lda</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;lda.components_.shape: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">lda</span><span class="o">.</span><span class="n">components_</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>lda.components_.shape: (3, 4069)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sorting</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">lda</span><span class="o">.</span><span class="n">components_</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)[:,</span> <span class="p">::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="n">feature_names</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">vec</span><span class="o">.</span><span class="n">get_feature_names</span><span class="p">())</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mglearn</span>

<span class="n">mglearn</span><span class="o">.</span><span class="n">tools</span><span class="o">.</span><span class="n">print_topics</span><span class="p">(</span>
    <span class="n">topics</span><span class="o">=</span><span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">),</span>
    <span class="n">feature_names</span><span class="o">=</span><span class="n">feature_names</span><span class="p">,</span>
    <span class="n">sorting</span><span class="o">=</span><span class="n">sorting</span><span class="p">,</span>
    <span class="n">topics_per_chunk</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
    <span class="n">n_words</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>topic 0       topic 1       topic 2       
--------      --------      --------      
court         hockey        datum         
human         player        mining        
intelligence  team          data          
law           ice           set           
machine       league        pattern       
artificial    play          information   
power         game          analysis      
provincial    puck          machine       
canada        penalty       software      
government    nhl           learning      
</pre></div>
</div>
</div>
</div>
<p><br><br><br><br></p>
</div>
</div>
<div class="section" id="basic-text-preprocessing">
<h2>Basic text preprocessing<a class="headerlink" href="#basic-text-preprocessing" title="Permalink to this headline">¶</a></h2>
<div class="section" id="introduction">
<h3>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Why do we need preprocessing?</p>
<ul>
<li><p>Text data is unstructured and messy.</p></li>
<li><p>We need to “normalize” it before we do anything interesting with it.</p></li>
</ul>
</li>
<li><p>Example:</p>
<ul>
<li><p><strong>Lemma</strong>: Same stem, same part-of-speech, roughly the same meaning</p>
<ul>
<li><p>Vancouver’s → Vancouver</p></li>
<li><p>computers → computer</p></li>
<li><p>rising → rise, rose, rises</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
<div class="section" id="tokenization">
<h3>Tokenization<a class="headerlink" href="#tokenization" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Sentence segmentation</p>
<ul>
<li><p>Split text into sentences</p></li>
</ul>
</li>
<li><p>Word tokenization</p>
<ul>
<li><p>Split sentences into words</p></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="tokenization-sentence-segmentation">
<h3>Tokenization: sentence segmentation<a class="headerlink" href="#tokenization-sentence-segmentation" title="Permalink to this headline">¶</a></h3>
<blockquote>
MDS is a Master's program at UBC in British Columbia. MDS teaching team is truly multicultural!! Dr. Beuzen did his Ph.D. in Australia. Dr. Timbers, Dr. Ostblom, Dr. Rodríguez-Arelis, and Dr. Kolhatkar did theirs in Canada. Dr. George did his in Scotland. Dr. Gelbart did his PhD in the U.S.
</blockquote>
<ul class="simple">
<li><p>How many sentences are there in this text?</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">### Let&#39;s do sentence segmentation on &quot;.&quot;</span>
<span class="n">text</span> <span class="o">=</span> <span class="p">(</span>
    <span class="s2">&quot;MDS is a Master&#39;s program at UBC in British Columbia. &quot;</span>
    <span class="s2">&quot;MDS teaching team is truly multicultural!! &quot;</span>
    <span class="s2">&quot;Dr. Beuzen did his Ph.D. in Australia. &quot;</span>
    <span class="s2">&quot;Dr. Timbers, Dr. Ostblom, Dr. Rodríguez-Arelis, and Dr. Kolhatkar did theirs in Canada. &quot;</span>
    <span class="s2">&quot;Dr. George did his in Scotland. &quot;</span>
    <span class="s2">&quot;Dr. Gelbart did his PhD in the U.S.&quot;</span>
<span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">text</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&quot;MDS is a Master&#39;s program at UBC in British Columbia&quot;, &#39; MDS teaching team is truly multicultural!! Dr&#39;, &#39; Beuzen did his Ph&#39;, &#39;D&#39;, &#39; in Australia&#39;, &#39; Dr&#39;, &#39; Timbers, Dr&#39;, &#39; Ostblom, Dr&#39;, &#39; Rodríguez-Arelis, and Dr&#39;, &#39; Kolhatkar did theirs in Canada&#39;, &#39; Dr&#39;, &#39; George did his in Scotland&#39;, &#39; Dr&#39;, &#39; Gelbart did his PhD in the U&#39;, &#39;S&#39;, &#39;&#39;]
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="sentence-segmentation">
<h3>Sentence segmentation<a class="headerlink" href="#sentence-segmentation" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>In English, period (.) is quite ambiguous. (In Chinese, it is unambiguous.)</p>
<ul>
<li><p>Abbreviations like Dr., U.S., Inc.</p></li>
<li><p>Numbers like 60.44%, 0.98</p></li>
</ul>
</li>
<li><p>! and ? are relatively ambiguous.</p></li>
<li><p>How about writing regular expressions?</p></li>
<li><p>A common way is using off-the-shelf models for sentence segmentation.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">### Let&#39;s try to do sentence segmentation using nltk</span>
<span class="kn">from</span> <span class="nn">nltk.tokenize</span> <span class="kn">import</span> <span class="n">sent_tokenize</span>

<span class="n">sent_tokenized</span> <span class="o">=</span> <span class="n">sent_tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sent_tokenized</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&quot;MDS is a Master&#39;s program at UBC in British Columbia.&quot;, &#39;MDS teaching team is truly multicultural!!&#39;, &#39;Dr. Beuzen did his Ph.D. in Australia.&#39;, &#39;Dr. Timbers, Dr. Ostblom, Dr. Rodríguez-Arelis, and Dr. Kolhatkar did theirs in Canada.&#39;, &#39;Dr. George did his in Scotland.&#39;, &#39;Dr. Gelbart did his PhD in the U.S.&#39;]
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="word-tokenization">
<h3>Word tokenization<a class="headerlink" href="#word-tokenization" title="Permalink to this headline">¶</a></h3>
<blockquote>
MDS is a Master's program at UBC in British Columbia. 
</blockquote>
<ul class="simple">
<li><p>How many words are there in this sentence?</p></li>
<li><p>Is whitespace a sufficient condition for a word boundary?</p></li>
</ul>
</div>
<div class="section" id="id9">
<h3>Word tokenization<a class="headerlink" href="#id9" title="Permalink to this headline">¶</a></h3>
<blockquote>
MDS is a Master's program at UBC in British Columbia. 
</blockquote>
<ul class="simple">
<li><p>What’s our definition of a word?</p>
<ul>
<li><p>Should British Columbia be one word or two words?</p></li>
<li><p>Should punctuation be considered a separate word?</p></li>
<li><p>What about the punctuations in <code class="docutils literal notranslate"><span class="pre">U.S.</span></code>?</p></li>
<li><p>What do we do with words like <code class="docutils literal notranslate"><span class="pre">Master's</span></code>?</p></li>
</ul>
</li>
<li><p>This process of identifying word boundaries is referred to as <strong>tokenization</strong>.</p></li>
<li><p>You can use regex but better to do it with off-the-shelf ML models.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">### Let&#39;s do word segmentation on white spaces</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Splitting on whitespace: &quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">sent</span><span class="o">.</span><span class="n">split</span><span class="p">()</span> <span class="k">for</span> <span class="n">sent</span> <span class="ow">in</span> <span class="n">sent_tokenized</span><span class="p">])</span>

<span class="c1">### Let&#39;s try to do word segmentation using nltk</span>
<span class="kn">from</span> <span class="nn">nltk.tokenize</span> <span class="kn">import</span> <span class="n">word_tokenize</span>

<span class="n">word_tokenized</span> <span class="o">=</span> <span class="p">[</span><span class="n">word_tokenize</span><span class="p">(</span><span class="n">sent</span><span class="p">)</span> <span class="k">for</span> <span class="n">sent</span> <span class="ow">in</span> <span class="n">sent_tokenized</span><span class="p">]</span>
<span class="c1"># This is similar to the input format of word2vec algorithm</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n\n\n</span><span class="s2">Tokenized: &quot;</span><span class="p">,</span> <span class="n">word_tokenized</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Splitting on whitespace:  [[&#39;MDS&#39;, &#39;is&#39;, &#39;a&#39;, &quot;Master&#39;s&quot;, &#39;program&#39;, &#39;at&#39;, &#39;UBC&#39;, &#39;in&#39;, &#39;British&#39;, &#39;Columbia.&#39;], [&#39;MDS&#39;, &#39;teaching&#39;, &#39;team&#39;, &#39;is&#39;, &#39;truly&#39;, &#39;multicultural!!&#39;], [&#39;Dr.&#39;, &#39;Beuzen&#39;, &#39;did&#39;, &#39;his&#39;, &#39;Ph.D.&#39;, &#39;in&#39;, &#39;Australia.&#39;], [&#39;Dr.&#39;, &#39;Timbers,&#39;, &#39;Dr.&#39;, &#39;Ostblom,&#39;, &#39;Dr.&#39;, &#39;Rodríguez-Arelis,&#39;, &#39;and&#39;, &#39;Dr.&#39;, &#39;Kolhatkar&#39;, &#39;did&#39;, &#39;theirs&#39;, &#39;in&#39;, &#39;Canada.&#39;], [&#39;Dr.&#39;, &#39;George&#39;, &#39;did&#39;, &#39;his&#39;, &#39;in&#39;, &#39;Scotland.&#39;], [&#39;Dr.&#39;, &#39;Gelbart&#39;, &#39;did&#39;, &#39;his&#39;, &#39;PhD&#39;, &#39;in&#39;, &#39;the&#39;, &#39;U.S.&#39;]]



Tokenized:  [[&#39;MDS&#39;, &#39;is&#39;, &#39;a&#39;, &#39;Master&#39;, &quot;&#39;s&quot;, &#39;program&#39;, &#39;at&#39;, &#39;UBC&#39;, &#39;in&#39;, &#39;British&#39;, &#39;Columbia&#39;, &#39;.&#39;], [&#39;MDS&#39;, &#39;teaching&#39;, &#39;team&#39;, &#39;is&#39;, &#39;truly&#39;, &#39;multicultural&#39;, &#39;!&#39;, &#39;!&#39;], [&#39;Dr.&#39;, &#39;Beuzen&#39;, &#39;did&#39;, &#39;his&#39;, &#39;Ph.D.&#39;, &#39;in&#39;, &#39;Australia&#39;, &#39;.&#39;], [&#39;Dr.&#39;, &#39;Timbers&#39;, &#39;,&#39;, &#39;Dr.&#39;, &#39;Ostblom&#39;, &#39;,&#39;, &#39;Dr.&#39;, &#39;Rodríguez-Arelis&#39;, &#39;,&#39;, &#39;and&#39;, &#39;Dr.&#39;, &#39;Kolhatkar&#39;, &#39;did&#39;, &#39;theirs&#39;, &#39;in&#39;, &#39;Canada&#39;, &#39;.&#39;], [&#39;Dr.&#39;, &#39;George&#39;, &#39;did&#39;, &#39;his&#39;, &#39;in&#39;, &#39;Scotland&#39;, &#39;.&#39;], [&#39;Dr.&#39;, &#39;Gelbart&#39;, &#39;did&#39;, &#39;his&#39;, &#39;PhD&#39;, &#39;in&#39;, &#39;the&#39;, &#39;U.S&#39;, &#39;.&#39;]]
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="word-segmentation">
<h3>Word segmentation<a class="headerlink" href="#word-segmentation" title="Permalink to this headline">¶</a></h3>
<p>For some languages you need much more sophisticated tokenizers.</p>
<ul class="simple">
<li><p>For languages such as Chinese, there are no spaces between words.</p>
<ul>
<li><p><a class="reference external" href="https://github.com/fxsjy/jieba">jieba</a> is a popular tokenizer for Chinese.</p></li>
</ul>
</li>
<li><p>German doesn’t separate compound words.</p>
<ul>
<li><p>Example: <em>rindfleischetikettierungsüberwachungsaufgabenübertragungsgesetz</em></p></li>
<li><p>(the law for the delegation of monitoring beef labeling)</p></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="types-and-tokens">
<h3>Types and tokens<a class="headerlink" href="#types-and-tokens" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Usually in NLP, we talk about</p>
<ul>
<li><p><strong>Type</strong> an element in the vocabulary</p></li>
<li><p><strong>Token</strong> an instance of that type in running text</p></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="exercise-for-you">
<h3>Exercise for you<a class="headerlink" href="#exercise-for-you" title="Permalink to this headline">¶</a></h3>
<blockquote>    
UBC is located in the beautiful province of British Columbia. It's very close 
to the U.S. border. You'll get to the USA border in about 45 mins by car.     
</blockquote>  
<ul class="simple">
<li><p>Consider the example above.</p>
<ul>
<li><p>How many types? (task dependent)</p></li>
<li><p>How many tokens?</p></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="other-commonly-used-preprocessing-steps">
<h3>Other commonly used preprocessing steps<a class="headerlink" href="#other-commonly-used-preprocessing-steps" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Punctuation and stopword removal</p></li>
<li><p>Stemming and lemmatization</p></li>
</ul>
</div>
<div class="section" id="punctuation-and-stopword-removal">
<h3>Punctuation and stopword removal<a class="headerlink" href="#punctuation-and-stopword-removal" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>The most frequently occurring words in English are not very useful in many NLP tasks.</p>
<ul>
<li><p>Example: <em>the</em> , <em>is</em> , <em>a</em> , and punctuation</p></li>
</ul>
</li>
<li><p>Probably not very informative in many tasks</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Let&#39;s use `nltk.stopwords`.</span>
<span class="c1"># Add punctuations to the list.</span>
<span class="n">stop_words</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">stopwords</span><span class="o">.</span><span class="n">words</span><span class="p">(</span><span class="s2">&quot;english&quot;</span><span class="p">)))</span>
<span class="kn">import</span> <span class="nn">string</span>

<span class="n">punctuation</span> <span class="o">=</span> <span class="n">string</span><span class="o">.</span><span class="n">punctuation</span>
<span class="n">stop_words</span> <span class="o">+=</span> <span class="nb">list</span><span class="p">(</span><span class="n">punctuation</span><span class="p">)</span>
<span class="c1"># stop_words.extend([&#39;``&#39;,&#39;`&#39;,&#39;br&#39;,&#39;&quot;&#39;,&quot;”&quot;, &quot;&#39;&#39;&quot;, &quot;&#39;s&quot;])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">stop_words</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&quot;you&#39;d&quot;, &#39;ours&#39;, &#39;and&#39;, &#39;him&#39;, &#39;them&#39;, &#39;all&#39;, &quot;mightn&#39;t&quot;, &#39;during&#39;, &quot;you&#39;re&quot;, &#39;mustn&#39;, &#39;having&#39;, &#39;by&#39;, &#39;which&#39;, &#39;ve&#39;, &#39;here&#39;, &quot;weren&#39;t&quot;, &#39;to&#39;, &#39;was&#39;, &#39;we&#39;, &#39;a&#39;, &#39;does&#39;, &#39;o&#39;, &quot;didn&#39;t&quot;, &#39;yourself&#39;, &#39;been&#39;, &#39;nor&#39;, &#39;their&#39;, &#39;whom&#39;, &#39;ourselves&#39;, &#39;into&#39;, &#39;haven&#39;, &#39;aren&#39;, &quot;haven&#39;t&quot;, &#39;what&#39;, &#39;ain&#39;, &#39;as&#39;, &#39;me&#39;, &#39;is&#39;, &#39;his&#39;, &#39;other&#39;, &#39;should&#39;, &#39;will&#39;, &#39;out&#39;, &#39;wouldn&#39;, &quot;isn&#39;t&quot;, &quot;don&#39;t&quot;, &#39;re&#39;, &quot;it&#39;s&quot;, &quot;won&#39;t&quot;, &#39;those&#39;, &#39;am&#39;, &quot;couldn&#39;t&quot;, &#39;yourselves&#39;, &#39;t&#39;, &#39;she&#39;, &#39;so&#39;, &#39;at&#39;, &#39;mightn&#39;, &#39;did&#39;, &#39;have&#39;, &#39;how&#39;, &quot;she&#39;s&quot;, &#39;then&#39;, &#39;no&#39;, &#39;its&#39;, &#39;he&#39;, &#39;who&#39;, &#39;themselves&#39;, &#39;why&#39;, &#39;our&#39;, &#39;only&#39;, &#39;over&#39;, &#39;both&#39;, &#39;couldn&#39;, &#39;d&#39;, &#39;from&#39;, &#39;each&#39;, &#39;y&#39;, &#39;ll&#39;, &#39;down&#39;, &#39;off&#39;, &#39;further&#39;, &#39;theirs&#39;, &#39;but&#39;, &#39;himself&#39;, &quot;hasn&#39;t&quot;, &#39;shouldn&#39;, &#39;it&#39;, &#39;hers&#39;, &#39;ma&#39;, &quot;that&#39;ll&quot;, &#39;not&#39;, &#39;an&#39;, &#39;they&#39;, &#39;just&#39;, &#39;this&#39;, &#39;hasn&#39;, &#39;has&#39;, &#39;on&#39;, &quot;doesn&#39;t&quot;, &#39;you&#39;, &#39;very&#39;, &#39;after&#39;, &#39;didn&#39;, &#39;some&#39;, &#39;or&#39;, &#39;the&#39;, &#39;about&#39;, &#39;while&#39;, &#39;than&#39;, &#39;yours&#39;, &#39;can&#39;, &quot;shan&#39;t&quot;, &quot;you&#39;ve&quot;, &#39;more&#39;, &quot;hadn&#39;t&quot;, &#39;own&#39;, &#39;herself&#39;, &#39;with&#39;, &quot;needn&#39;t&quot;, &#39;these&#39;, &#39;such&#39;, &#39;that&#39;, &#39;under&#39;, &#39;my&#39;, &#39;be&#39;, &#39;again&#39;, &#39;her&#39;, &quot;you&#39;ll&quot;, &#39;were&#39;, &#39;isn&#39;, &#39;wasn&#39;, &quot;wouldn&#39;t&quot;, &#39;do&#39;, &#39;because&#39;, &#39;once&#39;, &#39;through&#39;, &#39;there&#39;, &#39;when&#39;, &#39;hadn&#39;, &#39;doesn&#39;, &#39;s&#39;, &#39;myself&#39;, &quot;aren&#39;t&quot;, &#39;few&#39;, &#39;your&#39;, &#39;above&#39;, &#39;up&#39;, &#39;too&#39;, &#39;same&#39;, &#39;for&#39;, &#39;m&#39;, &#39;needn&#39;, &#39;are&#39;, &#39;now&#39;, &#39;had&#39;, &#39;shan&#39;, &#39;in&#39;, &#39;until&#39;, &quot;shouldn&#39;t&quot;, &quot;mustn&#39;t&quot;, &quot;wasn&#39;t&quot;, &#39;itself&#39;, &#39;i&#39;, &#39;between&#39;, &#39;below&#39;, &#39;being&#39;, &#39;weren&#39;, &#39;of&#39;, &#39;won&#39;, &#39;if&#39;, &#39;most&#39;, &#39;don&#39;, &#39;any&#39;, &#39;against&#39;, &#39;doing&#39;, &#39;where&#39;, &#39;before&#39;, &quot;should&#39;ve&quot;, &#39;!&#39;, &#39;&quot;&#39;, &#39;#&#39;, &#39;$&#39;, &#39;%&#39;, &#39;&amp;&#39;, &quot;&#39;&quot;, &#39;(&#39;, &#39;)&#39;, &#39;*&#39;, &#39;+&#39;, &#39;,&#39;, &#39;-&#39;, &#39;.&#39;, &#39;/&#39;, &#39;:&#39;, &#39;;&#39;, &#39;&lt;&#39;, &#39;=&#39;, &#39;&gt;&#39;, &#39;?&#39;, &#39;@&#39;, &#39;[&#39;, &#39;\\&#39;, &#39;]&#39;, &#39;^&#39;, &#39;_&#39;, &#39;`&#39;, &#39;{&#39;, &#39;|&#39;, &#39;}&#39;, &#39;~&#39;]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">### Get rid of stop words</span>
<span class="n">preprocessed</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">sent</span> <span class="ow">in</span> <span class="n">word_tokenized</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">sent</span><span class="p">:</span>
        <span class="n">token</span> <span class="o">=</span> <span class="n">token</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">token</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">stop_words</span><span class="p">:</span>
            <span class="n">preprocessed</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">token</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">preprocessed</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;mds&#39;, &#39;master&#39;, &quot;&#39;s&quot;, &#39;program&#39;, &#39;ubc&#39;, &#39;british&#39;, &#39;columbia&#39;, &#39;mds&#39;, &#39;teaching&#39;, &#39;team&#39;, &#39;truly&#39;, &#39;multicultural&#39;, &#39;dr.&#39;, &#39;beuzen&#39;, &#39;ph.d.&#39;, &#39;australia&#39;, &#39;dr.&#39;, &#39;timbers&#39;, &#39;dr.&#39;, &#39;ostblom&#39;, &#39;dr.&#39;, &#39;rodríguez-arelis&#39;, &#39;dr.&#39;, &#39;kolhatkar&#39;, &#39;canada&#39;, &#39;dr.&#39;, &#39;george&#39;, &#39;scotland&#39;, &#39;dr.&#39;, &#39;gelbart&#39;, &#39;phd&#39;, &#39;u.s&#39;]
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="lemmatization">
<h3>Lemmatization<a class="headerlink" href="#lemmatization" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>For many NLP tasks (e.g., web search) we want to ignore morphological differences between words</p>
<ul>
<li><p>Example: If your search term is “studying for ML quiz” you might want to include pages containing “tips to study for an ML quiz” or “here is how I studied for my ML quiz”</p></li>
</ul>
</li>
<li><p>Lemmatization converts inflected forms into the base form.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">nltk</span>

<span class="n">nltk</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="s2">&quot;wordnet&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[nltk_data] Downloading package wordnet to /Users/kvarada/nltk_data...
[nltk_data]   Package wordnet is already up-to-date!
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>True
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># nltk has a lemmatizer</span>
<span class="kn">from</span> <span class="nn">nltk.stem</span> <span class="kn">import</span> <span class="n">WordNetLemmatizer</span>

<span class="n">lemmatizer</span> <span class="o">=</span> <span class="n">WordNetLemmatizer</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Lemma of studying: &quot;</span><span class="p">,</span> <span class="n">lemmatizer</span><span class="o">.</span><span class="n">lemmatize</span><span class="p">(</span><span class="s2">&quot;studying&quot;</span><span class="p">,</span> <span class="s2">&quot;v&quot;</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Lemma of studied: &quot;</span><span class="p">,</span> <span class="n">lemmatizer</span><span class="o">.</span><span class="n">lemmatize</span><span class="p">(</span><span class="s2">&quot;studied&quot;</span><span class="p">,</span> <span class="s2">&quot;v&quot;</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Lemma of studying:  study
Lemma of studied:  study
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="stemming">
<h3>Stemming<a class="headerlink" href="#stemming" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Has a similar purpose but it is a crude chopping of affixes</p>
<ul>
<li><p><em>automates, automatic, automation</em> all reduced to <em>automat</em>.</p></li>
</ul>
</li>
<li><p>Usually these reduced forms (stems) are not actual words themselves.</p></li>
<li><p>A popular stemming algorithm for English is PorterStemmer.</p></li>
<li><p>Beware that it can be aggressive sometimes.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">nltk.stem.porter</span> <span class="kn">import</span> <span class="n">PorterStemmer</span>

<span class="n">text</span> <span class="o">=</span> <span class="p">(</span>
    <span class="s2">&quot;UBC is located in the beautiful province of British Columbia... &quot;</span>
    <span class="s2">&quot;It&#39;s very close to the U.S. border.&quot;</span>
<span class="p">)</span>
<span class="n">ps</span> <span class="o">=</span> <span class="n">PorterStemmer</span><span class="p">()</span>
<span class="n">tokenized</span> <span class="o">=</span> <span class="n">word_tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
<span class="n">stemmed</span> <span class="o">=</span> <span class="p">[</span><span class="n">ps</span><span class="o">.</span><span class="n">stem</span><span class="p">(</span><span class="n">token</span><span class="p">)</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">tokenized</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Before stemming: &quot;</span><span class="p">,</span> <span class="n">text</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n\n</span><span class="s2">After stemming: &quot;</span><span class="p">,</span> <span class="s2">&quot; &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">stemmed</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Before stemming:  UBC is located in the beautiful province of British Columbia... It&#39;s very close to the U.S. border.


After stemming:  ubc is locat in the beauti provinc of british columbia ... It &#39;s veri close to the u.s. border .
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="other-tools-for-preprocessing">
<h3>Other tools for preprocessing<a class="headerlink" href="#other-tools-for-preprocessing" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>We used <a class="reference external" href="https://www.nltk.org/">Natural Language Processing Toolkit (nltk)</a> above</p>
<ul>
<li><p>You already have used it in 571 and 573</p></li>
</ul>
</li>
<li><p>Many available tools</p></li>
<li><p><a class="reference external" href="https://spacy.io/">spaCy</a></p></li>
</ul>
</div>
<div class="section" id="spacy">
<h3><a class="reference external" href="https://spacy.io/">spaCy</a><a class="headerlink" href="#spacy" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>We already have used spaCy before in 573 and 563.</p></li>
<li><p>Industrial strength NLP library.</p></li>
<li><p>Lightweight, fast, and convenient to use.</p></li>
<li><p>spaCy does many things that we did above in one line of code!</p></li>
<li><p>Also has <a class="reference external" href="https://spacy.io/models/xx">multi-lingual</a> support.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">spacy</span>

<span class="c1"># Load the model</span>
<span class="n">nlp</span> <span class="o">=</span> <span class="n">spacy</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;en_core_web_sm&quot;</span><span class="p">)</span>
<span class="n">text</span> <span class="o">=</span> <span class="p">(</span>
    <span class="s2">&quot;MDS is a Master&#39;s program at UBC in British Columbia. &quot;</span>
    <span class="s2">&quot;MDS teaching team is truly multicultural!! &quot;</span>
    <span class="s2">&quot;Dr. Beuzen did his Ph.D. in Australia. &quot;</span>
    <span class="s2">&quot;Dr. George did his in Scotland. &quot;</span>
    <span class="s2">&quot;Dr. Timbers, Dr. Ostblom, Dr. Rodríguez-Arelis, and Dr. Kolhatkar did theirs in Canada. &quot;</span>
    <span class="s2">&quot;Dr. Gelbart did his PhD in the U.S.&quot;</span>
<span class="p">)</span>

<span class="n">doc</span> <span class="o">=</span> <span class="n">nlp</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Accessing tokens</span>
<span class="n">tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">token</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">doc</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Tokens: &quot;</span><span class="p">,</span> <span class="n">tokens</span><span class="p">)</span>

<span class="c1"># Accessing lemma</span>
<span class="n">lemmas</span> <span class="o">=</span> <span class="p">[</span><span class="n">token</span><span class="o">.</span><span class="n">lemma_</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">doc</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Lemmas: &quot;</span><span class="p">,</span> <span class="n">lemmas</span><span class="p">)</span>

<span class="c1"># Accessing pos</span>
<span class="n">pos</span> <span class="o">=</span> <span class="p">[</span><span class="n">token</span><span class="o">.</span><span class="n">pos_</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">doc</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">POS: &quot;</span><span class="p">,</span> <span class="n">pos</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Tokens:  [MDS, is, a, Master, &#39;s, program, at, UBC, in, British, Columbia, ., MDS, teaching, team, is, truly, multicultural, !, !, Dr., Beuzen, did, his, Ph.D., in, Australia, ., Dr., George, did, his, in, Scotland, ., Dr., Timbers, ,, Dr., Ostblom, ,, Dr., Rodríguez, -, Arelis, ,, and, Dr., Kolhatkar, did, theirs, in, Canada, ., Dr., Gelbart, did, his, PhD, in, the, U.S.]

Lemmas:  [&#39;MDS&#39;, &#39;be&#39;, &#39;a&#39;, &#39;Master&#39;, &quot;&#39;s&quot;, &#39;program&#39;, &#39;at&#39;, &#39;UBC&#39;, &#39;in&#39;, &#39;British&#39;, &#39;Columbia&#39;, &#39;.&#39;, &#39;MDS&#39;, &#39;teaching&#39;, &#39;team&#39;, &#39;be&#39;, &#39;truly&#39;, &#39;multicultural&#39;, &#39;!&#39;, &#39;!&#39;, &#39;Dr.&#39;, &#39;Beuzen&#39;, &#39;do&#39;, &#39;his&#39;, &#39;ph.d.&#39;, &#39;in&#39;, &#39;Australia&#39;, &#39;.&#39;, &#39;Dr.&#39;, &#39;George&#39;, &#39;do&#39;, &#39;his&#39;, &#39;in&#39;, &#39;Scotland&#39;, &#39;.&#39;, &#39;Dr.&#39;, &#39;Timbers&#39;, &#39;,&#39;, &#39;Dr.&#39;, &#39;Ostblom&#39;, &#39;,&#39;, &#39;Dr.&#39;, &#39;Rodríguez&#39;, &#39;-&#39;, &#39;Arelis&#39;, &#39;,&#39;, &#39;and&#39;, &#39;Dr.&#39;, &#39;Kolhatkar&#39;, &#39;do&#39;, &#39;theirs&#39;, &#39;in&#39;, &#39;Canada&#39;, &#39;.&#39;, &#39;Dr.&#39;, &#39;Gelbart&#39;, &#39;do&#39;, &#39;his&#39;, &#39;phd&#39;, &#39;in&#39;, &#39;the&#39;, &#39;U.S.&#39;]

POS:  [&#39;PROPN&#39;, &#39;AUX&#39;, &#39;DET&#39;, &#39;PROPN&#39;, &#39;PART&#39;, &#39;NOUN&#39;, &#39;ADP&#39;, &#39;PROPN&#39;, &#39;ADP&#39;, &#39;PROPN&#39;, &#39;PROPN&#39;, &#39;PUNCT&#39;, &#39;PROPN&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;AUX&#39;, &#39;ADV&#39;, &#39;ADJ&#39;, &#39;PUNCT&#39;, &#39;PUNCT&#39;, &#39;PROPN&#39;, &#39;PROPN&#39;, &#39;VERB&#39;, &#39;PRON&#39;, &#39;NOUN&#39;, &#39;ADP&#39;, &#39;PROPN&#39;, &#39;PUNCT&#39;, &#39;PROPN&#39;, &#39;PROPN&#39;, &#39;VERB&#39;, &#39;PRON&#39;, &#39;ADP&#39;, &#39;PROPN&#39;, &#39;PUNCT&#39;, &#39;PROPN&#39;, &#39;PROPN&#39;, &#39;PUNCT&#39;, &#39;PROPN&#39;, &#39;PROPN&#39;, &#39;PUNCT&#39;, &#39;PROPN&#39;, &#39;PROPN&#39;, &#39;PUNCT&#39;, &#39;PROPN&#39;, &#39;PUNCT&#39;, &#39;CCONJ&#39;, &#39;PROPN&#39;, &#39;PROPN&#39;, &#39;VERB&#39;, &#39;PRON&#39;, &#39;ADP&#39;, &#39;PROPN&#39;, &#39;PUNCT&#39;, &#39;PROPN&#39;, &#39;PROPN&#39;, &#39;VERB&#39;, &#39;PRON&#39;, &#39;NOUN&#39;, &#39;ADP&#39;, &#39;DET&#39;, &#39;PROPN&#39;]
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="other-typical-nlp-tasks">
<h3>Other typical NLP tasks<a class="headerlink" href="#other-typical-nlp-tasks" title="Permalink to this headline">¶</a></h3>
<p>In order to understand text, we usually are interested in extracting information from text. Some common tasks in NLP pipeline are:</p>
<ul class="simple">
<li><p>Part of speech tagging</p>
<ul>
<li><p>Assigning part-of-speech tags to all words in a sentence.</p></li>
</ul>
</li>
<li><p>Named entity recognition</p>
<ul>
<li><p>Labelling named “real-world” objects, like persons, companies or locations.</p></li>
</ul>
</li>
<li><p>Coreference resolution</p>
<ul>
<li><p>Deciding whether two strings (e.g., UBC vs University of British Columbia) refer to the same entity</p></li>
</ul>
</li>
<li><p>Dependency parsing</p>
<ul>
<li><p>Representing grammatical structure of a sentence</p></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="extracting-named-entities-using-spacy">
<h3>Extracting named-entities using spaCy<a class="headerlink" href="#extracting-named-entities-using-spacy" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">spacy</span> <span class="kn">import</span> <span class="n">displacy</span>

<span class="n">doc</span> <span class="o">=</span> <span class="n">nlp</span><span class="p">(</span>
    <span class="s2">&quot;University of British Columbia &quot;</span>
    <span class="s2">&quot;is located in the beautiful &quot;</span>
    <span class="s2">&quot;province of British Columbia.&quot;</span>
<span class="p">)</span>
<span class="n">displacy</span><span class="o">.</span><span class="n">render</span><span class="p">(</span><span class="n">doc</span><span class="p">,</span> <span class="n">style</span><span class="o">=</span><span class="s2">&quot;ent&quot;</span><span class="p">)</span>
<span class="c1"># Text and label of named entity span</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Named entities:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="p">[(</span><span class="n">ent</span><span class="o">.</span><span class="n">text</span><span class="p">,</span> <span class="n">ent</span><span class="o">.</span><span class="n">label_</span><span class="p">)</span> <span class="k">for</span> <span class="n">ent</span> <span class="ow">in</span> <span class="n">doc</span><span class="o">.</span><span class="n">ents</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">ORG means: &quot;</span><span class="p">,</span> <span class="n">spacy</span><span class="o">.</span><span class="n">explain</span><span class="p">(</span><span class="s2">&quot;ORG&quot;</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;GPE means: &quot;</span><span class="p">,</span> <span class="n">spacy</span><span class="o">.</span><span class="n">explain</span><span class="p">(</span><span class="s2">&quot;GPE&quot;</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><span class="tex2jax_ignore"><div class="entities" style="line-height: 2.5; direction: ltr">
<mark class="entity" style="background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    University of British Columbia
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">ORG</span>
</mark>
 is located in the beautiful province of 
<mark class="entity" style="background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    British Columbia
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">GPE</span>
</mark>
.</div></span></div><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Named entities:
 [(&#39;University of British Columbia&#39;, &#39;ORG&#39;), (&#39;British Columbia&#39;, &#39;GPE&#39;)]

ORG means:  Companies, agencies, institutions, etc.
GPE means:  Countries, cities, states
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="dependency-parsing-using-spacy">
<h3>Dependency parsing using spaCy<a class="headerlink" href="#dependency-parsing-using-spacy" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">doc</span> <span class="o">=</span> <span class="n">nlp</span><span class="p">(</span><span class="s2">&quot;I like cats&quot;</span><span class="p">)</span>
<span class="n">displacy</span><span class="o">.</span><span class="n">render</span><span class="p">(</span><span class="n">doc</span><span class="p">,</span> <span class="n">style</span><span class="o">=</span><span class="s2">&quot;dep&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><span class="tex2jax_ignore"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" xml:lang="en" id="36200a61453c41558a5e0ecb812a8175-0" class="displacy" width="575" height="224.5" direction="ltr" style="max-width: none; height: 224.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr">
<text class="displacy-token" fill="currentColor" text-anchor="middle" y="134.5">
    <tspan class="displacy-word" fill="currentColor" x="50">I</tspan>
    <tspan class="displacy-tag" dy="2em" fill="currentColor" x="50">PRON</tspan>
</text>

<text class="displacy-token" fill="currentColor" text-anchor="middle" y="134.5">
    <tspan class="displacy-word" fill="currentColor" x="225">like</tspan>
    <tspan class="displacy-tag" dy="2em" fill="currentColor" x="225">VERB</tspan>
</text>

<text class="displacy-token" fill="currentColor" text-anchor="middle" y="134.5">
    <tspan class="displacy-word" fill="currentColor" x="400">cats</tspan>
    <tspan class="displacy-tag" dy="2em" fill="currentColor" x="400">NOUN</tspan>
</text>

<g class="displacy-arrow">
    <path class="displacy-arc" id="arrow-36200a61453c41558a5e0ecb812a8175-0-0" stroke-width="2px" d="M70,89.5 C70,2.0 225.0,2.0 225.0,89.5" fill="none" stroke="currentColor"/>
    <text dy="1.25em" style="font-size: 0.8em; letter-spacing: 1px">
        <textPath xlink:href="#arrow-36200a61453c41558a5e0ecb812a8175-0-0" class="displacy-label" startOffset="50%" side="left" fill="currentColor" text-anchor="middle">nsubj</textPath>
    </text>
    <path class="displacy-arrowhead" d="M70,91.5 L62,79.5 78,79.5" fill="currentColor"/>
</g>

<g class="displacy-arrow">
    <path class="displacy-arc" id="arrow-36200a61453c41558a5e0ecb812a8175-0-1" stroke-width="2px" d="M245,89.5 C245,2.0 400.0,2.0 400.0,89.5" fill="none" stroke="currentColor"/>
    <text dy="1.25em" style="font-size: 0.8em; letter-spacing: 1px">
        <textPath xlink:href="#arrow-36200a61453c41558a5e0ecb812a8175-0-1" class="displacy-label" startOffset="50%" side="left" fill="currentColor" text-anchor="middle">dobj</textPath>
    </text>
    <path class="displacy-arrowhead" d="M400.0,91.5 L408.0,79.5 392.0,79.5" fill="currentColor"/>
</g>
</svg></span></div></div>
</div>
</div>
<div class="section" id="many-other-things-possible">
<h3>Many other things possible<a class="headerlink" href="#many-other-things-possible" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>A powerful tool</p></li>
<li><p>All my Capstone groups last year used this tool.</p></li>
<li><p>You can build your own rule-based searches.</p></li>
<li><p>You can also access word vectors using spaCy with bigger models. (Currently we are using <code class="docutils literal notranslate"><span class="pre">en_core_web_sm</span></code> model.)</p></li>
</ul>
<p><br><br><br><br></p>
</div>
</div>
<div class="section" id="summary">
<h2>Summary<a class="headerlink" href="#summary" title="Permalink to this headline">¶</a></h2>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "conda-env-cpsc330-py"
        },
        kernelOptions: {
            kernelName: "conda-env-cpsc330-py",
            path: "./lectures"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'conda-env-cpsc330-py'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Varada Kolhatkar<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>