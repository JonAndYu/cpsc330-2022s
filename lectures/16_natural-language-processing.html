
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Lecture 16: Introduction to natural language processing &#8212; CPSC 330 Applied Machine Learning</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet" />
  <link href="../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.e8e5499552300ddf5d7adccae7cc3b70.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Lecture 17: Multi-class classification and introduction to computer vision" href="17_intro_to_computer-vision.html" />
    <link rel="prev" title="Lecture 15: DBSCAN and Recommender Systems" href="15_recommender-systems.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      <img src="../_static/UBC-CS-logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">CPSC 330 Applied Machine Learning</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <div class="bd-toc-item active">
        <p class="caption">
 <span class="caption-text">
  Things you should know
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../docs/README.html">
   CPSC 330 Documents
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Lectures
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="01_intro.html">
   Lecture 1: Course Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="02_decision-trees.html">
   Lecture 2: Terminology, Baselines, Decision Trees
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="03_ml-fundamentals.html">
   Lecture 3: Machine Learning Fundamentals
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="04_kNNs-SVM-RBF.html">
   Lecture 4:
   <span class="math notranslate nohighlight">
    \(k\)
   </span>
   -Nearest Neighbours and SVM RBFs
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="05_preprocessing-pipelines.html">
   Lecture 5: Preprocessing and
   <code class="docutils literal notranslate">
    <span class="pre">
     sklearn
    </span>
   </code>
   pipelines
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="06_column-transformer-text-feats.html">
   Lecture 6:
   <code class="docutils literal notranslate">
    <span class="pre">
     sklearn
    </span>
   </code>
   <code class="docutils literal notranslate">
    <span class="pre">
     ColumnTransformer
    </span>
   </code>
   and Text Features
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="07_linear-models.html">
   Lecture 7: Linear Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="08_hyperparameter-optimization.html">
   Lecture 8: Hyperparameter Optimization and Optimization Bias
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="09_classification-metrics.html">
   Lecture 9: Classification Metrics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="10_regression-metrics.html">
   Lecture 10: Regression Evaluation Metrics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="11_ensembles.html">
   Lecture 11: Ensembles
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="12_feat-importances.html">
   Lecture 12: Feature importances
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="13_feature-engineering-selection.html">
   Lecture 13: Feature engineering and feature selection
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="14_k-means-clustering.html">
   Lecture 14: Clustering
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="15_recommender-systems.html">
   Lecture 15: DBSCAN and Recommender Systems
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Lecture 16: Introduction to natural language processing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="17_intro_to_computer-vision.html">
   Lecture 17: Multi-class classification and introduction to computer vision
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="18_time-series.html">
   Lecture 18: Time series
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="19_survival-analysis.html">
   Lecture 19: Survival analysis
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="20_ethics.html">
   Lecture 20: Ethics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="21_communication.html">
   Lecture 21: Communication
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="22_deployment-conclusion.html">
   Lecture 22: Deployment and conclusion
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Attribution
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../attribution.html">
   Attributions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../LICENSE.html">
   LICENSE
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Varada Kolhatkar, CPSC 330 2021-22<br>Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/lectures/16_natural-language-processing.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/UBC-CS/cpsc330/master?urlpath=tree/lectures/16_natural-language-processing.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#announcements">
   Announcements
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#learning-objectives">
   Learning objectives
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#what-is-natural-language-processing-nlp">
   What is Natural Language Processing (NLP)?
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id1">
     What is Natural Language Processing (NLP)?
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#how-often-do-you-search-everyday">
       How often do you search everyday?
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id2">
     What is Natural Language Processing (NLP)?
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#everyday-nlp-applications">
     Everyday NLP applications
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#nlp-in-news">
     NLP in news
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#why-is-nlp-hard">
     Why is NLP hard?
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#example-lexical-ambiguity">
     Example: Lexical ambiguity
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#example-referential-ambiguity">
     Example: Referential ambiguity
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#ambiguous-news-headlines">
     Ambiguous news headlines
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#overall-goal">
     Overall goal
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#word-embeddings">
   Word Embeddings
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#why-do-we-care-about-word-representation">
     Why do we care about word representation?
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#word-meaning">
     Word meaning
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#word-meaning-ml-and-nlp-view">
     Word meaning: ML and NLP view
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#word-representations">
   Word representations
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#how-do-we-represent-words">
     How do we represent words?
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#need-a-representation-that-captures-relationships-between-words">
     Need a representation that captures relationships between words.
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#distributional-hypothesis">
     Distributional hypothesis
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#vector-space-model">
     Vector space model
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#term-term-co-occurrence-matrix">
     Term-term co-occurrence matrix
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#visualizing-word-vectors-and-similarity">
     Visualizing word vectors and similarity
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id3">
     Visualizing word vectors and similarity
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#sparse-vs-dense-word-vectors">
     Sparse vs. dense word vectors
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#alternative">
     Alternative
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#word2vec">
     Word2Vec
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#success-of-word2vec">
     Success of Word2Vec
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#finding-similar-words">
     Finding similar words
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#finding-similarity-scores-between-words">
     Finding similarity scores between words
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#examples-of-semantic-and-syntactic-relationships">
     Examples of semantic and syntactic relationships
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#implicit-biases-and-stereotypes-in-word-embeddings">
     Implicit biases and stereotypes in word embeddings
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#word-vectors-with-spacy">
     Word vectors with spaCy
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#representing-documents-using-word-embeddings">
     Representing documents using word embeddings
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#averaging-embeddings">
     Averaging embeddings
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#average-embeddings-with-spacy">
     Average embeddings with spaCy
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#similarity-between-documents">
     Similarity between documents
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#airline-sentiment-analysis-using-average-embedding-representation">
     Airline sentiment analysis using average embedding representation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#bag-of-words-representation-for-sentiment-analysis">
     Bag-of-words representation for sentiment analysis
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#sentiment-analysis-with-average-embedding-representation">
     Sentiment analysis with average embedding representation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#sentiment-classification-using-average-embeddings">
     Sentiment classification using average embeddings
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#sentiment-classification-using-advanced-sentence-representations">
     Sentiment classification using advanced sentence representations
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#break-5-min">
   Break (5 min)
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#topic-modeling">
   Topic modeling
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#topic-modeling-motivation">
     Topic modeling motivation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#example-a-corpus-of-news-articles">
     Example: A corpus of news articles
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#example-a-corpus-of-food-magazines">
     Example: A corpus of food magazines
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#a-corpus-of-scientific-articles">
     A corpus of scientific articles
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id4">
     Topic modeling motivation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id5">
     Topic modeling
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#topic-modeling-input-and-output">
     Topic modeling: Input and output
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#topic-modeling-example">
     Topic modeling: Example
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id6">
     Topic modeling: Example
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id7">
     Topic modeling: Input and output
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#topic-modeling-some-applications">
     Topic modeling: Some applications
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#topic-modeling-toy-example">
     Topic modeling toy example
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#topic-modeling-pipeline">
   Topic modeling pipeline
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#data">
     Data
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#preprocessing-the-corpus">
     Preprocessing the corpus
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#training-lda-with-gensim">
     Training LDA with gensim
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gensim-s-doc2bow">
     <code class="docutils literal notranslate">
      <span class="pre">
       Gensim
      </span>
     </code>
     ’s
     <code class="docutils literal notranslate">
      <span class="pre">
       doc2bow
      </span>
     </code>
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id8">
     <code class="docutils literal notranslate">
      <span class="pre">
       Gensim
      </span>
     </code>
     ’s
     <code class="docutils literal notranslate">
      <span class="pre">
       doc2bow
      </span>
     </code>
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#examine-the-topics-and-topic-distribution-for-a-document-in-our-lda-model">
     Examine the topics and topic distribution for a document in our LDA model
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#visualize-topics">
     Visualize topics
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#optional-topic-modeling-with-sklearn">
     (Optional) Topic modeling with
     <code class="docutils literal notranslate">
      <span class="pre">
       sklearn
      </span>
     </code>
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#basic-text-preprocessing">
   Basic text preprocessing
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#introduction">
     Introduction
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#tokenization">
     Tokenization
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#tokenization-sentence-segmentation">
     Tokenization: sentence segmentation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#sentence-segmentation">
     Sentence segmentation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#word-tokenization">
     Word tokenization
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id9">
     Word tokenization
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#word-segmentation">
     Word segmentation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#types-and-tokens">
     Types and tokens
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#exercise-for-you">
     Exercise for you
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#other-commonly-used-preprocessing-steps">
     Other commonly used preprocessing steps
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#punctuation-and-stopword-removal">
     Punctuation and stopword removal
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#lemmatization">
     Lemmatization
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#stemming">
     Stemming
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#other-tools-for-preprocessing">
     Other tools for preprocessing
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#spacy">
     spaCy
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#other-typical-nlp-tasks">
     Other typical NLP tasks
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#extracting-named-entities-using-spacy">
     Extracting named-entities using spaCy
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#dependency-parsing-using-spacy">
     Dependency parsing using spaCy
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#many-other-things-possible">
     Many other things possible
    </a>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <p><img alt="" src="../_images/330-banner.png" /></p>
<div class="section" id="lecture-16-introduction-to-natural-language-processing">
<h1>Lecture 16: Introduction to natural language processing<a class="headerlink" href="#lecture-16-introduction-to-natural-language-processing" title="Permalink to this headline">¶</a></h1>
<p>UBC 2020-21</p>
<p>Instructor: Varada Kolhatkar</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">## Imports</span>

<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">re</span>
<span class="kn">import</span> <span class="nn">string</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">Counter</span><span class="p">,</span> <span class="n">defaultdict</span>

<span class="kn">import</span> <span class="nn">IPython</span>
<span class="kn">import</span> <span class="nn">nltk</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">numpy.random</span> <span class="k">as</span> <span class="nn">npr</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">HTML</span>
<span class="kn">from</span> <span class="nn">ipywidgets</span> <span class="kn">import</span> <span class="n">interactive</span>
<span class="kn">from</span> <span class="nn">nltk.corpus</span> <span class="kn">import</span> <span class="n">stopwords</span>
<span class="kn">from</span> <span class="nn">nltk.tokenize</span> <span class="kn">import</span> <span class="n">sent_tokenize</span><span class="p">,</span> <span class="n">word_tokenize</span>
<span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">CountVectorizer</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">make_pipeline</span>
</pre></div>
</div>
</div>
</div>
<p><br><br></p>
<div class="section" id="announcements">
<h2>Announcements<a class="headerlink" href="#announcements" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Final exam: December 19th at 12:00pm <a class="reference external" href="https://learningspaces.ubc.ca/classrooms/phrm-1101">PHRM 1101</a></p></li>
<li><p>Homework 5 grades have been posted.</p></li>
<li><p>Homework 6 was due on Monday, Nov 15th at 11:59pm (submission window extended till November 16 at 10am.)</p></li>
<li><p>Homework 7 due Wednesday, Nov 17th at 11:59pm</p></li>
<li><p>It seems like many of you are quite overwhelmed. So I decided to skip one homework. We’ll only have one more homework after this.</p></li>
</ul>
</div>
<div class="section" id="learning-objectives">
<h2>Learning objectives<a class="headerlink" href="#learning-objectives" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Broadly explain what is natural language processing (NLP).</p></li>
<li><p>Name some common NLP applications.</p></li>
<li><p>Explain the general idea of a vector space model.</p></li>
<li><p>Explain the difference between different word representations: term-term co-occurrence matrix representation and Word2Vec representation.</p></li>
<li><p>Describe the reasons and benefits of using pre-trained embeddings.</p></li>
<li><p>Load and use pre-trained word embeddings to find word similarities and analogies.</p></li>
<li><p>Demonstrate biases in embeddings and learn to watch out for such biases in pre-trained embeddings.</p></li>
<li><p>Use word embeddings in text classification and document clustering using <code class="docutils literal notranslate"><span class="pre">spaCy</span></code>.</p></li>
<li><p>Explain the general idea of topic modeling.</p></li>
<li><p>Describe the input and output of topic modeling.</p></li>
<li><p>Carry out basic text preprocessing using <code class="docutils literal notranslate"><span class="pre">spaCy</span></code>.</p></li>
</ul>
<p><br><br></p>
</div>
<div class="section" id="what-is-natural-language-processing-nlp">
<h2>What is Natural Language Processing (NLP)?<a class="headerlink" href="#what-is-natural-language-processing-nlp" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>What should a search engine return when asked the following question?</p></li>
</ul>
<p><img alt="" src="../_images/lexical_ambiguity.png" /></p>
<!-- <center> -->
<!-- <img src="img/lexical_ambiguity.png" width="1000" height="1000"> -->
<!-- </center> --><div class="section" id="id1">
<h3>What is Natural Language Processing (NLP)?<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h3>
<div class="section" id="how-often-do-you-search-everyday">
<h4>How often do you search everyday?<a class="headerlink" href="#how-often-do-you-search-everyday" title="Permalink to this headline">¶</a></h4>
<p><img alt="" src="../_images/Google_search.png" /></p>
<!-- <center> -->
<!-- <img src="img/Google_search.png" width="900" height="900"> -->
<!-- </center> --></div>
</div>
<div class="section" id="id2">
<h3>What is Natural Language Processing (NLP)?<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h3>
<p><img alt="" src="../_images/WhatisNLP.png" /></p>
<!-- <center> -->
<!-- <img src="img/WhatisNLP.png" width="800" height="800"> -->
<!-- </center> -->    </div>
<div class="section" id="everyday-nlp-applications">
<h3>Everyday NLP applications<a class="headerlink" href="#everyday-nlp-applications" title="Permalink to this headline">¶</a></h3>
<p><img alt="" src="../_images/annotation-image.png" /></p>
<!-- <center> -->
<!-- <img src="img/annotation-image.png" height="1200" width="1200"> -->
<!-- </center> --></div>
<div class="section" id="nlp-in-news">
<h3>NLP in news<a class="headerlink" href="#nlp-in-news" title="Permalink to this headline">¶</a></h3>
<p>Often you’ll NLP in news. Some examples:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://abcnews.go.com/GMA/Wellness/suicide-prevention-boost-artificial-intelligence-exclusive/story?id=76541481">How suicide prevention is getting a boost from artificial intelligence</a></p></li>
<li><p><a class="reference external" href="https://www.nytimes.com/2020/11/24/science/artificial-intelligence-ai-gpt3.html">Meet GPT-3. It Has Learned to Code (and Blog and Argue).</a></p></li>
<li><p><a class="reference external" href="https://www.nytimes.com/2020/07/29/opinion/gpt-3-ai-automation.html">How Do You Know a Human Wrote This?</a></p></li>
<li><p>…</p></li>
</ul>
</div>
<div class="section" id="why-is-nlp-hard">
<h3>Why is NLP hard?<a class="headerlink" href="#why-is-nlp-hard" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Language is complex and subtle.</p></li>
<li><p>Language is ambiguous at different levels.</p></li>
<li><p>Language understanding involves common-sense knowledge and real-world reasoning.</p></li>
<li><p>All the problems related to representation and reasoning in artificial intelligence arise in this domain.</p></li>
</ul>
</div>
<div class="section" id="example-lexical-ambiguity">
<h3>Example: Lexical ambiguity<a class="headerlink" href="#example-lexical-ambiguity" title="Permalink to this headline">¶</a></h3>
<p><br><br></p>
<p><img alt="" src="../_images/lexical_ambiguity.png" /></p>
<!-- <img src="img/lexical_ambiguity.png" width="1000" height="1000"> -->
</div>
<div class="section" id="example-referential-ambiguity">
<h3>Example: Referential ambiguity<a class="headerlink" href="#example-referential-ambiguity" title="Permalink to this headline">¶</a></h3>
<p><br><br></p>
<!-- <img src="img/referential_ambiguity.png" width="1000" height="1000"> -->
<p><img alt="" src="../_images/referential_ambiguity.png" /></p>
</div>
<div class="section" id="ambiguous-news-headlines">
<h3><a class="reference external" href="http://www.fun-with-words.com/ambiguous_headlines.html">Ambiguous news headlines</a><a class="headerlink" href="#ambiguous-news-headlines" title="Permalink to this headline">¶</a></h3>
<blockquote>
PROSTITUTES APPEAL TO POPE
</blockquote>    
<ul class="simple">
<li><p><strong>appeal to</strong> means make a serious or urgent request or be attractive or interesting?</p></li>
</ul>
<blockquote>
KICKING BABY CONSIDERED TO BE HEALTHY    
</blockquote> 
<ul class="simple">
<li><p><strong>kicking</strong> is used as an adjective or a verb?</p></li>
</ul>
<blockquote>
MILK DRINKERS ARE TURNING TO POWDER
</blockquote>
<ul class="simple">
<li><p><strong>turning</strong> means becoming or take up?</p></li>
</ul>
</div>
<div class="section" id="overall-goal">
<h3>Overall goal<a class="headerlink" href="#overall-goal" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Give you a quick introduction to you of this important field in artificial intelligence which extensively used machine learning.</p></li>
</ul>
<p><img alt="" src="../_images/NLP_in_industry.png" /></p>
<!-- <center> -->
<!-- <img src="img/NLP_in_industry.png" width="900" height="800"> -->
<!-- </center> --><p>Today’s plan</p>
<ul class="simple">
<li><p>Word embeddings</p></li>
<li><p>Topic modeling</p></li>
<li><p>Basic text preprocessing</p></li>
</ul>
</div>
</div>
<div class="section" id="word-embeddings">
<h2>Word Embeddings<a class="headerlink" href="#word-embeddings" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>The idea is to represent word meaning so that similar words are close together.</p></li>
</ul>
<p><img alt="" src="../_images/t-SNE_word_embeddings.png" /></p>
<!-- <center> -->
<!-- <img src="img/t-SNE_word_embeddings.png" width="900" height="900"> -->
<!-- </center> -->    
<p>(Attribution: Jurafsky and Martin 3rd edition)</p>
<div class="section" id="why-do-we-care-about-word-representation">
<h3>Why do we care about word representation?<a class="headerlink" href="#why-do-we-care-about-word-representation" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>So far we have been talking about sentence or document representation.</p></li>
<li><p>Now we are going one step back and talking about word representation.</p></li>
<li><p>Although word representation cannot be directly used in text classification tasks such as sentiment analysis using tradition ML models, it’s good to know about word embeddings because they are so widely used.</p></li>
<li><p>They are quite useful in more advanced machine learning models such as recurrent neural networks.</p></li>
</ul>
</div>
<div class="section" id="word-meaning">
<h3>Word meaning<a class="headerlink" href="#word-meaning" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>A favourite topic of philosophers for centuries.</p></li>
<li><p>An example from legal domain: <a class="reference external" href="https://www.scc-csc.ca/case-dossier/info/sum-som-eng.aspx?cas=36258">Are hockey gloves gloves or “articles of plastics”?</a></p></li>
</ul>
<blockquote>
Canada (A.G.) v. Igloo Vikski Inc. was a tariff code case that made its way to the SCC (Supreme Court of Canada). The case disputed the definition of hockey gloves as either gloves or as "articles of plastics."
</blockquote>
<p><img alt="" src="../_images/hockey_gloves_case.png" /></p>
<!-- <center> -->
<!-- <img src="img/hockey_gloves_case.png" width="800" height="800"> -->
<!-- </center> --></div>
<div class="section" id="word-meaning-ml-and-nlp-view">
<h3>Word meaning: ML and NLP view<a class="headerlink" href="#word-meaning-ml-and-nlp-view" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Modeling word meaning that allows us to</p>
<ul>
<li><p>draw useful inferences to solve meaning-related problems</p></li>
<li><p>find relationship between words,</p>
<ul>
<li><p>E.g., which words are similar, which ones have positive or negative connotations</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</div>
<div class="section" id="word-representations">
<h2>Word representations<a class="headerlink" href="#word-representations" title="Permalink to this headline">¶</a></h2>
<div class="section" id="how-do-we-represent-words">
<h3>How do we represent words?<a class="headerlink" href="#how-do-we-represent-words" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Suppose you are building a question answering system and you are given the following question and three candidate answers.</p></li>
<li><p>What kind of relationship between words would we like our representation to capture in order to arrive at the correct answer?</p></li>
</ul>
<blockquote>       
<p style="font-size:30px"><b>Question:</b> How <b>tall</b> is Machu Picchu?</p>
    <p style="font-size:30px"><b>Candidate 1:</b> Machu Picchu is 13.164 degrees south of the equator.</p>    
<p style="font-size:30px"><b>Candidate 2:</b> The official height of Machu Picchu is 2,430 m.</p>
<p style="font-size:30px"><b>Candidate 3:</b> Machu Picchu is 80 kilometres (50 miles) northwest of Cusco.</p>    
</blockquote> 
</div>
<div class="section" id="need-a-representation-that-captures-relationships-between-words">
<h3>Need a representation that captures relationships between words.<a class="headerlink" href="#need-a-representation-that-captures-relationships-between-words" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>We will be looking at two such representations.</p>
<ol class="simple">
<li><p>Sparse representation with <strong>term-term co-occurrence matrix</strong></p></li>
<li><p>Dense representation with <strong>Word2Vec</strong></p></li>
</ol>
</li>
<li><p>Both are based on two ideas: <strong>distributional hypothesis</strong> and <strong>vector space model</strong>.</p></li>
</ul>
</div>
<div class="section" id="distributional-hypothesis">
<h3>Distributional hypothesis<a class="headerlink" href="#distributional-hypothesis" title="Permalink to this headline">¶</a></h3>
<blockquote> 
    <p>You shall know a word by the company it keeps.</p>
    <footer>Firth, 1957</footer>        
</blockquote>
<blockquote> 
If A and B have almost identical environments we say that they are synonyms.
<footer>Harris, 1954</footer>    
</blockquote>    
<p>Example:</p>
<ul class="simple">
<li><p>Her <strong>child</strong> loves to play in the playground.</p></li>
<li><p>Her <strong>kid</strong> loves to play in the playground.</p></li>
</ul>
</div>
<div class="section" id="vector-space-model">
<h3>Vector space model<a class="headerlink" href="#vector-space-model" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Model the meaning of a word by placing it into a vector space.</p></li>
<li><p>A standard way to represent meaning in NLP</p></li>
<li><p>The idea is to create <strong>embeddings of words</strong> so that distances among words in the vector space indicate the relationship between them.</p></li>
</ul>
<p><img alt="" src="../_images/t-SNE_word_embeddings.png" /></p>
<!-- <center> -->
<!-- <img src="img/t-SNE_word_embeddings.png" width="900" height="900"> -->
<!-- </center> -->    
<p>(Attribution: Jurafsky and Martin 3rd edition)</p>
</div>
<div class="section" id="term-term-co-occurrence-matrix">
<h3>Term-term co-occurrence matrix<a class="headerlink" href="#term-term-co-occurrence-matrix" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>So far we have been talking about documents and we created document-term co-occurrence matrix (e.g., bag-of-words representation of text).</p></li>
<li><p>We can also do this with words. The idea is to go through a corpus of text, keeping a count of all of the words that appear in context of each word (within a window).</p></li>
<li><p>An example:</p></li>
</ul>
<p><img alt="" src="../_images/term-term_comat.png" /></p>
<!-- <center> -->
<!-- <img src="img/term-term_comat.png" width="600" height="600"> -->
<!-- </center> -->
<p>(Credit: Jurafsky and Martin 3rd edition)</p>
</div>
<div class="section" id="visualizing-word-vectors-and-similarity">
<h3>Visualizing word vectors and similarity<a class="headerlink" href="#visualizing-word-vectors-and-similarity" title="Permalink to this headline">¶</a></h3>
<p><img alt="" src="../_images/word_vectors_and_angles.png" /></p>
<!-- <center> -->
<!-- <img src="img/word_vectors_and_angles.png" width="800" height="800"> -->
<!-- </center> -->
<p>(Credit: Jurafsky and Martin 3rd edition)</p>
<ul class="simple">
<li><p>The similarity is calculated using dot products between word vectors.</p>
<ul>
<li><p>Example: <span class="math notranslate nohighlight">\(\vec{\text{digital}}.\vec{\text{information}} = 0 \times 1 + 1\times 6 = 6\)</span></p></li>
<li><p>Higher the dot product more similar the words.</p></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="id3">
<h3>Visualizing word vectors and similarity<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h3>
<p><img alt="" src="../_images/word_vectors_and_angles.png" /></p>
<!-- <img src="img/word_vectors_and_angles.png" width="600" height="600"> -->
<p>(Credit: Jurafsky and Martin 3rd edition)</p>
<ul class="simple">
<li><p>The similarity is calculated using dot products between word vectors.</p>
<ul>
<li><p>Example: <span class="math notranslate nohighlight">\(\vec{\text{digital}}.\vec{\text{information}} = 0 \times 1 + 1\times 6 = 6\)</span></p></li>
<li><p>Higher the dot product more similar the words.</p></li>
</ul>
</li>
<li><p>We can also calculate a normalized version of dot products.
$<span class="math notranslate nohighlight">\(similarity_{cosine}(w_1,w_2) = \frac{w_1.w_2}{\left\lVert w_1\right\rVert_2 \left\lVert w_2\right\rVert_2}\)</span>$</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">### Let&#39;s build term-term co-occurrence matrix for our text.</span>
<span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;code/.&quot;</span><span class="p">)</span>

<span class="kn">from</span> <span class="nn">comat</span> <span class="kn">import</span> <span class="n">CooccurrenceMatrix</span>
<span class="kn">from</span> <span class="nn">preprocessing</span> <span class="kn">import</span> <span class="n">MyPreprocessor</span>

<span class="n">corpus</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;How tall is Machu Picchu?&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Machu Picchu is 13.164 degrees south of the equator.&quot;</span><span class="p">,</span>
    <span class="s2">&quot;The official height of Machu Picchu is 2,430 m.&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Machu Picchu is 80 kilometres (50 miles) northwest of Cusco.&quot;</span><span class="p">,</span>
    <span class="s2">&quot;It is 80 kilometres (50 miles) northwest of Cusco, on the crest of the mountain Machu Picchu, located about 2,430 metres (7,970 feet) above mean sea level, over 1,000 metres (3,300 ft) lower than Cusco, which has an elevation of 3,400 metres (11,200 ft).&quot;</span><span class="p">,</span>
<span class="p">]</span>
<span class="n">pp</span> <span class="o">=</span> <span class="n">MyPreprocessor</span><span class="p">()</span>
<span class="n">pp_corpus</span> <span class="o">=</span> <span class="n">pp</span><span class="o">.</span><span class="n">preprocess_corpus</span><span class="p">(</span><span class="n">corpus</span><span class="p">)</span>
<span class="n">cm</span> <span class="o">=</span> <span class="n">CooccurrenceMatrix</span><span class="p">(</span><span class="n">pp_corpus</span><span class="p">)</span>
<span class="n">vocab</span><span class="p">,</span> <span class="n">comat</span> <span class="o">=</span> <span class="n">cm</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">()</span>
<span class="n">words</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">key</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">vocab</span><span class="o">.</span><span class="n">items</span><span class="p">(),</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">item</span><span class="p">:</span> <span class="p">(</span><span class="n">item</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">item</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
<span class="p">]</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">comat</span><span class="o">.</span><span class="n">todense</span><span class="p">(),</span> <span class="n">columns</span><span class="o">=</span><span class="n">words</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">words</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int8</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>tall</th>
      <th>machu</th>
      <th>picchu</th>
      <th>13.164</th>
      <th>degrees</th>
      <th>south</th>
      <th>equator</th>
      <th>official</th>
      <th>height</th>
      <th>2,430</th>
      <th>...</th>
      <th>mean</th>
      <th>sea</th>
      <th>level</th>
      <th>1,000</th>
      <th>3,300</th>
      <th>ft</th>
      <th>lower</th>
      <th>elevation</th>
      <th>3,400</th>
      <th>11,200</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>tall</th>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>machu</th>
      <td>1</td>
      <td>0</td>
      <td>5</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>2</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>picchu</th>
      <td>1</td>
      <td>5</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>2</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>13.164</th>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>degrees</th>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 32 columns</p>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.metrics.pairwise</span> <span class="kn">import</span> <span class="n">cosine_similarity</span>


<span class="k">def</span> <span class="nf">similarity</span><span class="p">(</span><span class="n">word1</span><span class="p">,</span> <span class="n">word2</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns similarity score between word1 and word2</span>
<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    word1 -- (str)</span>
<span class="sd">        The first word</span>
<span class="sd">    word2 -- (str)</span>
<span class="sd">        The second word</span>

<span class="sd">    Returns</span>
<span class="sd">    --------</span>
<span class="sd">    None. Prints the similarity score between word1 and word2.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">vec1</span> <span class="o">=</span> <span class="n">cm</span><span class="o">.</span><span class="n">get_word_vector</span><span class="p">(</span><span class="n">word1</span><span class="p">)</span><span class="o">.</span><span class="n">todense</span><span class="p">()</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
    <span class="n">vec2</span> <span class="o">=</span> <span class="n">cm</span><span class="o">.</span><span class="n">get_word_vector</span><span class="p">(</span><span class="n">word2</span><span class="p">)</span><span class="o">.</span><span class="n">todense</span><span class="p">()</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
    <span class="n">v1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">vec1</span><span class="p">))</span>
    <span class="n">v2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">vec2</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span>
        <span class="s2">&quot;The dot product between </span><span class="si">%s</span><span class="s2"> and </span><span class="si">%s</span><span class="s2"> is </span><span class="si">%0.2f</span><span class="s2"> and cosine similarity is </span><span class="si">%0.2f</span><span class="s2">&quot;</span>
        <span class="o">%</span> <span class="p">(</span><span class="n">word1</span><span class="p">,</span> <span class="n">word2</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">v1</span><span class="p">,</span> <span class="n">v2</span><span class="p">),</span> <span class="n">cosine_similarity</span><span class="p">(</span><span class="n">vec1</span><span class="p">,</span> <span class="n">vec2</span><span class="p">))</span>
    <span class="p">)</span>


<span class="n">similarity</span><span class="p">(</span><span class="s2">&quot;tall&quot;</span><span class="p">,</span> <span class="s2">&quot;height&quot;</span><span class="p">)</span>
<span class="n">similarity</span><span class="p">(</span><span class="s2">&quot;tall&quot;</span><span class="p">,</span> <span class="s2">&quot;official&quot;</span><span class="p">)</span>

<span class="c1">### Not very reliable similarity scores because we used only 4 sentences.</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The dot product between tall and height is 2.00 and cosine similarity is 0.71
The dot product between tall and official is 2.00 and cosine similarity is 0.82
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>We are able to capture some similarities between words now.</p></li>
<li><p>That said similarities do not make much sense in the toy example above because we’re using a tiny corpus.</p></li>
<li><p>To find meaningful patterns of similarities between words, we need a large corpus.</p></li>
<li><p>Let’s try a bit larger corpus and check whether the similarities make sense.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">wikipedia</span>
<span class="kn">from</span> <span class="nn">nltk.tokenize</span> <span class="kn">import</span> <span class="n">sent_tokenize</span><span class="p">,</span> <span class="n">word_tokenize</span>

<span class="n">corpus</span> <span class="o">=</span> <span class="p">[]</span>

<span class="n">queries</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;Machu Picchu&quot;</span><span class="p">,</span> <span class="s2">&quot;human stature&quot;</span><span class="p">,</span> <span class="s2">&quot;Everest&quot;</span><span class="p">]</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">queries</span><span class="p">)):</span>
    <span class="n">sents</span> <span class="o">=</span> <span class="n">sent_tokenize</span><span class="p">(</span><span class="n">wikipedia</span><span class="o">.</span><span class="n">page</span><span class="p">(</span><span class="n">queries</span><span class="p">[</span><span class="n">i</span><span class="p">])</span><span class="o">.</span><span class="n">content</span><span class="p">)</span>
    <span class="n">corpus</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">sents</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Number of sentences in the corpus: &quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">sents</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Number of sentences in the corpus:  681
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pp</span> <span class="o">=</span> <span class="n">MyPreprocessor</span><span class="p">()</span>
<span class="n">pp_corpus</span> <span class="o">=</span> <span class="n">pp</span><span class="o">.</span><span class="n">preprocess_corpus</span><span class="p">(</span><span class="n">corpus</span><span class="p">)</span>
<span class="n">cm</span> <span class="o">=</span> <span class="n">CooccurrenceMatrix</span><span class="p">(</span><span class="n">pp_corpus</span><span class="p">)</span>
<span class="n">vocab</span><span class="p">,</span> <span class="n">comat</span> <span class="o">=</span> <span class="n">cm</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">()</span>
<span class="n">words</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">key</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">vocab</span><span class="o">.</span><span class="n">items</span><span class="p">(),</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">item</span><span class="p">:</span> <span class="p">(</span><span class="n">item</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">item</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
<span class="p">]</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">comat</span><span class="o">.</span><span class="n">todense</span><span class="p">(),</span> <span class="n">columns</span><span class="o">=</span><span class="n">words</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">words</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int8</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>machu</th>
      <th>picchu</th>
      <th>15th-century</th>
      <th>inca</th>
      <th>citadel</th>
      <th>located</th>
      <th>eastern</th>
      <th>cordillera</th>
      <th>southern</th>
      <th>peru</th>
      <th>...</th>
      <th>panorama</th>
      <th>drawing</th>
      <th>imaging</th>
      <th>panoramas</th>
      <th>interactive</th>
      <th>summitpost</th>
      <th>format</th>
      <th>quicktime</th>
      <th>virtual</th>
      <th>info-graphic</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>machu</th>
      <td>0</td>
      <td>92</td>
      <td>1</td>
      <td>6</td>
      <td>2</td>
      <td>2</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>5</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>picchu</th>
      <td>92</td>
      <td>0</td>
      <td>1</td>
      <td>6</td>
      <td>4</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>4</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>15th-century</th>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>inca</th>
      <td>6</td>
      <td>6</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>2</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>citadel</th>
      <td>2</td>
      <td>4</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 5591 columns</p>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">similarity</span><span class="p">(</span><span class="s2">&quot;tall&quot;</span><span class="p">,</span> <span class="s2">&quot;height&quot;</span><span class="p">)</span>
<span class="n">similarity</span><span class="p">(</span><span class="s2">&quot;tall&quot;</span><span class="p">,</span> <span class="s2">&quot;official&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The dot product between tall and height is 38.00 and cosine similarity is 0.13
The dot product between tall and official is 0.00 and cosine similarity is 0.00
</pre></div>
</div>
</div>
</div>
<p><br><br></p>
</div>
<div class="section" id="sparse-vs-dense-word-vectors">
<h3>Sparse vs. dense word vectors<a class="headerlink" href="#sparse-vs-dense-word-vectors" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Term-term co-occurrence matrices are long and sparse.</p>
<ul>
<li><p>length |V| is usually large (e.g., &gt; 50,000)</p></li>
<li><p>most elements are zero</p></li>
</ul>
</li>
<li><p>OK because there are efficient ways to deal with sparse matrices.</p></li>
</ul>
</div>
<div class="section" id="alternative">
<h3>Alternative<a class="headerlink" href="#alternative" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Learn short (~100 to 1000 dimensions) and dense vectors.</p></li>
<li><p>Short vectors may be easier to train with ML models (less weights to train).</p></li>
<li><p>They may generalize better.</p></li>
<li><p>In practice they work much better!</p></li>
</ul>
</div>
<div class="section" id="word2vec">
<h3>Word2Vec<a class="headerlink" href="#word2vec" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>A family of algorithms to create dense word embeddings</p></li>
</ul>
<p><img alt="" src="../_images/word2vec.png" /></p>
<!-- <img src="img/word2vec.png" width="1000" height="1000"> -->
</div>
<div class="section" id="success-of-word2vec">
<h3>Success of Word2Vec<a class="headerlink" href="#success-of-word2vec" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Able to capture complex relationships between words.</p></li>
<li><p>Example: What is the word that is similar to <strong>WOMAN</strong> in the same sense as <strong>KING</strong> is similar to <strong>MAN</strong>?</p></li>
<li><p>Perform a simple algebraic operations with the vector representation of words.
<span class="math notranslate nohighlight">\(\vec{X} = \vec{\text{KING}} − \vec{\text{MAN}} + \vec{\text{WOMAN}}\)</span></p></li>
<li><p>Search in the vector space for the word closest to <span class="math notranslate nohighlight">\(\vec{X}\)</span> measured by cosine distance.</p></li>
</ul>
<p><img alt="" src="../_images/word_analogies1.png" /></p>
<!-- <center> -->
<!-- <img src="img/word_analogies1.png" width="500" height="500"> -->
<!-- </center> -->
<p>(Credit: Mikolov et al. 2013)</p>
<ul class="simple">
<li><p>We can create a dense representation with a library called <code class="docutils literal notranslate"><span class="pre">gensim</span></code>.</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">conda</span> <span class="n">install</span> <span class="o">-</span><span class="n">c</span> <span class="n">anaconda</span> <span class="n">gensim</span>
</pre></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">gensim.models</span> <span class="kn">import</span> <span class="n">Word2Vec</span>

<span class="n">sentences</span> <span class="o">=</span> <span class="p">[[</span><span class="s2">&quot;cat&quot;</span><span class="p">,</span> <span class="s2">&quot;say&quot;</span><span class="p">,</span> <span class="s2">&quot;meow&quot;</span><span class="p">],</span> <span class="p">[</span><span class="s2">&quot;dog&quot;</span><span class="p">,</span> <span class="s2">&quot;say&quot;</span><span class="p">,</span> <span class="s2">&quot;woof&quot;</span><span class="p">]]</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Word2Vec</span><span class="p">(</span><span class="n">sentences</span><span class="p">,</span> <span class="n">min_count</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/Users/kvarada/opt/miniconda3/envs/cpsc330/lib/python3.9/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package &lt;https://pypi.org/project/python-Levenshtein/&gt; is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.
  warnings.warn(msg)
</pre></div>
</div>
</div>
</div>
<p>Let’s look at the word vector of the word <em>cat</em>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">wv</span><span class="p">[</span><span class="s2">&quot;cat&quot;</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([-0.00713902,  0.00124103, -0.00717672, -0.00224462,  0.0037193 ,
        0.00583312,  0.00119818,  0.00210273, -0.00411039,  0.00722533,
       -0.00630704,  0.00464721, -0.00821997,  0.00203647, -0.00497705,
       -0.00424769, -0.00310899,  0.00565521,  0.0057984 , -0.00497465,
        0.00077333, -0.00849578,  0.00780981,  0.00925729, -0.00274233,
        0.00080022,  0.00074665,  0.00547788, -0.00860608,  0.00058445,
        0.00686942,  0.00223159,  0.00112468, -0.00932216,  0.00848237,
       -0.00626413, -0.00299237,  0.00349379, -0.00077263,  0.00141129,
        0.00178199, -0.0068289 , -0.00972481,  0.00904058,  0.00619805,
       -0.00691293,  0.00340348,  0.00020606,  0.00475374, -0.00711994,
        0.00402695,  0.00434743,  0.00995737, -0.00447374, -0.00138927,
       -0.00731732, -0.00969783, -0.00908026, -0.00102276, -0.00650329,
        0.00484973, -0.00616403,  0.00251919,  0.00073944, -0.00339216,
       -0.00097922,  0.00997912,  0.00914589, -0.00446183,  0.00908303,
       -0.00564176,  0.00593092, -0.00309722,  0.00343175,  0.00301723,
        0.00690046, -0.00237388,  0.00877504,  0.00758943, -0.00954765,
       -0.00800821, -0.0076379 ,  0.00292326, -0.00279472, -0.00692952,
       -0.00812826,  0.00830918,  0.00199049, -0.00932802, -0.00479272,
        0.00313674, -0.00471321,  0.00528084, -0.00423344,  0.00264179,
       -0.00804569,  0.00620989,  0.00481889,  0.00078719,  0.00301345],
      dtype=float32)
</pre></div>
</div>
</div>
</div>
<p>What’s the most similar word to the word <em>cat</em>?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="s2">&quot;cat&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[(&#39;dog&#39;, 0.17018885910511017),
 (&#39;woof&#39;, 0.004503009375184774),
 (&#39;say&#39;, -0.027750369161367416),
 (&#39;meow&#39;, -0.04461709037423134)]
</pre></div>
</div>
</div>
</div>
<p>This is good. But if you want good and meaningful representations of words you need to train models on a large corpus such as the whole Wikipedia, which is computationally intensive.</p>
<p>So instead of training our own models, we use the <strong>pre-trained embeddings</strong>. These are the word embeddings people have trained embeddings on huge corpora and made them available for us to use.</p>
<p>Let’s try out Google news pre-trained word vectors.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># It&#39;ll take a while to run this when you try it out for the first time.</span>
<span class="kn">import</span> <span class="nn">gensim.downloader</span> <span class="k">as</span> <span class="nn">api</span>

<span class="n">google_news_vectors</span> <span class="o">=</span> <span class="n">api</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;word2vec-google-news-300&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">KeyboardInterrupt</span><span class="g g-Whitespace">                         </span>Traceback (most recent call last)
<span class="o">/</span><span class="n">var</span><span class="o">/</span><span class="n">folders</span><span class="o">/</span><span class="mi">80</span><span class="o">/</span><span class="n">kr9rkqfj4w78h49djkz8yy9r0000gp</span><span class="o">/</span><span class="n">T</span><span class="o">/</span><span class="n">ipykernel_16765</span><span class="o">/</span><span class="mf">1679810779.</span><span class="n">py</span> <span class="ow">in</span> <span class="o">&lt;</span><span class="n">module</span><span class="o">&gt;</span>
<span class="g g-Whitespace">      </span><span class="mi">2</span> <span class="kn">import</span> <span class="nn">gensim.downloader</span> <span class="k">as</span> <span class="nn">api</span>
<span class="g g-Whitespace">      </span><span class="mi">3</span> 
<span class="ne">----&gt; </span><span class="mi">4</span> <span class="n">google_news_vectors</span> <span class="o">=</span> <span class="n">api</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;word2vec-google-news-300&quot;</span><span class="p">)</span>

<span class="nn">~/opt/miniconda3/envs/cpsc330/lib/python3.9/site-packages/gensim/downloader.py</span> in <span class="ni">load</span><span class="nt">(name, return_path)</span>
<span class="g g-Whitespace">    </span><span class="mi">501</span>         <span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">BASE_DIR</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">502</span>         <span class="n">module</span> <span class="o">=</span> <span class="nb">__import__</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
<span class="ne">--&gt; </span><span class="mi">503</span>         <span class="k">return</span> <span class="n">module</span><span class="o">.</span><span class="n">load_data</span><span class="p">()</span>
<span class="g g-Whitespace">    </span><span class="mi">504</span> 
<span class="g g-Whitespace">    </span><span class="mi">505</span> 

<span class="nn">~/gensim-data/word2vec-google-news-300/__init__.py</span> in <span class="ni">load_data</span><span class="nt">()</span>
<span class="g g-Whitespace">      </span><span class="mi">6</span> <span class="k">def</span> <span class="nf">load_data</span><span class="p">():</span>
<span class="g g-Whitespace">      </span><span class="mi">7</span>     <span class="n">path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">base_dir</span><span class="p">,</span> <span class="s1">&#39;word2vec-google-news-300&#39;</span><span class="p">,</span> <span class="s2">&quot;word2vec-google-news-300.gz&quot;</span><span class="p">)</span>
<span class="ne">----&gt; </span><span class="mi">8</span>     <span class="n">model</span> <span class="o">=</span> <span class="n">KeyedVectors</span><span class="o">.</span><span class="n">load_word2vec_format</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">binary</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="g g-Whitespace">      </span><span class="mi">9</span>     <span class="k">return</span> <span class="n">model</span>

<span class="nn">~/opt/miniconda3/envs/cpsc330/lib/python3.9/site-packages/gensim/models/keyedvectors.py</span> in <span class="ni">load_word2vec_format</span><span class="nt">(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header)</span>
<span class="g g-Whitespace">   </span><span class="mi">1628</span> 
<span class="g g-Whitespace">   </span><span class="mi">1629</span>         <span class="sd">&quot;&quot;&quot;</span>
<span class="ne">-&gt; </span><span class="mi">1630</span><span class="sd">         return _load_word2vec_format(</span>
<span class="g g-Whitespace">   </span><span class="mi">1631</span><span class="sd">             cls, fname, fvocab=fvocab, binary=binary, encoding=encoding, unicode_errors=unicode_errors,</span>
<span class="g g-Whitespace">   </span><span class="mi">1632</span><span class="sd">             limit=limit, datatype=datatype, no_header=no_header,</span>

<span class="nn">~/opt/miniconda3/envs/cpsc330/lib/python3.9/site-packages/gensim/models/keyedvectors.py</span> in <span class="ni">_load_word2vec_format</span><span class="nt">(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header, binary_chunk_size)</span>
<span class="g g-Whitespace">   </span><span class="mi">1907</span><span class="sd"> </span>
<span class="g g-Whitespace">   </span><span class="mi">1908</span><span class="sd">         if binary:</span>
<span class="ne">-&gt; </span><span class="mi">1909</span><span class="sd">             _word2vec_read_binary(</span>
<span class="g g-Whitespace">   </span><span class="mi">1910</span><span class="sd">                 fin, kv, counts, vocab_size, vector_size, datatype, unicode_errors, binary_chunk_size,</span>
<span class="g g-Whitespace">   </span><span class="mi">1911</span><span class="sd">             )</span>

<span class="nn">~/opt/miniconda3/envs/cpsc330/lib/python3.9/site-packages/gensim/models/keyedvectors.py</span> in <span class="ni">_word2vec_read_binary</span><span class="nt">(fin, kv, counts, vocab_size, vector_size, datatype, unicode_errors, binary_chunk_size)</span>
<span class="g g-Whitespace">   </span><span class="mi">1802</span><span class="sd">         new_chunk = fin.read(binary_chunk_size)</span>
<span class="g g-Whitespace">   </span><span class="mi">1803</span><span class="sd">         chunk += new_chunk</span>
<span class="ne">-&gt; </span><span class="mi">1804</span><span class="sd">         processed_words, chunk = _add_bytes_to_kv(</span>
<span class="g g-Whitespace">   </span><span class="mi">1805</span><span class="sd">             kv, counts, chunk, vocab_size, vector_size, datatype, unicode_errors)</span>
<span class="g g-Whitespace">   </span><span class="mi">1806</span><span class="sd">         tot_processed_words += processed_words</span>

<span class="nn">~/opt/miniconda3/envs/cpsc330/lib/python3.9/site-packages/gensim/models/keyedvectors.py</span> in <span class="ni">_add_bytes_to_kv</span><span class="nt">(kv, counts, chunk, vocab_size, vector_size, datatype, unicode_errors)</span>
<span class="g g-Whitespace">   </span><span class="mi">1788</span><span class="sd">         word = word.lstrip(&#39;\n&#39;)</span>
<span class="g g-Whitespace">   </span><span class="mi">1789</span><span class="sd">         vector = frombuffer(chunk, offset=i_vector, count=vector_size, dtype=REAL).astype(datatype)</span>
<span class="ne">-&gt; </span><span class="mi">1790</span><span class="sd">         _add_word_to_kv(kv, counts, word, vector, vocab_size)</span>
<span class="g g-Whitespace">   </span><span class="mi">1791</span><span class="sd">         start = i_vector + bytes_per_vector</span>
<span class="g g-Whitespace">   </span><span class="mi">1792</span><span class="sd">         processed_words += 1</span>

<span class="nn">~/opt/miniconda3/envs/cpsc330/lib/python3.9/site-packages/gensim/models/keyedvectors.py</span> in <span class="ni">_add_word_to_kv</span><span class="nt">(kv, counts, word, weights, vocab_size)</span>
<span class="g g-Whitespace">   </span><span class="mi">1768</span><span class="sd">         logger.warning(&quot;vocabulary file is incomplete: &#39;%s&#39; is missing&quot;, word)</span>
<span class="g g-Whitespace">   </span><span class="mi">1769</span><span class="sd">         word_count = None</span>
<span class="ne">-&gt; </span><span class="mi">1770</span><span class="sd">     kv.set_vecattr(word, &#39;count&#39;, word_count)</span>
<span class="g g-Whitespace">   </span><span class="mi">1771</span><span class="sd"> </span>
<span class="g g-Whitespace">   </span><span class="mi">1772</span><span class="sd"> </span>

<span class="nn">~/opt/miniconda3/envs/cpsc330/lib/python3.9/site-packages/gensim/models/keyedvectors.py</span> in <span class="ni">set_vecattr</span><span class="nt">(self, key, attr, val)</span>
<span class="g g-Whitespace">    </span><span class="mi">328</span><span class="sd">         &quot;&quot;&quot;</span>
<span class="g g-Whitespace">    </span><span class="mi">329</span>         <span class="bp">self</span><span class="o">.</span><span class="n">allocate_vecattrs</span><span class="p">(</span><span class="n">attrs</span><span class="o">=</span><span class="p">[</span><span class="n">attr</span><span class="p">],</span> <span class="n">types</span><span class="o">=</span><span class="p">[</span><span class="nb">type</span><span class="p">(</span><span class="n">val</span><span class="p">)])</span>
<span class="ne">--&gt; </span><span class="mi">330</span>         <span class="n">index</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_index</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">331</span>         <span class="bp">self</span><span class="o">.</span><span class="n">expandos</span><span class="p">[</span><span class="n">attr</span><span class="p">][</span><span class="n">index</span><span class="p">]</span> <span class="o">=</span> <span class="n">val</span>
<span class="g g-Whitespace">    </span><span class="mi">332</span> 

<span class="nn">~/opt/miniconda3/envs/cpsc330/lib/python3.9/site-packages/gensim/models/keyedvectors.py</span> in <span class="ni">get_index</span><span class="nt">(self, key, default)</span>
<span class="g g-Whitespace">    </span><span class="mi">386</span> 
<span class="g g-Whitespace">    </span><span class="mi">387</span>         <span class="s2">&quot;&quot;&quot;</span>
<span class="ne">--&gt; </span><span class="mi">388</span><span class="s2">         val = self.key_to_index.get(key, -1)</span>
<span class="g g-Whitespace">    </span><span class="mi">389</span><span class="s2">         if val &gt;= 0:</span>
<span class="g g-Whitespace">    </span><span class="mi">390</span><span class="s2">             return val</span>

<span class="ne">KeyboardInterrupt</span>: 
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Size of vocabulary: &quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">google_news_vectors</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Size of vocabulary:  3000000
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">google_news_vectors</span></code> above has 300 dimensional word vectors for 3,000,000 unique words from Google news.</p></li>
<li><p>What can we do with these word vectors?</p></li>
</ul>
</div>
<div class="section" id="finding-similar-words">
<h3>Finding similar words<a class="headerlink" href="#finding-similar-words" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Given word <span class="math notranslate nohighlight">\(w\)</span>, search in the vector space for the word closest to <span class="math notranslate nohighlight">\(w\)</span> as measured by cosine distance.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">google_news_vectors</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="s2">&quot;UBC&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[(&#39;UVic&#39;, 0.7886475920677185),
 (&#39;SFU&#39;, 0.7588528394699097),
 (&#39;Simon_Fraser&#39;, 0.7356574535369873),
 (&#39;UFV&#39;, 0.6880435943603516),
 (&#39;VIU&#39;, 0.6778583526611328),
 (&#39;Kwantlen&#39;, 0.6771429181098938),
 (&#39;UBCO&#39;, 0.6734487414360046),
 (&#39;UPEI&#39;, 0.6731126308441162),
 (&#39;UBC_Okanagan&#39;, 0.6709133982658386),
 (&#39;Lakehead_University&#39;, 0.6622507572174072)]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">google_news_vectors</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="s2">&quot;information&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[(&#39;info&#39;, 0.7363681793212891),
 (&#39;infomation&#39;, 0.680029571056366),
 (&#39;infor_mation&#39;, 0.673384964466095),
 (&#39;informaiton&#39;, 0.6639009118080139),
 (&#39;informa_tion&#39;, 0.6601256728172302),
 (&#39;informationon&#39;, 0.6339334845542908),
 (&#39;informationabout&#39;, 0.6320980191230774),
 (&#39;Information&#39;, 0.6186580657958984),
 (&#39;informaion&#39;, 0.6093292236328125),
 (&#39;details&#39;, 0.6063088774681091)]
</pre></div>
</div>
</div>
</div>
<p>If you want to extract all documents containing information or similar words, you could use this information.</p>
</div>
<div class="section" id="finding-similarity-scores-between-words">
<h3>Finding similarity scores between words<a class="headerlink" href="#finding-similarity-scores-between-words" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">google_news_vectors</span><span class="o">.</span><span class="n">similarity</span><span class="p">(</span><span class="s2">&quot;Canada&quot;</span><span class="p">,</span> <span class="s2">&quot;hockey&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.27610135
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">google_news_vectors</span><span class="o">.</span><span class="n">similarity</span><span class="p">(</span><span class="s2">&quot;China&quot;</span><span class="p">,</span> <span class="s2">&quot;hockey&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.060384665
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">word_pairs</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">(</span><span class="s2">&quot;height&quot;</span><span class="p">,</span> <span class="s2">&quot;tall&quot;</span><span class="p">),</span>
    <span class="p">(</span><span class="s2">&quot;pineapple&quot;</span><span class="p">,</span> <span class="s2">&quot;mango&quot;</span><span class="p">),</span>
    <span class="p">(</span><span class="s2">&quot;pineapple&quot;</span><span class="p">,</span> <span class="s2">&quot;juice&quot;</span><span class="p">),</span>
    <span class="p">(</span><span class="s2">&quot;sun&quot;</span><span class="p">,</span> <span class="s2">&quot;robot&quot;</span><span class="p">),</span>
    <span class="p">(</span><span class="s2">&quot;GPU&quot;</span><span class="p">,</span> <span class="s2">&quot;lion&quot;</span><span class="p">),</span>
<span class="p">]</span>
<span class="k">for</span> <span class="n">pair</span> <span class="ow">in</span> <span class="n">word_pairs</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span>
        <span class="s2">&quot;The similarity between </span><span class="si">%s</span><span class="s2"> and </span><span class="si">%s</span><span class="s2"> is </span><span class="si">%0.3f</span><span class="s2">&quot;</span>
        <span class="o">%</span> <span class="p">(</span><span class="n">pair</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">pair</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">google_news_vectors</span><span class="o">.</span><span class="n">similarity</span><span class="p">(</span><span class="n">pair</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">pair</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
    <span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The similarity between height and tall is 0.473
The similarity between pineapple and mango is 0.668
The similarity between pineapple and juice is 0.418
The similarity between sun and robot is 0.029
The similarity between GPU and lion is 0.002
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">analogy</span><span class="p">(</span><span class="n">word1</span><span class="p">,</span> <span class="n">word2</span><span class="p">,</span> <span class="n">word3</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">google_news_vectors</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns analogy word using the given model.</span>

<span class="sd">    Parameters</span>
<span class="sd">    --------------</span>
<span class="sd">    word1 : (str)</span>
<span class="sd">        word1 in the analogy relation</span>
<span class="sd">    word2 : (str)</span>
<span class="sd">        word2 in the analogy relation</span>
<span class="sd">    word3 : (str)</span>
<span class="sd">        word3 in the analogy relation</span>
<span class="sd">    model :</span>
<span class="sd">        word embedding model</span>

<span class="sd">    Returns</span>
<span class="sd">    ---------------</span>
<span class="sd">        pd.dataframe</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">%s</span><span class="s2"> : </span><span class="si">%s</span><span class="s2"> :: </span><span class="si">%s</span><span class="s2"> : ?&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">word1</span><span class="p">,</span> <span class="n">word2</span><span class="p">,</span> <span class="n">word3</span><span class="p">))</span>
    <span class="n">sim_words</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="n">positive</span><span class="o">=</span><span class="p">[</span><span class="n">word3</span><span class="p">,</span> <span class="n">word2</span><span class="p">],</span> <span class="n">negative</span><span class="o">=</span><span class="p">[</span><span class="n">word1</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">sim_words</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;Analogy word&quot;</span><span class="p">,</span> <span class="s2">&quot;Score&quot;</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">analogy</span><span class="p">(</span><span class="s2">&quot;man&quot;</span><span class="p">,</span> <span class="s2">&quot;king&quot;</span><span class="p">,</span> <span class="s2">&quot;woman&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>man : king :: woman : ?
</pre></div>
</div>
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Analogy word</th>
      <th>Score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>queen</td>
      <td>0.711819</td>
    </tr>
    <tr>
      <th>1</th>
      <td>monarch</td>
      <td>0.618967</td>
    </tr>
    <tr>
      <th>2</th>
      <td>princess</td>
      <td>0.590243</td>
    </tr>
    <tr>
      <th>3</th>
      <td>crown_prince</td>
      <td>0.549946</td>
    </tr>
    <tr>
      <th>4</th>
      <td>prince</td>
      <td>0.537732</td>
    </tr>
    <tr>
      <th>5</th>
      <td>kings</td>
      <td>0.523684</td>
    </tr>
    <tr>
      <th>6</th>
      <td>Queen_Consort</td>
      <td>0.523595</td>
    </tr>
    <tr>
      <th>7</th>
      <td>queens</td>
      <td>0.518113</td>
    </tr>
    <tr>
      <th>8</th>
      <td>sultan</td>
      <td>0.509859</td>
    </tr>
    <tr>
      <th>9</th>
      <td>monarchy</td>
      <td>0.508741</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">analogy</span><span class="p">(</span><span class="s2">&quot;Montreal&quot;</span><span class="p">,</span> <span class="s2">&quot;Canadiens&quot;</span><span class="p">,</span> <span class="s2">&quot;Vancouver&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Montreal : Canadiens :: Vancouver : ?
</pre></div>
</div>
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Analogy word</th>
      <th>Score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Canucks</td>
      <td>0.821327</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Vancouver_Canucks</td>
      <td>0.750401</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Calgary_Flames</td>
      <td>0.705470</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Leafs</td>
      <td>0.695783</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Maple_Leafs</td>
      <td>0.691617</td>
    </tr>
    <tr>
      <th>5</th>
      <td>Thrashers</td>
      <td>0.687504</td>
    </tr>
    <tr>
      <th>6</th>
      <td>Avs</td>
      <td>0.681716</td>
    </tr>
    <tr>
      <th>7</th>
      <td>Sabres</td>
      <td>0.665307</td>
    </tr>
    <tr>
      <th>8</th>
      <td>Blackhawks</td>
      <td>0.664625</td>
    </tr>
    <tr>
      <th>9</th>
      <td>Habs</td>
      <td>0.661023</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">analogy</span><span class="p">(</span><span class="s2">&quot;Toronto&quot;</span><span class="p">,</span> <span class="s2">&quot;UofT&quot;</span><span class="p">,</span> <span class="s2">&quot;Vancouver&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Toronto : UofT :: Vancouver : ?
</pre></div>
</div>
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Analogy word</th>
      <th>Score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>SFU</td>
      <td>0.579245</td>
    </tr>
    <tr>
      <th>1</th>
      <td>UVic</td>
      <td>0.576921</td>
    </tr>
    <tr>
      <th>2</th>
      <td>UBC</td>
      <td>0.571431</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Simon_Fraser</td>
      <td>0.543464</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Langara_College</td>
      <td>0.541347</td>
    </tr>
    <tr>
      <th>5</th>
      <td>UVIC</td>
      <td>0.520495</td>
    </tr>
    <tr>
      <th>6</th>
      <td>Grant_MacEwan</td>
      <td>0.517273</td>
    </tr>
    <tr>
      <th>7</th>
      <td>UFV</td>
      <td>0.514150</td>
    </tr>
    <tr>
      <th>8</th>
      <td>Ubyssey</td>
      <td>0.510421</td>
    </tr>
    <tr>
      <th>9</th>
      <td>Kwantlen</td>
      <td>0.503807</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
</div>
<div class="section" id="examples-of-semantic-and-syntactic-relationships">
<h3>Examples of semantic and syntactic relationships<a class="headerlink" href="#examples-of-semantic-and-syntactic-relationships" title="Permalink to this headline">¶</a></h3>
<p><img alt="" src="../_images/word_analogies2.png" /></p>
<!-- <img src="img/word_analogies2.png" width="800" height="800"> -->
<p>(Credit: Mikolov 2013)</p>
</div>
<div class="section" id="implicit-biases-and-stereotypes-in-word-embeddings">
<h3>Implicit biases and stereotypes in word embeddings<a class="headerlink" href="#implicit-biases-and-stereotypes-in-word-embeddings" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Reflect gender stereotypes present in broader society.</p></li>
<li><p>They may also amplify these stereotypes because of their widespread usage.</p></li>
<li><p>See the paper <a class="reference external" href="http://papers.nips.cc/paper/6228-man-is-to-computer-programmer-as-woman-is-to-homemaker-debiasing-word-embeddings.pdf">Man is to Computer Programmer as Woman is to …</a>.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">analogy</span><span class="p">(</span><span class="s2">&quot;man&quot;</span><span class="p">,</span> <span class="s2">&quot;computer_programmer&quot;</span><span class="p">,</span> <span class="s2">&quot;woman&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>man : computer_programmer :: woman : ?
</pre></div>
</div>
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Analogy word</th>
      <th>Score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>homemaker</td>
      <td>0.562712</td>
    </tr>
    <tr>
      <th>1</th>
      <td>housewife</td>
      <td>0.510505</td>
    </tr>
    <tr>
      <th>2</th>
      <td>graphic_designer</td>
      <td>0.505180</td>
    </tr>
    <tr>
      <th>3</th>
      <td>schoolteacher</td>
      <td>0.497949</td>
    </tr>
    <tr>
      <th>4</th>
      <td>businesswoman</td>
      <td>0.493489</td>
    </tr>
    <tr>
      <th>5</th>
      <td>paralegal</td>
      <td>0.492551</td>
    </tr>
    <tr>
      <th>6</th>
      <td>registered_nurse</td>
      <td>0.490797</td>
    </tr>
    <tr>
      <th>7</th>
      <td>saleswoman</td>
      <td>0.488163</td>
    </tr>
    <tr>
      <th>8</th>
      <td>electrical_engineer</td>
      <td>0.479773</td>
    </tr>
    <tr>
      <th>9</th>
      <td>mechanical_engineer</td>
      <td>0.475540</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p><img alt="" src="../_images/eva-srsly.png" /></p>
<p>Most of the modern embeddings are de-biased.</p>
<p>(NLP)### Other pre-trained embeddings</p>
<p>A number of pre-trained word embeddings are available. The most popular ones are:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://code.google.com/archive/p/word2vec/">Word2Vec</a></p>
<ul>
<li><p>trained on several corpora using the word2vec algorithm</p></li>
</ul>
</li>
<li><p><a class="reference external" href="https://wikipedia2vec.github.io/wikipedia2vec/pretrained/">wikipedia2vec</a></p>
<ul>
<li><p>pretrained embeddings for 12 languages</p></li>
</ul>
</li>
<li><p><a class="reference external" href="https://nlp.stanford.edu/projects/glove/">GloVe</a></p>
<ul>
<li><p>trained using <a class="reference external" href="https://nlp.stanford.edu/pubs/glove.pdf">the GloVe algorithm</a></p></li>
<li><p>published by Stanford University</p></li>
</ul>
</li>
<li><p><a class="reference external" href="https://fasttext.cc/docs/en/pretrained-vectors.html">fastText pre-trained embeddings for 294 languages</a></p>
<ul>
<li><p>trained using <a class="reference external" href="http://aclweb.org/anthology/Q17-1010">the fastText algorithm</a></p></li>
<li><p>published by Facebook</p></li>
</ul>
</li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Note that these pre-trained word vectors are of big size (in several gigabytes).</p>
</div>
<p>Here is a list of all pre-trained embeddings available with <code class="docutils literal notranslate"><span class="pre">gensim</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">gensim.downloader</span>

<span class="nb">print</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">gensim</span><span class="o">.</span><span class="n">downloader</span><span class="o">.</span><span class="n">info</span><span class="p">()[</span><span class="s2">&quot;models&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">keys</span><span class="p">()))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;fasttext-wiki-news-subwords-300&#39;, &#39;conceptnet-numberbatch-17-06-300&#39;, &#39;word2vec-ruscorpora-300&#39;, &#39;word2vec-google-news-300&#39;, &#39;glove-wiki-gigaword-50&#39;, &#39;glove-wiki-gigaword-100&#39;, &#39;glove-wiki-gigaword-200&#39;, &#39;glove-wiki-gigaword-300&#39;, &#39;glove-twitter-25&#39;, &#39;glove-twitter-50&#39;, &#39;glove-twitter-100&#39;, &#39;glove-twitter-200&#39;, &#39;__testing_word2vec-matrix-synopsis&#39;]
</pre></div>
</div>
</div>
</div>
<p><br><br></p>
</div>
<div class="section" id="word-vectors-with-spacy">
<h3>Word vectors with spaCy<a class="headerlink" href="#word-vectors-with-spacy" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>spaCy also gives you access to word vectors with bigger models: <code class="docutils literal notranslate"><span class="pre">en_core_web_md</span></code> or <code class="docutils literal notranslate"><span class="pre">en_core_web_lr</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">spaCy</span></code>’s pre-trained embeddings are trained on <a class="reference external" href="https://catalog.ldc.upenn.edu/LDC2013T19">OntoNotes corpus</a>.</p></li>
<li><p>This corpus has a collection of different styles of texts such as telephone conversations, newswire, newsgroups, broadcast news, broadcast conversation, weblogs, religious texts.</p></li>
<li><p>Let’s try it out.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">spacy</span>

<span class="n">nlp</span> <span class="o">=</span> <span class="n">spacy</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;en_core_web_md&quot;</span><span class="p">)</span>

<span class="n">doc</span> <span class="o">=</span> <span class="n">nlp</span><span class="p">(</span><span class="s2">&quot;pineapple&quot;</span><span class="p">)</span>
<span class="n">doc</span><span class="o">.</span><span class="n">vector</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([-2.8445e-01,  5.5363e-01,  4.9800e-01, -2.7769e-01,  9.3481e-02,
        5.8978e-01, -6.1267e-01,  1.5415e-01,  3.7752e-01,  1.4348e-01,
       -2.0126e-01,  2.6869e-01, -7.2758e-01,  2.4405e-01,  3.5321e-01,
       -3.8314e-01,  1.8920e-01,  9.0860e-01,  2.0685e-01,  6.7174e-02,
        4.9190e-01,  4.8224e-01,  1.2929e-01,  4.9490e-01, -9.3981e-02,
       -6.0443e-01,  2.8314e-01, -7.4459e-02,  5.9333e-02, -9.5484e-01,
       -1.2755e-01,  1.1871e-01,  2.9725e-01, -1.8604e-01, -4.9672e-01,
       -1.4352e-01, -1.2770e-02,  1.0423e-01, -6.7861e-01,  7.6421e-01,
        2.0364e-02, -3.7836e-02,  2.9399e-01, -4.1602e-01,  3.1965e-01,
        9.8503e-01, -6.8950e-02, -4.0057e-01, -1.3972e-01,  3.2916e-01,
       -8.3725e-02,  7.1081e-02,  4.5407e-01, -6.0930e-02,  8.5099e-01,
       -7.3595e-01,  3.5860e-01, -1.7554e-01, -1.5838e-01,  3.1525e-02,
       -1.4029e-01,  9.3486e-02,  4.2341e-01,  2.2277e-01, -8.6968e-02,
       -4.4123e-01,  1.1326e-01,  1.1435e-01, -8.7185e-01,  5.2618e-01,
       -8.7317e-02,  8.8284e-01, -2.1643e-01,  4.1260e-01, -1.9629e-01,
        2.4883e-01,  8.2752e-02, -1.0800e-01, -3.7362e-01, -2.0995e-01,
        7.9307e-02, -2.3805e-01, -1.8537e-01, -1.1211e-01, -8.4294e-02,
       -2.9430e-01,  1.2991e+00,  8.2207e-01, -8.0108e-01, -3.9065e-01,
       -8.9309e-02, -3.7571e-01,  5.4184e-01, -3.5247e-02, -2.2929e-01,
        6.1834e-01, -1.7637e-01, -1.0375e-01, -5.3831e-01,  2.0187e-01,
        4.1651e-01, -4.8564e-01, -9.3836e-02, -1.6272e-01, -2.3528e-01,
       -1.0950e+00,  1.7226e-01, -5.8347e-01,  1.0375e+00,  3.8504e-01,
       -4.3338e-01, -2.8801e-01, -9.3194e-01,  1.1582e-01,  2.1183e-01,
       -4.7209e-02,  5.2106e-02,  1.0386e-01,  3.1998e-01,  3.3064e-01,
       -7.5731e-02,  1.5550e-01, -1.3200e-01, -4.2069e-01, -4.0630e-01,
       -3.9150e-02, -1.1429e-01,  7.1862e-02, -4.1747e-01, -3.3113e-02,
       -5.8413e-01, -5.5429e-01, -4.2042e-01, -1.1280e-02, -3.3124e-01,
       -9.0115e-02,  3.8209e-01,  1.2274e-01, -3.2571e-01,  3.5727e-02,
       -2.1267e+00,  6.5307e-01, -8.7850e-02,  2.7738e-01, -3.8208e-01,
       -6.4254e-02, -3.8063e-01,  7.2693e-01,  2.5506e-01, -4.4696e-01,
        5.4132e-01,  1.2553e-01,  5.2933e-01,  3.7021e-01, -1.0748e+00,
       -5.3942e-01,  3.4178e-01,  1.5028e-01,  3.1055e-01,  6.1673e-01,
       -9.9166e-02,  4.2590e-01,  1.4452e-01, -1.6359e-01, -1.5430e-01,
       -8.1012e-05, -7.2212e-02, -1.7403e-01,  1.6741e-01,  2.3766e-01,
        2.2990e-01,  3.2401e-01,  3.2437e-01,  1.4617e-01, -8.6237e-01,
       -1.6606e-01,  2.5771e-01, -2.3734e-01,  3.6835e-01, -6.2753e-01,
       -2.7425e-01, -1.2848e-01,  2.9455e-01, -2.4697e-01, -3.8050e-01,
       -3.1025e-01, -2.7280e-01, -4.8886e-01, -5.8880e-01, -1.1161e-01,
       -8.8502e-01, -2.7199e-02, -3.8848e-02, -2.2087e-01, -1.6948e-01,
       -2.2928e-01,  6.5381e-01,  1.0211e-01, -2.4038e-01,  3.0906e-01,
       -6.8702e-01,  9.5880e-02,  5.4251e-01,  2.5873e-01,  3.2799e-01,
        8.3257e-02,  3.0840e-01,  3.2223e-01,  5.4355e-01,  7.9113e-01,
       -7.5587e-02,  4.0929e-01,  3.6541e-01, -2.4388e-02, -6.5300e-01,
        1.9694e-01, -4.9399e-02, -3.0366e-01, -1.9463e-01, -2.2815e-01,
       -6.4144e-01,  2.7839e-02,  5.5869e-01,  3.9416e-01,  5.1166e-01,
        1.4871e-01, -2.2047e-01,  9.8443e-02, -2.8448e-01,  1.8304e-02,
       -7.6165e-01, -1.8388e-01, -2.8458e-01,  1.3345e-01, -5.5309e-01,
       -4.3729e-01,  7.7534e-02, -4.9492e-01,  4.6410e-01,  2.4458e-01,
       -1.6128e-01, -6.6307e-01, -8.2413e-01, -3.3230e-01, -9.4846e-02,
       -1.1402e-01,  1.6437e-01, -1.4815e-01,  9.4412e-03,  4.3647e-01,
       -1.3735e-01,  5.2660e-02, -4.8225e-01,  3.3501e-01, -1.4767e-01,
        3.3204e-01,  3.3985e-01,  1.5218e-01,  3.5595e-01,  1.4562e-01,
       -1.3912e-01, -5.4815e-02, -5.7572e-01, -2.6978e-01, -1.4739e-01,
       -2.7083e-01, -4.4642e-01,  2.3675e-02, -7.0934e-01, -3.0456e-01,
       -3.2839e-01, -1.1621e-02,  1.7080e-01,  7.2882e-02, -4.0110e-01,
        8.1377e-01, -3.8940e-01,  3.7111e-01, -6.4290e-01,  4.0496e-01,
        2.7992e-02,  1.7528e-01,  3.3878e-01,  4.3610e-01,  4.3892e-01,
        1.5003e-01,  4.4302e-01,  4.0898e-01,  3.8504e-01, -1.5575e-01,
        2.0590e-01,  1.0659e-02, -2.5892e-01, -1.3161e-01, -5.7390e-01,
        6.2052e-01, -2.7414e-01, -1.0214e+00, -5.5313e-02,  5.6540e-01],
      dtype=float32)
</pre></div>
</div>
</div>
</div>
<p><br><br></p>
</div>
<div class="section" id="representing-documents-using-word-embeddings">
<h3>Representing documents using word embeddings<a class="headerlink" href="#representing-documents-using-word-embeddings" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Assuming that we have reasonable representations of words.</p></li>
<li><p>How do we represent meaning of paragraphs or documents?</p></li>
<li><p>Two simple approaches</p>
<ul>
<li><p>Averaging embeddings</p></li>
<li><p>Concatenating embeddings</p></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="averaging-embeddings">
<h3>Averaging embeddings<a class="headerlink" href="#averaging-embeddings" title="Permalink to this headline">¶</a></h3>
<blockquote>
All empty promises
</blockquote>
<p><span class="math notranslate nohighlight">\((embedding(all) + embedding(empty) + embedding(promise))/3\)</span></p>
</div>
<div class="section" id="average-embeddings-with-spacy">
<h3>Average embeddings with spaCy<a class="headerlink" href="#average-embeddings-with-spacy" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>We can do this conveniently with <a class="reference external" href="https://spacy.io/usage/linguistic-features#vectors-similarity">spaCy</a>.</p></li>
<li><p>We need <code class="docutils literal notranslate"><span class="pre">en_core_web_md</span></code> model to access word vectors.</p></li>
<li><p>You can download the model by going to command line and in your course <code class="docutils literal notranslate"><span class="pre">conda</span></code> environment and download <code class="docutils literal notranslate"><span class="pre">en_core_web_md</span></code> as follows.</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">conda</span> <span class="n">activate</span> <span class="n">cpsc330</span>
<span class="n">python</span> <span class="o">-</span><span class="n">m</span> <span class="n">spacy</span> <span class="n">download</span> <span class="n">en_core_web_md</span>
</pre></div>
</div>
<p>We can access word vectors for individual words in <code class="docutils literal notranslate"><span class="pre">spaCy</span></code> as follows.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">nlp</span><span class="p">(</span><span class="s2">&quot;empty&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">vector</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">10</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([ 1.0127  , -0.20527 , -0.24555 , -0.076636,  0.11981 ,  0.21642 ,
       -0.55584 ,  0.020206,  0.39778 ,  1.7218  ], dtype=float32)
</pre></div>
</div>
</div>
</div>
<p>We can get average embeddings for a sentence or a document in <code class="docutils literal notranslate"><span class="pre">spaCy</span></code> as follows:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">s</span> <span class="o">=</span> <span class="s2">&quot;All empty promises&quot;</span>
<span class="n">doc</span> <span class="o">=</span> <span class="n">nlp</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
<span class="n">avg_sent_emb</span> <span class="o">=</span> <span class="n">doc</span><span class="o">.</span><span class="n">vector</span>
<span class="nb">print</span><span class="p">(</span><span class="n">avg_sent_emb</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Vector for: </span><span class="si">{}</span><span class="se">\n</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">((</span><span class="n">s</span><span class="p">),</span> <span class="p">(</span><span class="n">avg_sent_emb</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">10</span><span class="p">])))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(300,)
Vector for: All empty promises
[ 0.28903252 -0.09684668 -0.11497    -0.26554868  0.01983    -0.111594
 -0.10229333  0.126915    0.17705734  2.1837332 ]
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="similarity-between-documents">
<h3>Similarity between documents<a class="headerlink" href="#similarity-between-documents" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>We can also get similarity between documents as follows.</p></li>
<li><p>Note that this is based on average embeddings of each sentence.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">doc1</span> <span class="o">=</span> <span class="n">nlp</span><span class="p">(</span><span class="s2">&quot;Deep learning is very popular these days.&quot;</span><span class="p">)</span>
<span class="n">doc2</span> <span class="o">=</span> <span class="n">nlp</span><span class="p">(</span><span class="s2">&quot;Machine learning is dominated by neural networks.&quot;</span><span class="p">)</span>
<span class="n">doc3</span> <span class="o">=</span> <span class="n">nlp</span><span class="p">(</span><span class="s2">&quot;A home-made fresh bread with butter and cheese.&quot;</span><span class="p">)</span>

<span class="c1"># Similarity of two documents</span>
<span class="nb">print</span><span class="p">(</span><span class="n">doc1</span><span class="p">,</span> <span class="s2">&quot;&lt;-&gt;&quot;</span><span class="p">,</span> <span class="n">doc2</span><span class="p">,</span> <span class="n">doc1</span><span class="o">.</span><span class="n">similarity</span><span class="p">(</span><span class="n">doc2</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">doc2</span><span class="p">,</span> <span class="s2">&quot;&lt;-&gt;&quot;</span><span class="p">,</span> <span class="n">doc3</span><span class="p">,</span> <span class="n">doc2</span><span class="o">.</span><span class="n">similarity</span><span class="p">(</span><span class="n">doc3</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Deep learning is very popular these days. &lt;-&gt; Machine learning is dominated by neural networks. 0.7564516644025884
Machine learning is dominated by neural networks. &lt;-&gt; A home-made fresh bread with butter and cheese. 0.5363564587815752
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Do these scores make sense?</p></li>
<li><p>There are no common words, but we are still able to identify that doc1 and doc2 are more similar that doc2 and doc3.</p></li>
<li><p>You can use such average embedding representation in text classification tasks.</p></li>
</ul>
</div>
<div class="section" id="airline-sentiment-analysis-using-average-embedding-representation">
<h3>Airline sentiment analysis using average embedding representation<a class="headerlink" href="#airline-sentiment-analysis-using-average-embedding-representation" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Let’s try average embedding representation for airline sentiment analysis.</p></li>
<li><p>We used this dataset last week so you should already have it in the data directory. If not you can download it <a class="reference external" href="https://www.kaggle.com/jaskarancr/airline-sentiment-dataset">here</a>.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;data/Airline-Sentiment-2-w-AA.csv&quot;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s2">&quot;ISO-8859-1&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_validate</span><span class="p">,</span> <span class="n">train_test_split</span>

<span class="n">train_df</span><span class="p">,</span> <span class="n">test_df</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span> <span class="o">=</span> <span class="n">train_df</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">],</span> <span class="n">train_df</span><span class="p">[</span><span class="s2">&quot;airline_sentiment&quot;</span><span class="p">]</span>
<span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">test_df</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">],</span> <span class="n">test_df</span><span class="p">[</span><span class="s2">&quot;airline_sentiment&quot;</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">train_df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>_unit_id</th>
      <th>_golden</th>
      <th>_unit_state</th>
      <th>_trusted_judgments</th>
      <th>_last_judgment_at</th>
      <th>airline_sentiment</th>
      <th>airline_sentiment:confidence</th>
      <th>negativereason</th>
      <th>negativereason:confidence</th>
      <th>airline</th>
      <th>airline_sentiment_gold</th>
      <th>name</th>
      <th>negativereason_gold</th>
      <th>retweet_count</th>
      <th>text</th>
      <th>tweet_coord</th>
      <th>tweet_created</th>
      <th>tweet_id</th>
      <th>tweet_location</th>
      <th>user_timezone</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>5789</th>
      <td>681455792</td>
      <td>False</td>
      <td>finalized</td>
      <td>3</td>
      <td>2/25/15 4:21</td>
      <td>negative</td>
      <td>1.0</td>
      <td>Can't Tell</td>
      <td>0.6667</td>
      <td>Southwest</td>
      <td>NaN</td>
      <td>mrssuperdimmock</td>
      <td>NaN</td>
      <td>0</td>
      <td>@SouthwestAir link doesn't work</td>
      <td>NaN</td>
      <td>2/19/15 18:53</td>
      <td>5.686040e+17</td>
      <td>Lake Arrowhead, CA</td>
      <td>Pacific Time (US &amp; Canada)</td>
    </tr>
    <tr>
      <th>8918</th>
      <td>681459957</td>
      <td>False</td>
      <td>finalized</td>
      <td>3</td>
      <td>2/25/15 9:45</td>
      <td>neutral</td>
      <td>1.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>Delta</td>
      <td>NaN</td>
      <td>labeles</td>
      <td>NaN</td>
      <td>0</td>
      <td>@JetBlue okayyyy. But I had huge irons on way ...</td>
      <td>NaN</td>
      <td>2/17/15 10:18</td>
      <td>5.677500e+17</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>11688</th>
      <td>681462990</td>
      <td>False</td>
      <td>finalized</td>
      <td>3</td>
      <td>2/25/15 9:53</td>
      <td>negative</td>
      <td>1.0</td>
      <td>Customer Service Issue</td>
      <td>0.6727</td>
      <td>US Airways</td>
      <td>NaN</td>
      <td>DropMeAnywhere</td>
      <td>NaN</td>
      <td>0</td>
      <td>@USAirways They're all reservations numbers an...</td>
      <td>[0.0, 0.0]</td>
      <td>2/17/15 14:50</td>
      <td>5.678190e+17</td>
      <td>Here, There and Everywhere</td>
      <td>Arizona</td>
    </tr>
    <tr>
      <th>413</th>
      <td>681448905</td>
      <td>False</td>
      <td>finalized</td>
      <td>3</td>
      <td>2/25/15 10:10</td>
      <td>neutral</td>
      <td>1.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>Virgin America</td>
      <td>NaN</td>
      <td>jsamaudio</td>
      <td>NaN</td>
      <td>0</td>
      <td>@VirginAmerica no A's channel this year?</td>
      <td>NaN</td>
      <td>2/18/15 12:25</td>
      <td>5.681440e+17</td>
      <td>St. Francis (Calif.)</td>
      <td>Pacific Time (US &amp; Canada)</td>
    </tr>
    <tr>
      <th>4135</th>
      <td>681454122</td>
      <td>False</td>
      <td>finalized</td>
      <td>3</td>
      <td>2/25/15 10:08</td>
      <td>negative</td>
      <td>1.0</td>
      <td>Bad Flight</td>
      <td>0.3544</td>
      <td>United</td>
      <td>NaN</td>
      <td>CajunSQL</td>
      <td>NaN</td>
      <td>0</td>
      <td>@united missed it.  Incoming on time, then Sat...</td>
      <td>NaN</td>
      <td>2/17/15 14:20</td>
      <td>5.678110e+17</td>
      <td>Baton Rouge, LA</td>
      <td>NaN</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
</div>
<div class="section" id="bag-of-words-representation-for-sentiment-analysis">
<h3>Bag-of-words representation for sentiment analysis<a class="headerlink" href="#bag-of-words-representation-for-sentiment-analysis" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pipe</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span>
    <span class="n">CountVectorizer</span><span class="p">(</span><span class="n">stop_words</span><span class="o">=</span><span class="s2">&quot;english&quot;</span><span class="p">),</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">max_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
<span class="p">)</span>
<span class="n">pipe</span><span class="o">.</span><span class="n">named_steps</span><span class="p">[</span><span class="s2">&quot;countvectorizer&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">X_train_transformed</span> <span class="o">=</span> <span class="n">pipe</span><span class="o">.</span><span class="n">named_steps</span><span class="p">[</span><span class="s2">&quot;countvectorizer&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Data matrix shape:&quot;</span><span class="p">,</span> <span class="n">X_train_transformed</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">pipe</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Data matrix shape: (11712, 13064)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Train accuracy </span><span class="si">{:.2f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">pipe</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Test accuracy </span><span class="si">{:.2f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">pipe</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Train accuracy 0.94
Test accuracy 0.80
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="sentiment-analysis-with-average-embedding-representation">
<h3>Sentiment analysis with average embedding representation<a class="headerlink" href="#sentiment-analysis-with-average-embedding-representation" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Let’s see how can we get word vectors using <code class="docutils literal notranslate"><span class="pre">spaCy</span></code>.</p></li>
<li><p>Let’s create average embedding representation for each example.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_train_embeddings</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">([</span><span class="n">text</span><span class="o">.</span><span class="n">vector</span> <span class="k">for</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">nlp</span><span class="o">.</span><span class="n">pipe</span><span class="p">(</span><span class="n">X_train</span><span class="p">)])</span>
<span class="n">X_test_embeddings</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">([</span><span class="n">text</span><span class="o">.</span><span class="n">vector</span> <span class="k">for</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">nlp</span><span class="o">.</span><span class="n">pipe</span><span class="p">(</span><span class="n">X_test</span><span class="p">)])</span>
</pre></div>
</div>
</div>
</div>
<p>We have reduced dimensionality from 13,064 to 300!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_train_embeddings</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(11712, 300)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_train_embeddings</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
      <th>6</th>
      <th>7</th>
      <th>8</th>
      <th>9</th>
      <th>...</th>
      <th>290</th>
      <th>291</th>
      <th>292</th>
      <th>293</th>
      <th>294</th>
      <th>295</th>
      <th>296</th>
      <th>297</th>
      <th>298</th>
      <th>299</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>-0.102596</td>
      <td>0.089241</td>
      <td>-0.236022</td>
      <td>0.030403</td>
      <td>-0.183757</td>
      <td>0.188970</td>
      <td>-0.057322</td>
      <td>-0.194976</td>
      <td>-0.025315</td>
      <td>1.887160</td>
      <td>...</td>
      <td>-0.186171</td>
      <td>-0.001768</td>
      <td>0.088228</td>
      <td>0.095176</td>
      <td>0.195018</td>
      <td>-0.085129</td>
      <td>-0.113560</td>
      <td>-0.108034</td>
      <td>0.095647</td>
      <td>0.274220</td>
    </tr>
    <tr>
      <th>1</th>
      <td>-0.063396</td>
      <td>0.124589</td>
      <td>-0.122292</td>
      <td>-0.189419</td>
      <td>-0.101237</td>
      <td>0.002607</td>
      <td>0.032192</td>
      <td>-0.151461</td>
      <td>-0.055412</td>
      <td>1.796026</td>
      <td>...</td>
      <td>-0.183419</td>
      <td>-0.016063</td>
      <td>-0.005495</td>
      <td>-0.045449</td>
      <td>0.092108</td>
      <td>0.052184</td>
      <td>0.099207</td>
      <td>0.011452</td>
      <td>0.004731</td>
      <td>0.065721</td>
    </tr>
    <tr>
      <th>2</th>
      <td>-0.064436</td>
      <td>0.148900</td>
      <td>-0.170613</td>
      <td>-0.155509</td>
      <td>0.216841</td>
      <td>0.002881</td>
      <td>0.078774</td>
      <td>-0.123730</td>
      <td>-0.041174</td>
      <td>2.200783</td>
      <td>...</td>
      <td>-0.276544</td>
      <td>0.022344</td>
      <td>-0.029640</td>
      <td>0.003638</td>
      <td>0.070680</td>
      <td>-0.000402</td>
      <td>-0.029196</td>
      <td>-0.033838</td>
      <td>0.040326</td>
      <td>0.118550</td>
    </tr>
    <tr>
      <th>3</th>
      <td>-0.029201</td>
      <td>0.253583</td>
      <td>-0.020073</td>
      <td>0.035106</td>
      <td>0.027043</td>
      <td>-0.122413</td>
      <td>0.076435</td>
      <td>-0.253022</td>
      <td>-0.044745</td>
      <td>1.879360</td>
      <td>...</td>
      <td>-0.070406</td>
      <td>-0.012429</td>
      <td>-0.075428</td>
      <td>-0.106797</td>
      <td>0.218251</td>
      <td>-0.218904</td>
      <td>-0.120035</td>
      <td>-0.085280</td>
      <td>0.002369</td>
      <td>0.034781</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.077888</td>
      <td>0.213935</td>
      <td>-0.108230</td>
      <td>0.056640</td>
      <td>0.071779</td>
      <td>0.035684</td>
      <td>-0.034920</td>
      <td>-0.154641</td>
      <td>-0.004615</td>
      <td>1.916412</td>
      <td>...</td>
      <td>-0.063743</td>
      <td>0.002112</td>
      <td>-0.026977</td>
      <td>-0.102208</td>
      <td>0.037013</td>
      <td>-0.101183</td>
      <td>-0.003163</td>
      <td>0.025034</td>
      <td>-0.087998</td>
      <td>0.123148</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 300 columns</p>
</div></div></div>
</div>
</div>
<div class="section" id="sentiment-classification-using-average-embeddings">
<h3>Sentiment classification using average embeddings<a class="headerlink" href="#sentiment-classification-using-average-embeddings" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>What are the train and test accuracies with average word embedding representation?</p></li>
<li><p>The accuracy is a bit better with less overfitting.</p></li>
<li><p>Note that we are using <strong>transfer learning</strong> here.</p></li>
<li><p>The embeddings are trained on a completely different corpus.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lgr</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">max_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
<span class="n">lgr</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_embeddings</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Train accuracy </span><span class="si">{:.2f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">lgr</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train_embeddings</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Test accuracy </span><span class="si">{:.2f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">lgr</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test_embeddings</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Train accuracy 0.81
Test accuracy 0.81
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="sentiment-classification-using-advanced-sentence-representations">
<h3>Sentiment classification using advanced sentence representations<a class="headerlink" href="#sentiment-classification-using-advanced-sentence-representations" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Since, representing documents is so essential for text classification tasks, there are more advanced methods for document representation.</p></li>
<li><p>In homework 7, you also explore sentence embedding representation.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sentence_transformers</span> <span class="kn">import</span> <span class="n">SentenceTransformer</span>

<span class="n">embedder</span> <span class="o">=</span> <span class="n">SentenceTransformer</span><span class="p">(</span><span class="s2">&quot;paraphrase-distilroberta-base-v1&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">emb_sents</span> <span class="o">=</span> <span class="n">embedder</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="s2">&quot;all empty promises&quot;</span><span class="p">)</span>
<span class="n">emb_sents</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(768,)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">emb_train</span> <span class="o">=</span> <span class="n">embedder</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">train_df</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">())</span>
<span class="n">emb_train_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">emb_train</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">train_df</span><span class="o">.</span><span class="n">index</span><span class="p">)</span>
<span class="n">emb_train_df</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
      <th>6</th>
      <th>7</th>
      <th>8</th>
      <th>9</th>
      <th>...</th>
      <th>758</th>
      <th>759</th>
      <th>760</th>
      <th>761</th>
      <th>762</th>
      <th>763</th>
      <th>764</th>
      <th>765</th>
      <th>766</th>
      <th>767</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>5789</th>
      <td>-0.120494</td>
      <td>0.250262</td>
      <td>-0.022795</td>
      <td>-0.116368</td>
      <td>0.078650</td>
      <td>0.037357</td>
      <td>-0.251341</td>
      <td>0.321429</td>
      <td>-0.143984</td>
      <td>-0.123487</td>
      <td>...</td>
      <td>0.199151</td>
      <td>-0.150143</td>
      <td>0.167078</td>
      <td>-0.407671</td>
      <td>-0.066161</td>
      <td>0.049514</td>
      <td>0.019385</td>
      <td>-0.357601</td>
      <td>0.125996</td>
      <td>0.381073</td>
    </tr>
    <tr>
      <th>8918</th>
      <td>-0.182954</td>
      <td>0.118282</td>
      <td>0.066341</td>
      <td>-0.136099</td>
      <td>0.094947</td>
      <td>-0.121303</td>
      <td>0.069233</td>
      <td>-0.097500</td>
      <td>0.025739</td>
      <td>-0.367980</td>
      <td>...</td>
      <td>0.113612</td>
      <td>0.114661</td>
      <td>0.049926</td>
      <td>0.256736</td>
      <td>-0.118687</td>
      <td>-0.190720</td>
      <td>0.011986</td>
      <td>-0.141883</td>
      <td>-0.230142</td>
      <td>0.024899</td>
    </tr>
    <tr>
      <th>11688</th>
      <td>-0.032988</td>
      <td>0.630251</td>
      <td>-0.079516</td>
      <td>0.148981</td>
      <td>0.194709</td>
      <td>-0.226264</td>
      <td>-0.043630</td>
      <td>0.217398</td>
      <td>-0.010716</td>
      <td>0.069644</td>
      <td>...</td>
      <td>0.676791</td>
      <td>0.244484</td>
      <td>0.051042</td>
      <td>0.064099</td>
      <td>-0.146945</td>
      <td>0.090878</td>
      <td>-0.090059</td>
      <td>0.077212</td>
      <td>-0.209226</td>
      <td>0.308773</td>
    </tr>
    <tr>
      <th>413</th>
      <td>-0.119259</td>
      <td>0.172168</td>
      <td>0.098698</td>
      <td>0.319859</td>
      <td>0.415475</td>
      <td>0.248360</td>
      <td>-0.025923</td>
      <td>0.385350</td>
      <td>0.066414</td>
      <td>-0.334289</td>
      <td>...</td>
      <td>-0.128482</td>
      <td>-0.232446</td>
      <td>-0.077805</td>
      <td>0.181328</td>
      <td>0.123244</td>
      <td>-0.143693</td>
      <td>0.660456</td>
      <td>-0.048714</td>
      <td>0.204774</td>
      <td>0.163496</td>
    </tr>
    <tr>
      <th>4135</th>
      <td>0.094240</td>
      <td>0.360193</td>
      <td>0.213747</td>
      <td>0.363690</td>
      <td>0.275521</td>
      <td>0.134936</td>
      <td>-0.276319</td>
      <td>0.009336</td>
      <td>-0.021523</td>
      <td>-0.258992</td>
      <td>...</td>
      <td>0.474885</td>
      <td>0.242125</td>
      <td>0.294532</td>
      <td>0.279014</td>
      <td>0.037831</td>
      <td>0.089761</td>
      <td>-0.548748</td>
      <td>-0.049258</td>
      <td>0.154525</td>
      <td>0.141268</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>5218</th>
      <td>-0.204409</td>
      <td>-0.145290</td>
      <td>-0.064201</td>
      <td>0.213571</td>
      <td>-0.140225</td>
      <td>0.338555</td>
      <td>-0.148578</td>
      <td>0.224516</td>
      <td>-0.042963</td>
      <td>0.075930</td>
      <td>...</td>
      <td>-0.161949</td>
      <td>0.040582</td>
      <td>0.003971</td>
      <td>-0.152549</td>
      <td>-0.582907</td>
      <td>-0.126527</td>
      <td>0.060502</td>
      <td>-0.111495</td>
      <td>-0.097492</td>
      <td>0.199321</td>
    </tr>
    <tr>
      <th>12252</th>
      <td>0.108408</td>
      <td>0.438293</td>
      <td>0.216812</td>
      <td>-0.349289</td>
      <td>0.422689</td>
      <td>0.377761</td>
      <td>0.045198</td>
      <td>-0.034096</td>
      <td>0.427570</td>
      <td>-0.328272</td>
      <td>...</td>
      <td>0.257849</td>
      <td>-0.032362</td>
      <td>-0.275003</td>
      <td>0.080452</td>
      <td>-0.078975</td>
      <td>-0.049972</td>
      <td>-0.009762</td>
      <td>-0.314754</td>
      <td>-0.020774</td>
      <td>0.268777</td>
    </tr>
    <tr>
      <th>1346</th>
      <td>0.068411</td>
      <td>0.017591</td>
      <td>0.236154</td>
      <td>0.221446</td>
      <td>-0.103567</td>
      <td>0.055510</td>
      <td>0.062909</td>
      <td>0.067425</td>
      <td>-0.003504</td>
      <td>-0.157758</td>
      <td>...</td>
      <td>0.007711</td>
      <td>0.323297</td>
      <td>0.334638</td>
      <td>0.367042</td>
      <td>-0.068821</td>
      <td>0.063667</td>
      <td>-0.329991</td>
      <td>0.232331</td>
      <td>-0.184768</td>
      <td>-0.000683</td>
    </tr>
    <tr>
      <th>11646</th>
      <td>-0.091488</td>
      <td>-0.155709</td>
      <td>0.032391</td>
      <td>0.018313</td>
      <td>0.524998</td>
      <td>0.563933</td>
      <td>-0.080984</td>
      <td>0.097983</td>
      <td>-0.535285</td>
      <td>-0.377195</td>
      <td>...</td>
      <td>0.428014</td>
      <td>-0.144572</td>
      <td>0.045296</td>
      <td>-0.107935</td>
      <td>-0.135673</td>
      <td>-0.290019</td>
      <td>-0.137200</td>
      <td>-0.503395</td>
      <td>-0.042567</td>
      <td>-0.282591</td>
    </tr>
    <tr>
      <th>3582</th>
      <td>0.185626</td>
      <td>0.092904</td>
      <td>0.097085</td>
      <td>-0.174650</td>
      <td>-0.193584</td>
      <td>0.047294</td>
      <td>0.098216</td>
      <td>0.332670</td>
      <td>0.163098</td>
      <td>-0.135102</td>
      <td>...</td>
      <td>0.078530</td>
      <td>-0.030177</td>
      <td>0.391598</td>
      <td>0.073519</td>
      <td>-0.454038</td>
      <td>-0.244358</td>
      <td>-0.790682</td>
      <td>-0.607009</td>
      <td>-0.255162</td>
      <td>0.029779</td>
    </tr>
  </tbody>
</table>
<p>11712 rows × 768 columns</p>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">emb_test</span> <span class="o">=</span> <span class="n">embedder</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">test_df</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">())</span>
<span class="n">emb_test_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">emb_test</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">test_df</span><span class="o">.</span><span class="n">index</span><span class="p">)</span>
<span class="n">emb_test_df</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
      <th>6</th>
      <th>7</th>
      <th>8</th>
      <th>9</th>
      <th>...</th>
      <th>758</th>
      <th>759</th>
      <th>760</th>
      <th>761</th>
      <th>762</th>
      <th>763</th>
      <th>764</th>
      <th>765</th>
      <th>766</th>
      <th>767</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1671</th>
      <td>-0.002864</td>
      <td>0.217326</td>
      <td>0.124350</td>
      <td>-0.082548</td>
      <td>0.709688</td>
      <td>-0.582442</td>
      <td>0.257897</td>
      <td>0.169356</td>
      <td>0.248880</td>
      <td>-0.266686</td>
      <td>...</td>
      <td>0.501767</td>
      <td>0.095387</td>
      <td>0.340173</td>
      <td>0.087452</td>
      <td>-0.368359</td>
      <td>0.276195</td>
      <td>0.238676</td>
      <td>-0.219546</td>
      <td>0.066603</td>
      <td>0.256149</td>
    </tr>
    <tr>
      <th>10951</th>
      <td>-0.141048</td>
      <td>0.137934</td>
      <td>0.131319</td>
      <td>0.194774</td>
      <td>0.868205</td>
      <td>0.078791</td>
      <td>-0.131656</td>
      <td>0.036243</td>
      <td>-0.215749</td>
      <td>-0.291946</td>
      <td>...</td>
      <td>-0.056256</td>
      <td>-0.056040</td>
      <td>0.147341</td>
      <td>0.189665</td>
      <td>-0.357366</td>
      <td>0.061799</td>
      <td>-0.161922</td>
      <td>-0.278956</td>
      <td>-0.173722</td>
      <td>0.065324</td>
    </tr>
    <tr>
      <th>5382</th>
      <td>-0.252943</td>
      <td>0.527507</td>
      <td>-0.065608</td>
      <td>0.013467</td>
      <td>0.207989</td>
      <td>0.003881</td>
      <td>-0.066281</td>
      <td>0.253166</td>
      <td>0.021039</td>
      <td>0.290956</td>
      <td>...</td>
      <td>0.180685</td>
      <td>-0.042605</td>
      <td>-0.173794</td>
      <td>-0.079129</td>
      <td>-0.169160</td>
      <td>0.001317</td>
      <td>-0.142593</td>
      <td>-0.070816</td>
      <td>-0.208826</td>
      <td>0.400736</td>
    </tr>
    <tr>
      <th>3954</th>
      <td>0.054318</td>
      <td>0.096738</td>
      <td>0.113037</td>
      <td>0.032039</td>
      <td>0.493064</td>
      <td>-0.641102</td>
      <td>0.078760</td>
      <td>0.402187</td>
      <td>0.189743</td>
      <td>-0.089538</td>
      <td>...</td>
      <td>0.123879</td>
      <td>-0.285019</td>
      <td>-0.297771</td>
      <td>0.557171</td>
      <td>0.076168</td>
      <td>-0.029826</td>
      <td>-0.076095</td>
      <td>0.225454</td>
      <td>0.002134</td>
      <td>0.235429</td>
    </tr>
    <tr>
      <th>11193</th>
      <td>-0.065858</td>
      <td>0.223270</td>
      <td>0.507333</td>
      <td>0.266193</td>
      <td>0.104696</td>
      <td>-0.219555</td>
      <td>0.146247</td>
      <td>0.315650</td>
      <td>-0.126193</td>
      <td>-0.435462</td>
      <td>...</td>
      <td>0.163994</td>
      <td>0.207813</td>
      <td>-0.001871</td>
      <td>0.109391</td>
      <td>-0.166779</td>
      <td>-0.249199</td>
      <td>-0.525419</td>
      <td>-0.413066</td>
      <td>0.119939</td>
      <td>0.064297</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>5861</th>
      <td>0.077512</td>
      <td>0.322276</td>
      <td>0.026697</td>
      <td>-0.111392</td>
      <td>0.174208</td>
      <td>0.235201</td>
      <td>0.053888</td>
      <td>0.244941</td>
      <td>0.181625</td>
      <td>-0.226870</td>
      <td>...</td>
      <td>0.149843</td>
      <td>0.311337</td>
      <td>0.045975</td>
      <td>-0.572319</td>
      <td>-0.068256</td>
      <td>0.217745</td>
      <td>-0.056509</td>
      <td>-0.355174</td>
      <td>-0.028610</td>
      <td>0.090676</td>
    </tr>
    <tr>
      <th>3627</th>
      <td>-0.173311</td>
      <td>-0.023604</td>
      <td>0.190388</td>
      <td>-0.136543</td>
      <td>-0.360269</td>
      <td>-0.444687</td>
      <td>0.056311</td>
      <td>0.291941</td>
      <td>-0.399719</td>
      <td>-0.167930</td>
      <td>...</td>
      <td>0.042209</td>
      <td>-0.161905</td>
      <td>-0.040535</td>
      <td>-0.050516</td>
      <td>-0.252020</td>
      <td>-0.133981</td>
      <td>0.155001</td>
      <td>-0.154482</td>
      <td>-0.060201</td>
      <td>-0.126555</td>
    </tr>
    <tr>
      <th>12559</th>
      <td>-0.124636</td>
      <td>-0.101799</td>
      <td>0.129061</td>
      <td>0.636908</td>
      <td>0.681090</td>
      <td>0.399300</td>
      <td>-0.078321</td>
      <td>0.221824</td>
      <td>-0.277218</td>
      <td>-0.178589</td>
      <td>...</td>
      <td>0.022364</td>
      <td>-0.109274</td>
      <td>-0.073540</td>
      <td>-0.153336</td>
      <td>-0.123705</td>
      <td>-0.238896</td>
      <td>0.296447</td>
      <td>-0.116798</td>
      <td>0.115076</td>
      <td>-0.345925</td>
    </tr>
    <tr>
      <th>8123</th>
      <td>0.063508</td>
      <td>0.332506</td>
      <td>0.119605</td>
      <td>-0.001362</td>
      <td>-0.161801</td>
      <td>-0.082302</td>
      <td>-0.025883</td>
      <td>0.048027</td>
      <td>0.126974</td>
      <td>-0.159802</td>
      <td>...</td>
      <td>0.002221</td>
      <td>-0.093885</td>
      <td>0.430285</td>
      <td>-0.088562</td>
      <td>0.321488</td>
      <td>0.447437</td>
      <td>0.292395</td>
      <td>-0.188566</td>
      <td>-0.272767</td>
      <td>0.126173</td>
    </tr>
    <tr>
      <th>210</th>
      <td>0.015537</td>
      <td>0.425568</td>
      <td>0.350672</td>
      <td>0.113120</td>
      <td>-0.128615</td>
      <td>0.098112</td>
      <td>0.222081</td>
      <td>0.101654</td>
      <td>0.224073</td>
      <td>-0.341075</td>
      <td>...</td>
      <td>0.100983</td>
      <td>-0.008055</td>
      <td>0.202025</td>
      <td>0.029846</td>
      <td>-0.019182</td>
      <td>0.107063</td>
      <td>0.002301</td>
      <td>0.038213</td>
      <td>-0.139270</td>
      <td>-0.007586</td>
    </tr>
  </tbody>
</table>
<p>2928 rows × 768 columns</p>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lgr</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">max_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
<span class="n">lgr</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">emb_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Train accuracy </span><span class="si">{:.2f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">lgr</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">emb_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Test accuracy </span><span class="si">{:.2f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">lgr</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">emb_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Train accuracy 0.87
Test accuracy 0.83
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Some improvement over bag of words and average embedding representations!</p></li>
<li><p>But much slower …</p></li>
</ul>
</div>
</div>
<div class="section" id="break-5-min">
<h2>Break (5 min)<a class="headerlink" href="#break-5-min" title="Permalink to this headline">¶</a></h2>
<p><img alt="" src="../_images/eva-coffee.png" /></p>
<p><br><br><br><br></p>
</div>
<div class="section" id="topic-modeling">
<h2>Topic modeling<a class="headerlink" href="#topic-modeling" title="Permalink to this headline">¶</a></h2>
<div class="section" id="topic-modeling-motivation">
<h3>Topic modeling motivation<a class="headerlink" href="#topic-modeling-motivation" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Suppose you have a large collection of documents on a variety of topics.</p></li>
</ul>
</div>
<div class="section" id="example-a-corpus-of-news-articles">
<h3>Example: A corpus of news articles<a class="headerlink" href="#example-a-corpus-of-news-articles" title="Permalink to this headline">¶</a></h3>
<p><img alt="" src="../_images/TM_NYT_articles.png" /></p>
<!-- <center> -->
<!-- <img src="img/TM_NYT_articles.png" height="2000" width="2000">  -->
<!-- </center> --></div>
<div class="section" id="example-a-corpus-of-food-magazines">
<h3>Example: A corpus of food magazines<a class="headerlink" href="#example-a-corpus-of-food-magazines" title="Permalink to this headline">¶</a></h3>
<p><img alt="" src="../_images/TM_food_magazines.png" /></p>
<!-- <center> -->
<!-- <img src="img/TM_food_magazines.png" height="2000" width="2000">  -->
<!-- </center> --></div>
<div class="section" id="a-corpus-of-scientific-articles">
<h3>A corpus of scientific articles<a class="headerlink" href="#a-corpus-of-scientific-articles" title="Permalink to this headline">¶</a></h3>
<p><img alt="" src="../_images/TM_science_articles.png" /></p>
<!-- <center> -->
<!-- <img src="img/TM_science_articles.png" height="2000" width="2000">  -->
<!-- </center> -->
<p>(Credit: <a class="reference external" href="http://www.cs.columbia.edu/~blei/talks/Blei_Science_2008.pdf">Dave Blei’s presentation</a>)</p>
</div>
<div class="section" id="id4">
<h3>Topic modeling motivation<a class="headerlink" href="#id4" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Humans are pretty good at reading and understanding a document and answering questions such as</p>
<ul>
<li><p>What is it about?</p></li>
<li><p>Which documents is it related to?</p></li>
</ul>
</li>
<li><p>But for a large collection of documents it would take years to read all documents and organize and categorize them so that they are easy to search.</p></li>
<li><p>You need an automated way</p>
<ul>
<li><p>to get an idea of what’s going on in the data or</p></li>
<li><p>to pull documents related to a certain topic</p></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="id5">
<h3>Topic modeling<a class="headerlink" href="#id5" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Topic modeling gives you an ability to summarize the major themes in a large collection of documents (corpus).</p>
<ul>
<li><p>Example: The major themes in a collection of news articles could be</p>
<ul>
<li><p><strong>politics</strong></p></li>
<li><p><strong>entertainment</strong></p></li>
<li><p><strong>sports</strong></p></li>
<li><p><strong>technology</strong></p></li>
<li><p>…</p></li>
</ul>
</li>
</ul>
</li>
<li><p>A common tool to solve such problems is unsupervised ML methods.</p></li>
<li><p>Given the hyperparameter <span class="math notranslate nohighlight">\(K\)</span>, the idea of topic modeling is to describe the data using <span class="math notranslate nohighlight">\(K\)</span> “topics”</p></li>
</ul>
</div>
<div class="section" id="topic-modeling-input-and-output">
<h3>Topic modeling: Input and output<a class="headerlink" href="#topic-modeling-input-and-output" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Input</p>
<ul>
<li><p>A large collection of documents</p></li>
<li><p>A value for the hyperparameter <span class="math notranslate nohighlight">\(K\)</span> (e.g., <span class="math notranslate nohighlight">\(K = 3\)</span>)</p></li>
</ul>
</li>
<li><p>Output</p>
<ol class="simple">
<li><p>Topic-words association</p>
<ul>
<li><p>For each topic, what words describe that topic?</p></li>
</ul>
</li>
<li><p>Document-topics association</p>
<ul>
<li><p>For each document, what topics are expressed by the document?</p></li>
</ul>
</li>
</ol>
</li>
</ul>
</div>
<div class="section" id="topic-modeling-example">
<h3>Topic modeling: Example<a class="headerlink" href="#topic-modeling-example" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Topic-words association</p>
<ul>
<li><p>For each topic, what words describe that topic?</p></li>
<li><p>A topic is a mixture of words.</p></li>
</ul>
</li>
</ul>
<p><img alt="" src="../_images/topic_modeling_word_topics.png" /></p>
<!-- <center> -->
<!-- <img src="img/topic_modeling_word_topics.png" height="1000" width="1000">  -->
<!-- </center> -->    </div>
<div class="section" id="id6">
<h3>Topic modeling: Example<a class="headerlink" href="#id6" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Document-topics association</p>
<ul>
<li><p>For each document, what topics are expressed by the document?</p></li>
<li><p>A document is a mixture of topics.</p></li>
</ul>
</li>
</ul>
<p><img alt="" src="../_images/topic_modeling_doc_topics.png" /></p>
<!-- <center> -->    
<!-- <img src="img/topic_modeling_doc_topics.png" height="800" width="800">  -->
<!-- </center> -->    </div>
<div class="section" id="id7">
<h3>Topic modeling: Input and output<a class="headerlink" href="#id7" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Input</p>
<ul>
<li><p>A large collection of documents</p></li>
<li><p>A value for the hyperparameter <span class="math notranslate nohighlight">\(K\)</span> (e.g., <span class="math notranslate nohighlight">\(K = 3\)</span>)</p></li>
</ul>
</li>
<li><p>Output</p>
<ul>
<li><p>For each topic, what words describe that topic?</p></li>
<li><p>For each document, what topics are expressed by the document?</p></li>
</ul>
</li>
</ul>
<p><img alt="" src="../_images/topic_modeling_output.png" /></p>
<!-- <center> -->
<!-- <img src="img/topic_modeling_output.png" height="800" width="800">  -->
<!-- </center> -->    </div>
<div class="section" id="topic-modeling-some-applications">
<h3>Topic modeling: Some applications<a class="headerlink" href="#topic-modeling-some-applications" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Topic modeling is a great EDA tool to get a sense of what’s going on in a large corpus.</p></li>
<li><p>Some examples</p>
<ul>
<li><p>If you want to pull documents related to a particular lawsuit.</p></li>
<li><p>You want to examine people’s sentiment towards a particular candidate and/or political party and so you want to pull tweets or Facebook posts related to election.</p></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="topic-modeling-toy-example">
<h3>Topic modeling toy example<a class="headerlink" href="#topic-modeling-toy-example" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">toy_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;data/toy_lda_data.csv&quot;</span><span class="p">)</span>
<span class="n">toy_df</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>doc_id</th>
      <th>text</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>famous fashion model</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>fashion model pattern</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>fashion model probabilistic topic model confer...</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>famous fashion model</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>fresh fashion model</td>
    </tr>
    <tr>
      <th>5</th>
      <td>6</td>
      <td>famous fashion model</td>
    </tr>
    <tr>
      <th>6</th>
      <td>7</td>
      <td>famous fashion model</td>
    </tr>
    <tr>
      <th>7</th>
      <td>8</td>
      <td>famous fashion model</td>
    </tr>
    <tr>
      <th>8</th>
      <td>9</td>
      <td>famous fashion model</td>
    </tr>
    <tr>
      <th>9</th>
      <td>10</td>
      <td>creative fashion model</td>
    </tr>
    <tr>
      <th>10</th>
      <td>11</td>
      <td>famous fashion model</td>
    </tr>
    <tr>
      <th>11</th>
      <td>12</td>
      <td>famous fashion model</td>
    </tr>
    <tr>
      <th>12</th>
      <td>13</td>
      <td>fashion model probabilistic topic model confer...</td>
    </tr>
    <tr>
      <th>13</th>
      <td>14</td>
      <td>probabilistic topic model</td>
    </tr>
    <tr>
      <th>14</th>
      <td>15</td>
      <td>probabilistic model pattern</td>
    </tr>
    <tr>
      <th>15</th>
      <td>16</td>
      <td>probabilistic topic model</td>
    </tr>
    <tr>
      <th>16</th>
      <td>17</td>
      <td>probabilistic topic model</td>
    </tr>
    <tr>
      <th>17</th>
      <td>18</td>
      <td>probabilistic topic model</td>
    </tr>
    <tr>
      <th>18</th>
      <td>19</td>
      <td>probabilistic topic model</td>
    </tr>
    <tr>
      <th>19</th>
      <td>20</td>
      <td>probabilistic topic model</td>
    </tr>
    <tr>
      <th>20</th>
      <td>21</td>
      <td>probabilistic topic model</td>
    </tr>
    <tr>
      <th>21</th>
      <td>22</td>
      <td>fashion model probabilistic topic model confer...</td>
    </tr>
    <tr>
      <th>22</th>
      <td>23</td>
      <td>apple kiwi nutrition</td>
    </tr>
    <tr>
      <th>23</th>
      <td>24</td>
      <td>kiwi health nutrition</td>
    </tr>
    <tr>
      <th>24</th>
      <td>25</td>
      <td>fresh apple health</td>
    </tr>
    <tr>
      <th>25</th>
      <td>26</td>
      <td>probabilistic topic model</td>
    </tr>
    <tr>
      <th>26</th>
      <td>27</td>
      <td>creative health nutrition</td>
    </tr>
    <tr>
      <th>27</th>
      <td>28</td>
      <td>probabilistic topic model</td>
    </tr>
    <tr>
      <th>28</th>
      <td>29</td>
      <td>probabilistic topic model</td>
    </tr>
    <tr>
      <th>29</th>
      <td>30</td>
      <td>hidden markov model probabilistic</td>
    </tr>
    <tr>
      <th>30</th>
      <td>31</td>
      <td>probabilistic topic model</td>
    </tr>
    <tr>
      <th>31</th>
      <td>32</td>
      <td>probabilistic topic model</td>
    </tr>
    <tr>
      <th>32</th>
      <td>33</td>
      <td>apple kiwi nutrition</td>
    </tr>
    <tr>
      <th>33</th>
      <td>34</td>
      <td>apple kiwi health</td>
    </tr>
    <tr>
      <th>34</th>
      <td>35</td>
      <td>apple kiwi nutrition</td>
    </tr>
    <tr>
      <th>35</th>
      <td>36</td>
      <td>fresh kiwi health</td>
    </tr>
    <tr>
      <th>36</th>
      <td>37</td>
      <td>apple kiwi nutrition</td>
    </tr>
    <tr>
      <th>37</th>
      <td>38</td>
      <td>apple kiwi nutrition</td>
    </tr>
    <tr>
      <th>38</th>
      <td>39</td>
      <td>apple kiwi nutrition</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">gensim</span> <span class="kn">import</span> <span class="n">corpora</span><span class="p">,</span> <span class="n">matutils</span><span class="p">,</span> <span class="n">models</span>

<span class="n">corpus</span> <span class="o">=</span> <span class="p">[</span><span class="n">doc</span><span class="o">.</span><span class="n">split</span><span class="p">()</span> <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">toy_df</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()]</span>
<span class="c1"># Create a vocabulary for the lda model</span>
<span class="n">dictionary</span> <span class="o">=</span> <span class="n">corpora</span><span class="o">.</span><span class="n">Dictionary</span><span class="p">(</span><span class="n">corpus</span><span class="p">)</span>
<span class="c1"># Convert our corpus into document-term matrix for Lda</span>
<span class="n">doc_term_matrix</span> <span class="o">=</span> <span class="p">[</span><span class="n">dictionary</span><span class="o">.</span><span class="n">doc2bow</span><span class="p">(</span><span class="n">doc</span><span class="p">)</span> <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">corpus</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">gensim.models</span> <span class="kn">import</span> <span class="n">LdaModel</span>

<span class="c1"># Train an lda model</span>
<span class="n">lda</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">LdaModel</span><span class="p">(</span>
    <span class="n">corpus</span><span class="o">=</span><span class="n">doc_term_matrix</span><span class="p">,</span>
    <span class="n">id2word</span><span class="o">=</span><span class="n">dictionary</span><span class="p">,</span>
    <span class="n">num_topics</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
    <span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">,</span>
    <span class="n">passes</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">### Examine the topics in our LDA model</span>
<span class="n">lda</span><span class="o">.</span><span class="n">print_topics</span><span class="p">(</span><span class="n">num_words</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[(0,
  &#39;0.303*&quot;model&quot; + 0.296*&quot;probabilistic&quot; + 0.261*&quot;topic&quot; + 0.040*&quot;pattern&quot;&#39;),
 (1, &#39;0.245*&quot;kiwi&quot; + 0.219*&quot;apple&quot; + 0.218*&quot;nutrition&quot; + 0.140*&quot;health&quot;&#39;),
 (2, &#39;0.308*&quot;fashion&quot; + 0.307*&quot;model&quot; + 0.180*&quot;famous&quot; + 0.071*&quot;conference&quot;&#39;)]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">### Examine the topic distribution for a document</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Document: &quot;</span><span class="p">,</span> <span class="n">corpus</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">lda</span><span class="p">[</span><span class="n">doc_term_matrix</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;topic id&quot;</span><span class="p">,</span> <span class="s2">&quot;probability&quot;</span><span class="p">])</span>
<span class="n">df</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="s2">&quot;probability&quot;</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Document:  [&#39;famous&#39;, &#39;fashion&#39;, &#39;model&#39;]
</pre></div>
</div>
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>topic id</th>
      <th>probability</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>2</th>
      <td>2</td>
      <td>0.828760</td>
    </tr>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>0.087849</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>0.083391</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>You can also visualize the topics using <code class="docutils literal notranslate"><span class="pre">pyLDAvis</span></code>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">pip</span> <span class="n">install</span> <span class="n">pyLDAvis</span>

</pre></div>
</div>
<blockquote>
<div><p>Do not install it using <code class="docutils literal notranslate"><span class="pre">conda</span></code>. They have made some changes in the recent version and <code class="docutils literal notranslate"><span class="pre">conda</span></code> build is not available for this version yet.</p>
</div></blockquote>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Visualize the topics</span>
<span class="kn">import</span> <span class="nn">pyLDAvis</span>

<span class="n">pyLDAvis</span><span class="o">.</span><span class="n">enable_notebook</span><span class="p">()</span>
<span class="kn">import</span> <span class="nn">pyLDAvis.gensim_models</span> <span class="k">as</span> <span class="nn">gensimvis</span>

<span class="n">vis</span> <span class="o">=</span> <span class="n">gensimvis</span><span class="o">.</span><span class="n">prepare</span><span class="p">(</span><span class="n">lda</span><span class="p">,</span> <span class="n">doc_term_matrix</span><span class="p">,</span> <span class="n">dictionary</span><span class="p">,</span> <span class="n">sort_topics</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">vis</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/Users/kvarada/opt/miniconda3/envs/cpsc330/lib/python3.9/site-packages/pyLDAvis/_prepare.py:246: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument &#39;labels&#39; will be keyword-only
  default_term_info = default_term_info.sort_values(
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/Users/kvarada/opt/miniconda3/envs/cpsc330/lib/python3.9/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module&#39;s documentation for alternative uses
  from imp import reload
/Users/kvarada/opt/miniconda3/envs/cpsc330/lib/python3.9/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module&#39;s documentation for alternative uses
  from imp import reload
/Users/kvarada/opt/miniconda3/envs/cpsc330/lib/python3.9/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module&#39;s documentation for alternative uses
  from imp import reload
/Users/kvarada/opt/miniconda3/envs/cpsc330/lib/python3.9/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module&#39;s documentation for alternative uses
  from imp import reload
/Users/kvarada/opt/miniconda3/envs/cpsc330/lib/python3.9/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module&#39;s documentation for alternative uses
  from imp import reload
</pre></div>
</div>
<div class="output text_html">
<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v1.0.0.css">


<div id="ldavis_el7348849061783683656566375"></div>
<script type="text/javascript">

var ldavis_el7348849061783683656566375_data = {"mdsDat": {"x": [-0.18796276816150773, 0.31219693667882936, -0.12423416851732189], "y": [0.14888014707464084, 0.021739796212790614, -0.1706199432874313], "topics": [1, 2, 3], "cluster": [1, 1, 1], "Freq": [40.125871892588755, 27.322216035712504, 32.55191207169874]}, "tinfo": {"Term": ["fashion", "probabilistic", "kiwi", "topic", "apple", "nutrition", "model", "famous", "health", "conference", "fresh", "creative", "pattern", "markov", "hidden", "probabilistic", "topic", "pattern", "markov", "hidden", "model", "creative", "conference", "fresh", "health", "famous", "nutrition", "apple", "kiwi", "fashion", "kiwi", "apple", "nutrition", "health", "fresh", "creative", "hidden", "markov", "pattern", "conference", "famous", "fashion", "topic", "probabilistic", "model", "fashion", "famous", "conference", "model", "creative", "fresh", "markov", "hidden", "pattern", "health", "topic", "nutrition", "apple", "probabilistic", "kiwi"], "Freq": [13.0, 15.0, 9.0, 14.0, 8.0, 8.0, 28.0, 8.0, 5.0, 3.0, 3.0, 2.0, 2.0, 1.0, 1.0, 15.092422404825797, 13.299502618730482, 2.0264060340990646, 1.1789434383263668, 1.178905185513376, 15.43498384384919, 0.2967537191968916, 0.33843495241705623, 0.29789734067112716, 0.2963123880455245, 0.2973333015197752, 0.29742748602024704, 0.29688620397042004, 0.2980852588014463, 0.3295632185972238, 8.491083928378536, 7.582725740102208, 7.581678179400556, 4.852260322422571, 2.2112535380183216, 1.2235201345432414, 0.304395493793989, 0.30434055631889, 0.3054068280782373, 0.30445747619236535, 0.30536223177492167, 0.30638717116329744, 0.30871571220344796, 0.30785196583024493, 0.30977493903951786, 12.75157176825618, 7.431464169270434, 2.929606637362001, 12.695807696117145, 1.1785066131082638, 1.1014765668986084, 0.2982582721503829, 0.2982430447122111, 0.33553011257220455, 0.2989865325625685, 0.5724222709111533, 0.2993747071052941, 0.29889414763742767, 0.5511730065956952, 0.2996084704197316], "Total": [13.0, 15.0, 9.0, 14.0, 8.0, 8.0, 28.0, 8.0, 5.0, 3.0, 3.0, 2.0, 2.0, 1.0, 1.0, 15.951447377251737, 14.180640601845083, 2.6673429747495065, 1.7815422667956398, 1.7815437240195762, 28.440566479005852, 2.6987804668483966, 3.5724990659714226, 3.610627445588057, 5.447559243030663, 8.03415970256513, 8.178480372526097, 8.178506091710055, 9.088777657599714, 13.387522158016703, 9.088777657599714, 8.178506091710055, 8.178480372526097, 5.447559243030663, 3.610627445588057, 2.6987804668483966, 1.7815437240195762, 1.7815422667956398, 2.6673429747495065, 3.5724990659714226, 8.03415970256513, 13.387522158016703, 14.180640601845083, 15.951447377251737, 28.440566479005852, 13.387522158016703, 8.03415970256513, 3.5724990659714226, 28.440566479005852, 2.6987804668483966, 3.610627445588057, 1.7815422667956398, 1.7815437240195762, 2.6673429747495065, 5.447559243030663, 14.180640601845083, 8.178480372526097, 8.178506091710055, 15.951447377251737, 9.088777657599714], "Category": ["Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3"], "logprob": [15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -1.2168, -1.3433, -3.2248, -3.7664, -3.7665, -1.1944, -5.1459, -5.0145, -5.142, -5.1474, -5.1439, -5.1436, -5.1454, -5.1414, -5.041, -1.4077, -1.5208, -1.521, -1.9673, -2.7532, -3.345, -4.7361, -4.7363, -4.7328, -4.7359, -4.733, -4.7296, -4.7221, -4.7249, -4.7186, -1.1762, -1.7161, -2.647, -1.1806, -3.5576, -3.6252, -4.9316, -4.9317, -4.8139, -4.9292, -4.2797, -4.9279, -4.9295, -4.3176, -4.9271], "loglift": [15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.8578, 0.849, 0.6383, 0.5003, 0.5003, 0.302, -1.2945, -1.4435, -1.5817, -1.9984, -2.3835, -2.4009, -2.4028, -2.5043, -2.7912, 1.2294, 1.2218, 1.2217, 1.1817, 0.8071, 0.5064, -0.4694, -0.4696, -0.8697, -1.165, -1.9725, -2.4798, -2.5297, -2.6502, -3.2223, 1.0737, 1.0444, 0.9239, 0.3158, 0.2938, -0.0649, -0.6649, -0.665, -0.9508, -1.7802, -2.0874, -2.1852, -2.1868, -2.2429, -2.29]}, "token.table": {"Topic": [2, 3, 2, 3, 3, 3, 2, 3, 2, 1, 2, 1, 1, 3, 2, 1, 1, 3, 1, 3], "Freq": [0.9781737532859462, 0.8397482951291548, 0.3705377344633697, 0.3705377344633697, 0.8712796682103611, 0.9710534814850225, 0.5539203449095433, 0.27696017245477167, 0.917842244009875, 0.5613109498900021, 0.8802063711296407, 0.5613114090178977, 0.5274156550669497, 0.4570935677246897, 0.9781768293868301, 0.749809836580098, 0.9403535394155774, 0.0626902359610385, 0.9167427879321992, 0.07051867599478455], "Term": ["apple", "conference", "creative", "creative", "famous", "fashion", "fresh", "fresh", "health", "hidden", "kiwi", "markov", "model", "model", "nutrition", "pattern", "probabilistic", "probabilistic", "topic", "topic"]}, "R": 15, "lambda.step": 0.01, "plot.opts": {"xlab": "PC1", "ylab": "PC2"}, "topic.order": [1, 2, 3]};

function LDAvis_load_lib(url, callback){
  var s = document.createElement('script');
  s.src = url;
  s.async = true;
  s.onreadystatechange = s.onload = callback;
  s.onerror = function(){console.warn("failed to load library " + url);};
  document.getElementsByTagName("head")[0].appendChild(s);
}

if(typeof(LDAvis) !== "undefined"){
   // already loaded: just create the visualization
   !function(LDAvis){
       new LDAvis("#" + "ldavis_el7348849061783683656566375", ldavis_el7348849061783683656566375_data);
   }(LDAvis);
}else if(typeof define === "function" && define.amd){
   // require.js is available: use it to load d3/LDAvis
   require.config({paths: {d3: "https://d3js.org/d3.v5"}});
   require(["d3"], function(d3){
      window.d3 = d3;
      LDAvis_load_lib("https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js", function(){
        new LDAvis("#" + "ldavis_el7348849061783683656566375", ldavis_el7348849061783683656566375_data);
      });
    });
}else{
    // require.js not available: dynamically load d3 & LDAvis
    LDAvis_load_lib("https://d3js.org/d3.v5.js", function(){
         LDAvis_load_lib("https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js", function(){
                 new LDAvis("#" + "ldavis_el7348849061783683656566375", ldavis_el7348849061783683656566375_data);
            })
         });
}
</script></div></div>
</div>
</div>
</div>
<div class="section" id="topic-modeling-pipeline">
<h2>Topic modeling pipeline<a class="headerlink" href="#topic-modeling-pipeline" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Preprocess your corpus.</p></li>
<li><p>Train LDA using <code class="docutils literal notranslate"><span class="pre">Gensim</span></code>.</p></li>
<li><p>Interpret your topics.</p></li>
</ul>
<div class="section" id="data">
<h3>Data<a class="headerlink" href="#data" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">wikipedia</span>

<span class="n">queries</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;Artificial Intelligence&quot;</span><span class="p">,</span>
    <span class="s2">&quot;unsupervised learning&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Supreme Court of Canada&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Peace, Order, and Good Government&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Canadian constitutional law&quot;</span><span class="p">,</span>
    <span class="s2">&quot;ice hockey&quot;</span><span class="p">,</span>
<span class="p">]</span>
<span class="n">wiki_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;wiki query&quot;</span><span class="p">:</span> <span class="p">[],</span> <span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="p">[]}</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">queries</span><span class="p">)):</span>
    <span class="n">wiki_dict</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">wikipedia</span><span class="o">.</span><span class="n">page</span><span class="p">(</span><span class="n">queries</span><span class="p">[</span><span class="n">i</span><span class="p">])</span><span class="o">.</span><span class="n">content</span><span class="p">)</span>
    <span class="n">wiki_dict</span><span class="p">[</span><span class="s2">&quot;wiki query&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">queries</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>

<span class="n">wiki_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">wiki_dict</span><span class="p">)</span>
<span class="n">wiki_df</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>wiki query</th>
      <th>text</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Artificial Intelligence</td>
      <td>Artificial intelligence (AI) is intelligence d...</td>
    </tr>
    <tr>
      <th>1</th>
      <td>unsupervised learning</td>
      <td>Unsupervised learning is a type of machine lea...</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Supreme Court of Canada</td>
      <td>The Supreme Court of Canada (SCC; French: Cour...</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Peace, Order, and Good Government</td>
      <td>In many Commonwealth jurisdictions, the phrase...</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Canadian constitutional law</td>
      <td>Canadian constitutional law (French: droit con...</td>
    </tr>
    <tr>
      <th>5</th>
      <td>ice hockey</td>
      <td>Ice hockey is a contact winter team sport play...</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
</div>
<div class="section" id="preprocessing-the-corpus">
<h3>Preprocessing the corpus<a class="headerlink" href="#preprocessing-the-corpus" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p><strong>Preprocessing is crucial!</strong></p></li>
<li><p>Tokenization, converting text to lower case</p></li>
<li><p>Removing punctuation and stopwords</p></li>
<li><p>Discarding words with length &lt; threshold or word frequency &lt; threshold</p></li>
<li><p>Possibly lemmatization: Consider the lemmas instead of inflected forms.</p></li>
<li><p>Depending upon your application, restrict to specific part of speech;</p>
<ul>
<li><p>For example, only consider nouns, verbs, and adjectives</p></li>
</ul>
</li>
</ul>
<p>We’ll use <a class="reference external" href="https://spacy.io/"><code class="docutils literal notranslate"><span class="pre">spaCy</span></code></a> for preprocessing.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">spacy</span>

<span class="n">nlp</span> <span class="o">=</span> <span class="n">spacy</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;en_core_web_md&quot;</span><span class="p">,</span> <span class="n">disable</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;parser&quot;</span><span class="p">,</span> <span class="s2">&quot;ner&quot;</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">preprocess</span><span class="p">(</span>
    <span class="n">doc</span><span class="p">,</span>
    <span class="n">min_token_len</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">irrelevant_pos</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;ADV&quot;</span><span class="p">,</span> <span class="s2">&quot;PRON&quot;</span><span class="p">,</span> <span class="s2">&quot;CCONJ&quot;</span><span class="p">,</span> <span class="s2">&quot;PUNCT&quot;</span><span class="p">,</span> <span class="s2">&quot;PART&quot;</span><span class="p">,</span> <span class="s2">&quot;DET&quot;</span><span class="p">,</span> <span class="s2">&quot;ADP&quot;</span><span class="p">,</span> <span class="s2">&quot;SPACE&quot;</span><span class="p">],</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Given text, min_token_len, and irrelevant_pos carry out preprocessing of the text</span>
<span class="sd">    and return a preprocessed string.</span>

<span class="sd">    Parameters</span>
<span class="sd">    -------------</span>
<span class="sd">    doc : (spaCy doc object)</span>
<span class="sd">        the spacy doc object of the text</span>
<span class="sd">    min_token_len : (int)</span>
<span class="sd">        min_token_length required</span>
<span class="sd">    irrelevant_pos : (list)</span>
<span class="sd">        a list of irrelevant pos tags</span>

<span class="sd">    Returns</span>
<span class="sd">    -------------</span>
<span class="sd">    (str) the preprocessed text</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">clean_text</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">doc</span><span class="p">:</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="n">token</span><span class="o">.</span><span class="n">is_stop</span> <span class="o">==</span> <span class="kc">False</span>  <span class="c1"># Check if it&#39;s not a stopword</span>
            <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">token</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">min_token_len</span>  <span class="c1"># Check if the word meets minimum threshold</span>
            <span class="ow">and</span> <span class="n">token</span><span class="o">.</span><span class="n">pos_</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">irrelevant_pos</span>
        <span class="p">):</span>  <span class="c1"># Check if the POS is in the acceptable POS tags</span>
            <span class="n">lemma</span> <span class="o">=</span> <span class="n">token</span><span class="o">.</span><span class="n">lemma_</span>  <span class="c1"># Take the lemma of the word</span>
            <span class="n">clean_text</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">lemma</span><span class="o">.</span><span class="n">lower</span><span class="p">())</span>
    <span class="k">return</span> <span class="s2">&quot; &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">clean_text</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">wiki_df</span><span class="p">[</span><span class="s2">&quot;text_pp&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">preprocess</span><span class="p">(</span><span class="n">text</span><span class="p">)</span> <span class="k">for</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">nlp</span><span class="o">.</span><span class="n">pipe</span><span class="p">(</span><span class="n">wiki_df</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">])]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">wiki_df</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>wiki query</th>
      <th>text</th>
      <th>text_pp</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Artificial Intelligence</td>
      <td>Artificial intelligence (AI) is intelligence d...</td>
      <td>artificial intelligence intelligence demonstra...</td>
    </tr>
    <tr>
      <th>1</th>
      <td>unsupervised learning</td>
      <td>Unsupervised learning is a type of machine lea...</td>
      <td>unsupervised learning type machine learning al...</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Supreme Court of Canada</td>
      <td>The Supreme Court of Canada (SCC; French: Cour...</td>
      <td>supreme court canada scc french cour suprême c...</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Peace, Order, and Good Government</td>
      <td>In many Commonwealth jurisdictions, the phrase...</td>
      <td>commonwealth jurisdiction phrase peace order g...</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Canadian constitutional law</td>
      <td>Canadian constitutional law (French: droit con...</td>
      <td>canadian constitutional law french droit const...</td>
    </tr>
    <tr>
      <th>5</th>
      <td>ice hockey</td>
      <td>Ice hockey is a contact winter team sport play...</td>
      <td>ice hockey contact winter team sport play ice ...</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
</div>
<div class="section" id="training-lda-with-gensim">
<h3>Training LDA with <a class="reference external" href="https://radimrehurek.com/gensim/models/ldamodel.html">gensim</a><a class="headerlink" href="#training-lda-with-gensim" title="Permalink to this headline">¶</a></h3>
<p>To train an LDA model with <a class="reference external" href="https://radimrehurek.com/gensim/models/ldamodel.html">gensim</a>, you need</p>
<ul class="simple">
<li><p>Document-term matrix</p></li>
<li><p>Dictionary (vocabulary)</p></li>
<li><p>The number of topics (<span class="math notranslate nohighlight">\(K\)</span>): <code class="docutils literal notranslate"><span class="pre">num_topics</span></code></p></li>
<li><p>The number of passes: <code class="docutils literal notranslate"><span class="pre">passes</span></code></p></li>
</ul>
</div>
<div class="section" id="gensim-s-doc2bow">
<h3><code class="docutils literal notranslate"><span class="pre">Gensim</span></code>’s <code class="docutils literal notranslate"><span class="pre">doc2bow</span></code><a class="headerlink" href="#gensim-s-doc2bow" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Let’s first create a dictionary using <a class="reference external" href="https://radimrehurek.com/gensim/corpora/dictionary.html"><code class="docutils literal notranslate"><span class="pre">corpora.Dictionary</span></code></a>.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">corpus</span> <span class="o">=</span> <span class="p">[</span><span class="n">doc</span><span class="o">.</span><span class="n">split</span><span class="p">()</span> <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">wiki_df</span><span class="p">[</span><span class="s2">&quot;text_pp&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()]</span>
<span class="n">dictionary</span> <span class="o">=</span> <span class="n">corpora</span><span class="o">.</span><span class="n">Dictionary</span><span class="p">(</span><span class="n">corpus</span><span class="p">)</span>  <span class="c1"># Create a vocabulary for the lda model</span>
<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="n">dictionary</span><span class="o">.</span><span class="n">token2id</span><span class="o">.</span><span class="n">keys</span><span class="p">(),</span> <span class="n">index</span><span class="o">=</span><span class="n">dictionary</span><span class="o">.</span><span class="n">token2id</span><span class="o">.</span><span class="n">values</span><span class="p">(),</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;Word&quot;</span><span class="p">]</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Word</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0026</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0030</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0036</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0040</td>
    </tr>
    <tr>
      <th>4</th>
      <td>007</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
    </tr>
    <tr>
      <th>4683</th>
      <td>wrist</td>
    </tr>
    <tr>
      <th>4684</th>
      <td>yale</td>
    </tr>
    <tr>
      <th>4685</th>
      <td>youth</td>
    </tr>
    <tr>
      <th>4686</th>
      <td>zhenskaya</td>
    </tr>
    <tr>
      <th>4687</th>
      <td>zhhl</td>
    </tr>
  </tbody>
</table>
<p>4688 rows × 1 columns</p>
</div></div></div>
</div>
</div>
<div class="section" id="id8">
<h3><code class="docutils literal notranslate"><span class="pre">Gensim</span></code>’s <code class="docutils literal notranslate"><span class="pre">doc2bow</span></code><a class="headerlink" href="#id8" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Now let’s convert our corpus into document-term matrix for LDA using <a class="reference external" href="https://radimrehurek.com/gensim/corpora/dictionary.html#gensim.corpora.dictionary.Dictionary.doc2bow"><code class="docutils literal notranslate"><span class="pre">dictionary.doc2bow</span></code></a>.</p></li>
<li><p>For each document, it stores the frequency of each token in the document in the format (token_id, frequency).</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">doc_term_matrix</span> <span class="o">=</span> <span class="p">[</span><span class="n">dictionary</span><span class="o">.</span><span class="n">doc2bow</span><span class="p">(</span><span class="n">doc</span><span class="p">)</span> <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">corpus</span><span class="p">]</span>
<span class="n">doc_term_matrix</span><span class="p">[</span><span class="mi">1</span><span class="p">][:</span><span class="mi">20</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[(105, 1),
 (113, 1),
 (121, 1),
 (161, 1),
 (258, 2),
 (277, 1),
 (282, 1),
 (292, 2),
 (295, 4),
 (304, 1),
 (312, 2),
 (315, 1),
 (343, 13),
 (352, 1),
 (365, 6),
 (367, 1),
 (368, 1),
 (391, 3),
 (410, 4),
 (413, 1)]
</pre></div>
</div>
</div>
</div>
<p>Now we are ready to train an LDA model.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">gensim.models</span> <span class="kn">import</span> <span class="n">LdaModel</span>

<span class="n">num_topics</span> <span class="o">=</span> <span class="mi">3</span>

<span class="n">lda</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">LdaModel</span><span class="p">(</span>
    <span class="n">corpus</span><span class="o">=</span><span class="n">doc_term_matrix</span><span class="p">,</span>
    <span class="n">id2word</span><span class="o">=</span><span class="n">dictionary</span><span class="p">,</span>
    <span class="n">num_topics</span><span class="o">=</span><span class="n">num_topics</span><span class="p">,</span>
    <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span>
    <span class="n">passes</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="examine-the-topics-and-topic-distribution-for-a-document-in-our-lda-model">
<h3>Examine the topics and topic distribution for a document in our LDA model<a class="headerlink" href="#examine-the-topics-and-topic-distribution-for-a-document-in-our-lda-model" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lda</span><span class="o">.</span><span class="n">print_topics</span><span class="p">(</span><span class="n">num_words</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>  <span class="c1"># Topics</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[(0, &#39;0.019*&quot;hockey&quot; + 0.011*&quot;ice&quot; + 0.010*&quot;player&quot; + 0.010*&quot;team&quot;&#39;),
 (1, &#39;0.037*&quot;court&quot; + 0.014*&quot;justice&quot; + 0.012*&quot;supreme&quot; + 0.011*&quot;law&quot;&#39;),
 (2, &#39;0.011*&quot;power&quot; + 0.010*&quot;law&quot; + 0.010*&quot;government&quot; + 0.009*&quot;provincial&quot;&#39;)]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Document: &quot;</span><span class="p">,</span> <span class="n">wiki_df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Topic assignment for document: &quot;</span><span class="p">,</span> <span class="n">lda</span><span class="p">[</span><span class="n">doc_term_matrix</span><span class="p">[</span><span class="mi">0</span><span class="p">]])</span>  <span class="c1"># Topic distribution</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Document:  Artificial Intelligence
Topic assignment for document:  [(0, 0.9998983)]
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="visualize-topics">
<h3>Visualize topics<a class="headerlink" href="#visualize-topics" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">vis</span> <span class="o">=</span> <span class="n">gensimvis</span><span class="o">.</span><span class="n">prepare</span><span class="p">(</span><span class="n">lda</span><span class="p">,</span> <span class="n">doc_term_matrix</span><span class="p">,</span> <span class="n">dictionary</span><span class="p">,</span> <span class="n">sort_topics</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">vis</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/Users/kvarada/opt/miniconda3/envs/cpsc330/lib/python3.9/site-packages/pyLDAvis/_prepare.py:246: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument &#39;labels&#39; will be keyword-only
  default_term_info = default_term_info.sort_values(
/Users/kvarada/opt/miniconda3/envs/cpsc330/lib/python3.9/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module&#39;s documentation for alternative uses
  from imp import reload
/Users/kvarada/opt/miniconda3/envs/cpsc330/lib/python3.9/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module&#39;s documentation for alternative uses
  from imp import reload
/Users/kvarada/opt/miniconda3/envs/cpsc330/lib/python3.9/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module&#39;s documentation for alternative uses
  from imp import reload
/Users/kvarada/opt/miniconda3/envs/cpsc330/lib/python3.9/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module&#39;s documentation for alternative uses
  from imp import reload
</pre></div>
</div>
<div class="output text_html">
<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v1.0.0.css">


<div id="ldavis_el7348856078391044998229239"></div>
<script type="text/javascript">

var ldavis_el7348856078391044998229239_data = {"mdsDat": {"x": [0.15260526131253155, -0.058827069776560006, -0.09377819153597156], "y": [0.013189887080643238, -0.09298041831048383, 0.07979053122984058], "topics": [1, 2, 3], "cluster": [1, 1, 1], "Freq": [72.3415705327995, 9.429272312965818, 18.229157154234684]}, "tinfo": {"Term": ["court", "law", "canada", "hockey", "justice", "case", "power", "provincial", "supreme", "government", "federal", "network", "year", "matter", "learning", "act", "ice", "appeal", "player", "decision", "team", "canadian", "league", "good", "doctrine", "parliament", "game", "judge", "constitution", "section", "hockey", "player", "ice", "league", "team", "game", "intelligence", "puck", "penalty", "retrieve", "archive", "goal", "sport", "nhl", "2019", "2015", "professional", "2020", "artificial", "2018", "minute", "original", "tournament", "university", "line", "knowledge", "play", "search", "logic", "agent", "woman", "stick", "world", "problem", "human", "system", "october", "machine", "include", "canada", "use", "national", "appeal", "appointment", "appoint", "clerk", "puisne", "superior", "tenure", "counsel", "minister", "prime", "lamer", "hear", "litigant", "override", "hearing", "judge", "building", "statue", "wagner", "scc", "appellate", "1949", "laskin", "website", "lawyer", "recommend", "dissent", "panel", "corporation", "justice", "supreme", "court", "territorial", "chief", "day", "french", "case", "law", "federal", "judicial", "year", "canada", "provincial", "decision", "charter", "parliament", "member", "canadian", "government", "reference", "power", "time", "act", "doctrine", "peace", "pogg", "substance", "pith", "variable", "purpose", "residual", "phrase", "immunity", "emergency", "boltzmann", "latent", "paramountcy", "interjurisdictional", "commonwealth", "authority", "unsupervised", "helmholtz", "federalism", "jurisprudence", "legislate", "enumerate", "branch", "cluster", "hopfield", "conflict", "competence", "invalid", "limitation", "matter", "section", "legislative", "legislation", "power", "constitution", "provincial", "government", "constitutional", "moment", "aspect", "jurisdiction", "act", "order", "federal", "law", "province", "good", "parliament", "network", "method", "learning", "training", "concern", "court", "canada", "right", "state", "canadian", "neural"], "Freq": [95.0, 63.0, 90.0, 259.0, 32.0, 43.0, 50.0, 43.0, 29.0, 47.0, 40.0, 57.0, 30.0, 30.0, 56.0, 37.0, 157.0, 14.0, 139.0, 25.0, 135.0, 34.0, 124.0, 27.0, 16.0, 18.0, 118.0, 11.0, 19.0, 18.0, 258.28443202311325, 138.84887430571155, 156.7527974257029, 123.48401153366082, 135.09872815911805, 118.01577722775616, 108.0756723229997, 70.42239692077987, 67.72461286083131, 69.41427037486703, 64.95001026138907, 55.97528861384745, 51.49355293143989, 48.824066511694184, 39.852452641214846, 37.178379168477406, 36.26625633277471, 38.02098901160111, 88.3311147114373, 32.62330903067314, 29.94229658597721, 78.44401719415241, 25.483760556660418, 25.46203369144678, 25.458312793440054, 24.574899589262657, 109.90794371512754, 25.43746505931606, 24.550545062858564, 24.54431977665551, 47.07118617180574, 38.039194486416925, 42.54329772920492, 44.31273882264553, 53.306172013084094, 39.81894509499735, 37.13976876910858, 57.81840802945784, 43.46749483027905, 52.67099169075031, 38.959339798815904, 38.09000566817747, 13.390850073086451, 7.0511722314713, 5.991888045690977, 4.939795075430042, 4.405695841198676, 3.87847915033234, 3.877440235971716, 3.875496869914968, 3.351935829884927, 3.3516605619774116, 3.3507546303463016, 7.056569765786875, 2.824081287039348, 2.822739620264313, 2.821772799879467, 9.699801728942612, 6.523504371853321, 2.2937482759297074, 2.2926634835844975, 2.291458816505369, 2.291589050784194, 2.291399196267336, 2.2909146232688515, 1.7646488963410767, 1.7639451872549274, 1.7637834515028157, 1.7632890475076346, 1.763227524496047, 1.7627425286589953, 24.518896465136788, 21.87130968757321, 67.94487402460365, 3.356685998353487, 7.058781211495488, 7.578993158650754, 3.890170636726828, 18.744147863394772, 19.322993566205174, 13.997033134970142, 5.474333216094589, 10.25490081779589, 18.83025640065767, 10.844582181739932, 8.14238157478351, 5.476632612225266, 6.063811617526177, 4.946576137538845, 6.597164964524275, 7.203960719492437, 4.438221006658004, 4.581314229162351, 4.4361275329097545, 4.005575544146767, 15.982350205596749, 14.565930717624692, 9.098571864991245, 7.784943872097182, 7.782028833277275, 7.782084420108389, 7.091937943874324, 7.063192195107622, 7.059701669095001, 6.40730028609437, 6.375081177836421, 5.723973414839296, 5.724562390013088, 5.722155398480497, 5.721185898749591, 5.6931971118301234, 5.017284542901623, 13.276945674889053, 4.3514229722373345, 4.349153721602431, 4.33724097322259, 4.329891249714516, 4.324623171286334, 12.502773163798475, 3.6639924849205845, 3.663696158063541, 3.661506200408316, 3.661125267124503, 3.650662681074026, 3.645340241994828, 26.915835191138342, 15.963977123006662, 8.458613757626239, 11.905535988505035, 37.87298218275987, 15.305634488703987, 32.4665056243578, 34.43606149421466, 13.266413605299684, 7.0975800072324295, 7.0960783453389435, 11.194886963577165, 24.19166193039557, 15.277319537878421, 25.63245133909087, 35.95662944990662, 10.5155112533249, 17.265861580374423, 12.519049478420794, 27.076276769709096, 13.986547542013136, 22.34428428042123, 9.84779569835227, 10.479310329561688, 23.636377280978877, 19.468702301163784, 12.636123519281579, 11.918007875009614, 11.899208081745385, 9.915238056108539], "Total": [95.0, 63.0, 90.0, 259.0, 32.0, 43.0, 50.0, 43.0, 29.0, 47.0, 40.0, 57.0, 30.0, 30.0, 56.0, 37.0, 157.0, 14.0, 139.0, 25.0, 135.0, 34.0, 124.0, 27.0, 16.0, 18.0, 118.0, 11.0, 19.0, 18.0, 259.5041344231166, 139.6148890615649, 157.63439321219406, 124.26094460330101, 135.94996516738468, 118.82974628779006, 108.8745652711093, 71.04758417069948, 68.34565749172044, 70.1012037833671, 65.60563094972439, 56.5994839897733, 52.08976647291506, 49.39597430248892, 40.378729394953545, 37.684907272905896, 36.77919255802795, 38.56746291682266, 89.73343927327541, 33.156947562523264, 30.457696244325792, 79.82393307143205, 25.960493740205848, 25.953177340642377, 25.95262673515277, 25.05496729165476, 112.08304768619807, 25.942441210443867, 25.04775055638393, 25.04516878394897, 48.13332198686975, 39.2651891697231, 44.150329251679445, 46.251754608780615, 57.18712430136374, 41.96607692399352, 39.268909537848735, 65.99089997233493, 54.54435310872088, 90.96995039257177, 45.74522401333829, 49.127251121399595, 14.040644648853146, 7.637616533190348, 6.574391279541901, 5.50383491741461, 4.974257961451311, 4.4398108021836, 4.440328020900442, 4.441778000911138, 3.9049318476074073, 3.905243251761704, 3.9061121052568826, 8.324567516249678, 3.371107036043117, 3.3721465651492966, 3.372694830969668, 11.890369878452105, 8.003819968277782, 2.8394530140335186, 2.839823668838771, 2.8402854077949002, 2.8404567671036425, 2.840565750612257, 2.840912367127982, 2.306086895907014, 2.3067484154337197, 2.306950049633244, 2.3070711000138533, 2.3072012134985003, 2.3072053002618578, 32.1221663013713, 29.668275394749465, 95.08297925533795, 4.592915097879342, 11.295389193927678, 12.683454792995013, 5.813665579742924, 43.43100870759807, 63.83709882982558, 40.96174154338469, 10.598343394987515, 30.66341755831693, 90.96995039257177, 43.75557180145938, 25.402000797912812, 13.623504770664574, 18.96885836624335, 11.600597721797405, 34.13889363945781, 47.49391202668328, 18.64876787601272, 50.06687580463475, 31.3901829071091, 37.55530326051573, 16.56414911559203, 15.17142891481635, 9.661970721635488, 8.289434874820302, 8.288820819946865, 8.289082466464205, 7.601372271051188, 7.594287570285901, 7.597885372457066, 6.911835551997204, 6.907164706100892, 6.220886725486666, 6.221933331311336, 6.222369043628188, 6.222656074279122, 6.218649918375647, 5.529552040909347, 14.704468982783528, 4.843965172373244, 4.8440938557198345, 4.841639418112528, 4.840520543305193, 4.840625387321919, 14.164688352910328, 4.153796381998607, 4.154458970988824, 4.154999650322861, 4.154928596288244, 4.152134199060869, 4.151858039306166, 30.81845520612945, 18.5172107472706, 10.044225338835588, 14.556044957032952, 50.06687580463475, 19.067704734697983, 43.75557180145938, 47.49391202668328, 17.000181287079194, 8.5015808514805, 8.499555747550048, 14.601048425160066, 37.55530326051573, 21.598276121591717, 40.96174154338469, 63.83709882982558, 13.708216355445526, 27.096771693489163, 18.96885836624335, 57.34107869437999, 23.504002578139954, 56.106379273261794, 13.961363024846591, 15.704427081485562, 95.08297925533795, 90.96995039257177, 30.938503436457562, 30.445706546927184, 34.13889363945781, 37.39749653425975], "Category": ["Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3"], "logprob": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -3.988, -4.6086, -4.4873, -4.7259, -4.636, -4.7712, -4.8592, -5.2875, -5.3266, -5.3019, -5.3684, -5.5171, -5.6006, -5.6538, -5.8568, -5.9263, -5.9511, -5.9039, -5.0609, -6.057, -6.1427, -5.1796, -6.304, -6.3048, -6.305, -6.3403, -4.8424, -6.3058, -6.3413, -6.3415, -5.6904, -5.9034, -5.7915, -5.7507, -5.566, -5.8577, -5.9273, -5.4847, -5.77, -5.5779, -5.8795, -5.9021, -4.9099, -5.5512, -5.714, -5.9071, -6.0215, -6.149, -6.1493, -6.1498, -6.2949, -6.295, -6.2952, -5.5505, -6.4663, -6.4667, -6.4671, -5.2323, -5.629, -6.6742, -6.6747, -6.6752, -6.6752, -6.6753, -6.6755, -6.9365, -6.9369, -6.937, -6.9373, -6.9373, -6.9376, -4.305, -4.4193, -3.2857, -6.2935, -5.5502, -5.4791, -6.146, -4.5736, -4.5431, -4.8656, -5.8044, -5.1767, -4.569, -5.1208, -5.4074, -5.8039, -5.7021, -5.9057, -5.6178, -5.5298, -6.0142, -5.9824, -6.0147, -6.1167, -5.3922, -5.485, -5.9555, -6.1114, -6.1118, -6.1118, -6.2047, -6.2087, -6.2092, -6.3062, -6.3112, -6.419, -6.4189, -6.4193, -6.4195, -6.4244, -6.5507, -5.5776, -6.6931, -6.6937, -6.6964, -6.6981, -6.6993, -5.6377, -6.8651, -6.8652, -6.8658, -6.8659, -6.8687, -6.8702, -4.8709, -5.3933, -6.0285, -5.6866, -4.5294, -5.4354, -4.6834, -4.6245, -5.5784, -6.2039, -6.2041, -5.7482, -4.9776, -5.4373, -4.9198, -4.5813, -5.8108, -5.3149, -5.6364, -4.865, -5.5255, -5.0571, -5.8764, -5.8142, -5.0009, -5.1948, -5.6271, -5.6856, -5.6872, -5.8696], "loglift": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.3191, 0.3183, 0.3182, 0.3175, 0.3175, 0.3169, 0.3164, 0.3149, 0.3146, 0.3139, 0.3137, 0.3127, 0.3123, 0.3121, 0.3107, 0.3102, 0.3097, 0.3095, 0.308, 0.3075, 0.3067, 0.3063, 0.3052, 0.3047, 0.3045, 0.3044, 0.3042, 0.3041, 0.3037, 0.3036, 0.3015, 0.292, 0.2867, 0.2809, 0.2535, 0.2713, 0.268, 0.1916, 0.0968, -0.2227, 0.1632, 0.0693, 2.314, 2.2815, 2.2686, 2.2532, 2.24, 2.2262, 2.2258, 2.225, 2.2086, 2.2085, 2.208, 2.1961, 2.1843, 2.1835, 2.183, 2.1577, 2.1568, 2.1479, 2.1473, 2.1466, 2.1466, 2.1465, 2.1462, 2.0938, 2.0931, 2.0929, 2.0926, 2.0925, 2.0922, 2.0912, 2.0564, 2.0253, 2.0478, 1.8912, 1.8464, 1.9596, 1.5211, 1.1663, 1.2876, 1.7007, 1.266, 0.7863, 0.9664, 1.2236, 1.45, 1.2209, 1.509, 0.7176, 0.4754, 0.9258, -0.03, 0.4046, 0.1232, 1.6664, 1.6614, 1.6421, 1.6394, 1.6391, 1.639, 1.6328, 1.6296, 1.6287, 1.6264, 1.622, 1.6189, 1.6188, 1.6183, 1.6181, 1.6139, 1.6049, 1.6, 1.5949, 1.5944, 1.5921, 1.5907, 1.5894, 1.5773, 1.5767, 1.5764, 1.5757, 1.5756, 1.5734, 1.572, 1.5667, 1.5538, 1.5303, 1.5011, 1.423, 1.4824, 1.4037, 1.3807, 1.4542, 1.5216, 1.5217, 1.4365, 1.2623, 1.3559, 1.2334, 1.1281, 1.437, 1.2515, 1.2866, 0.9518, 1.1831, 0.7815, 1.3531, 1.2976, 0.3102, 0.1604, 0.8067, 0.7643, 0.6482, 0.3746]}, "token.table": {"Topic": [2, 1, 1, 1, 1, 1, 2, 3, 1, 2, 2, 2, 2, 1, 1, 3, 1, 3, 3, 3, 2, 3, 1, 2, 1, 2, 3, 1, 2, 3, 1, 2, 3, 2, 3, 1, 2, 3, 2, 3, 3, 3, 1, 2, 3, 3, 2, 3, 2, 3, 2, 2, 1, 2, 3, 1, 2, 1, 2, 3, 2, 3, 3, 3, 1, 2, 3, 3, 2, 3, 1, 3, 1, 1, 2, 3, 1, 2, 3, 2, 3, 2, 3, 1, 2, 3, 3, 1, 2, 3, 1, 3, 3, 1, 2, 3, 1, 3, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 3, 2, 3, 1, 2, 2, 3, 1, 2, 3, 2, 1, 1, 3, 3, 2, 3, 2, 3, 3, 1, 2, 1, 1, 3, 1, 2, 3, 1, 2, 3, 1, 3, 2, 1, 1, 3, 1, 2, 3, 1, 3, 1, 3, 1, 1, 2, 1, 2, 3, 1, 3, 2, 2, 3, 2, 3, 3, 1, 3, 3, 1, 3, 1, 3, 1, 2, 3, 2, 1, 3, 1, 2, 3, 2, 3, 1, 2, 3, 2, 1, 2, 3, 3, 1, 1, 2, 3, 2, 1, 1, 2, 3, 1, 1, 3, 2, 1, 3, 3, 2, 1, 2, 3, 1, 2, 1, 2, 2, 3, 1, 2, 3, 1, 1, 3, 1, 1, 3, 1, 2, 3, 3, 2, 2, 1, 2, 1, 2, 1, 2], "Freq": [0.7040850927562297, 0.9818254223648224, 0.9952665255983738, 0.9906205717557601, 0.9852864857082642, 0.23964658033962066, 0.10650959126205363, 0.6390575475723217, 0.9981965071052779, 0.9258834138403932, 0.7041121073070795, 0.9126320209554177, 0.9165162940009498, 0.990768613288873, 0.9806823488844959, 0.011144117600960181, 0.11765320796775111, 0.8235724557742578, 0.9042323795867087, 0.964492726642698, 0.07059809401274518, 0.9177752221656873, 0.12494034148236527, 0.8745823903765569, 0.5826099692402136, 0.20886017765215206, 0.20886017765215206, 0.46867365325240545, 0.2050447232979274, 0.35150523993930405, 0.34537535383965917, 0.4374754481969016, 0.2302502358931061, 0.3670127536319784, 0.5872204058111654, 0.0885316993360081, 0.6197218953520567, 0.2655950980080243, 0.9084574801071098, 0.9629745014307592, 0.9648396482764607, 0.9627120917489058, 0.25470524835100317, 0.06367631208775079, 0.636763120877508, 0.9626956285517818, 0.15733409142532107, 0.7866704571266053, 0.1764687063825677, 0.7646977276577933, 0.8668496036191529, 0.9005402789557433, 0.04206851774446729, 0.715164801655944, 0.25241110646680376, 0.3942143589112224, 0.6307429742579558, 0.6298716438633708, 0.3149358219316854, 0.07873395548292135, 0.8669000274798598, 0.9659415577790839, 0.8686632294580118, 0.8263395077992194, 0.02441302450338564, 0.34178234304739896, 0.6347386370880267, 0.8257478321310515, 0.6880340716427787, 0.34401703582138937, 0.9930173520207598, 0.008415401288311523, 0.9894083135125116, 0.29523812247797204, 0.036904765309746505, 0.6273810102656906, 0.12633198117327224, 0.1473873113688176, 0.7158812266485426, 0.8408845247919363, 0.12012636068456234, 0.8894964265526164, 0.8257697687038172, 0.9942038132592363, 0.0038535031521675827, 0.0038535031521675827, 0.9628209179420393, 0.9267820448655802, 0.01748645367670906, 0.05245936103012718, 0.9959755406211379, 0.006343793252363936, 0.8680762085357014, 0.7883492524751367, 0.03666740709186682, 0.1650033319134007, 0.9919672214632358, 0.9642184829723349, 0.9633599995165669, 0.08410167305326759, 0.8410167305326759, 0.08410167305326759, 0.18870873734341348, 0.4717718433585337, 0.28306310601512025, 0.13697646509778455, 0.06848823254889227, 0.753370558037815, 0.8261664396229171, 0.7782787675479018, 0.2179180549134125, 0.997806131973157, 0.7680271121667429, 0.7039991881276852, 0.9643304870859873, 0.14098385053480963, 0.29763257335126475, 0.5639354021392385, 0.8670212956985843, 0.9898524463392215, 0.6059916972080059, 0.39211227466400383, 0.8263574060298748, 0.13739996035349372, 0.8243997621209623, 0.09955969388037729, 0.7964775510430183, 0.9634240771556959, 0.9632936293934965, 0.8899153802963465, 0.998093618974828, 0.8789090620724234, 0.12122883614792046, 0.0973442692028033, 0.032448089734267764, 0.8760984228252297, 0.5172147284036978, 0.43101227366974815, 0.08620245473394963, 0.3829135046288034, 0.5956432294225831, 0.7682592467876569, 0.9849727228003642, 0.11762518259481776, 0.8233762781637244, 0.7735014504698673, 0.040710602656308804, 0.1831977119533896, 0.5231851350389805, 0.4708666215350824, 0.7219734608509254, 0.26739757809293535, 0.9919836725951781, 0.9422212237479658, 0.0509308769593495, 0.18519996584362652, 0.09259998292181326, 0.6944998719135994, 0.9771505486982223, 0.012527571137156695, 0.8896410467458966, 0.8668511390765615, 0.964262961250121, 0.31630791290410476, 0.6853338112922269, 0.9887005425936556, 0.9949425098183843, 0.9213089770182046, 0.9651553790073705, 0.9814151405658594, 0.01784391164665199, 0.9955958202903864, 0.931486987416224, 0.15978628327472813, 0.09986642704670509, 0.7589848455549586, 0.7681979857840258, 0.951315260840869, 0.04324160276549405, 0.9788143103794738, 0.21884685229732778, 0.8024384584235352, 0.2513965547042198, 0.7313354318668212, 0.9852551753458282, 0.8041400407856899, 0.9208863545150349, 0.8669455154947796, 0.32173707345661146, 0.21449138230440762, 0.42898276460881524, 0.9217454481693366, 0.9842912286246877, 0.5171549436080929, 0.06464436795101161, 0.4201883916815755, 0.7041545876027758, 0.9636718378660347, 0.05400381373028322, 0.05400381373028322, 0.8640610196845315, 0.9790790677957503, 0.5912163664934457, 0.3941442443289638, 0.7043610125314054, 0.9677783503282171, 0.025467851324426766, 0.9650838833779274, 0.9009392918348478, 0.03370603739835092, 0.7415328227637203, 0.20223622439010555, 0.9531508049333665, 0.04765754024666832, 0.9930123912410345, 0.9008343485373521, 0.6531799382455755, 0.21772664608185852, 0.7964273439877956, 0.1274283750380473, 0.06371418751902365, 0.9630017152286168, 0.28650497754992316, 0.716262443874808, 0.9632731927913231, 0.06800653605178349, 0.8840849686731853, 0.8525480165673354, 0.0437204111060172, 0.0874408222120344, 0.9651249136881231, 0.7042690790790604, 0.8672700077129462, 0.97645452380829, 0.02077562816613383, 0.9739451716175891, 0.022649887712036954, 0.6522430176598284, 0.3261215088299142], "Term": ["1949", "2015", "2018", "2019", "2020", "act", "act", "act", "agent", "appeal", "appellate", "appoint", "appointment", "archive", "artificial", "artificial", "aspect", "aspect", "authority", "boltzmann", "branch", "branch", "building", "building", "canada", "canada", "canada", "canadian", "canadian", "canadian", "case", "case", "case", "charter", "charter", "chief", "chief", "chief", "clerk", "cluster", "commonwealth", "competence", "concern", "concern", "concern", "conflict", "constitution", "constitution", "constitutional", "constitutional", "corporation", "counsel", "court", "court", "court", "day", "day", "decision", "decision", "decision", "dissent", "doctrine", "emergency", "enumerate", "federal", "federal", "federal", "federalism", "french", "french", "game", "game", "goal", "good", "good", "good", "government", "government", "government", "hear", "hear", "hearing", "helmholtz", "hockey", "hockey", "hockey", "hopfield", "human", "human", "human", "ice", "ice", "immunity", "include", "include", "include", "intelligence", "interjurisdictional", "invalid", "judge", "judge", "judge", "judicial", "judicial", "judicial", "jurisdiction", "jurisdiction", "jurisdiction", "jurisprudence", "justice", "justice", "knowledge", "lamer", "laskin", "latent", "law", "law", "law", "lawyer", "league", "learning", "learning", "legislate", "legislation", "legislation", "legislative", "legislative", "limitation", "line", "litigant", "logic", "machine", "machine", "matter", "matter", "matter", "member", "member", "member", "method", "method", "minister", "minute", "moment", "moment", "national", "national", "national", "network", "network", "neural", "neural", "nhl", "october", "october", "order", "order", "order", "original", "original", "override", "panel", "paramountcy", "parliament", "parliament", "peace", "penalty", "phrase", "pith", "play", "play", "player", "pogg", "power", "power", "power", "prime", "problem", "problem", "professional", "province", "province", "provincial", "provincial", "puck", "puisne", "purpose", "recommend", "reference", "reference", "reference", "residual", "retrieve", "right", "right", "right", "scc", "search", "section", "section", "section", "sport", "state", "state", "statue", "stick", "stick", "substance", "superior", "supreme", "supreme", "supreme", "system", "system", "team", "tenure", "territorial", "territorial", "time", "time", "time", "tournament", "training", "training", "university", "unsupervised", "unsupervised", "use", "use", "use", "variable", "wagner", "website", "woman", "woman", "world", "world", "year", "year"]}, "R": 30, "lambda.step": 0.01, "plot.opts": {"xlab": "PC1", "ylab": "PC2"}, "topic.order": [1, 2, 3]};

function LDAvis_load_lib(url, callback){
  var s = document.createElement('script');
  s.src = url;
  s.async = true;
  s.onreadystatechange = s.onload = callback;
  s.onerror = function(){console.warn("failed to load library " + url);};
  document.getElementsByTagName("head")[0].appendChild(s);
}

if(typeof(LDAvis) !== "undefined"){
   // already loaded: just create the visualization
   !function(LDAvis){
       new LDAvis("#" + "ldavis_el7348856078391044998229239", ldavis_el7348856078391044998229239_data);
   }(LDAvis);
}else if(typeof define === "function" && define.amd){
   // require.js is available: use it to load d3/LDAvis
   require.config({paths: {d3: "https://d3js.org/d3.v5"}});
   require(["d3"], function(d3){
      window.d3 = d3;
      LDAvis_load_lib("https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js", function(){
        new LDAvis("#" + "ldavis_el7348856078391044998229239", ldavis_el7348856078391044998229239_data);
      });
    });
}else{
    // require.js not available: dynamically load d3 & LDAvis
    LDAvis_load_lib("https://d3js.org/d3.v5.js", function(){
         LDAvis_load_lib("https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js", function(){
                 new LDAvis("#" + "ldavis_el7348856078391044998229239", ldavis_el7348856078391044998229239_data);
            })
         });
}
</script></div></div>
</div>
</div>
<div class="section" id="optional-topic-modeling-with-sklearn">
<h3>(Optional) Topic modeling with <code class="docutils literal notranslate"><span class="pre">sklearn</span></code><a class="headerlink" href="#optional-topic-modeling-with-sklearn" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>We are using <code class="docutils literal notranslate"><span class="pre">Gensim</span></code> LDA so that we’ll be able to use <code class="docutils literal notranslate"><span class="pre">CoherenceModel</span></code> to evaluate topic model later.</p></li>
<li><p>Bit we can also train an LDA model with <code class="docutils literal notranslate"><span class="pre">sklearn</span></code>.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">CountVectorizer</span>

<span class="n">vec</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">()</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">vec</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">wiki_df</span><span class="p">[</span><span class="s2">&quot;text_pp&quot;</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">LatentDirichletAllocation</span>

<span class="n">n_topics</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">lda</span> <span class="o">=</span> <span class="n">LatentDirichletAllocation</span><span class="p">(</span>
    <span class="n">n_components</span><span class="o">=</span><span class="n">n_topics</span><span class="p">,</span> <span class="n">learning_method</span><span class="o">=</span><span class="s2">&quot;batch&quot;</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span>
<span class="p">)</span>
<span class="n">document_topics</span> <span class="o">=</span> <span class="n">lda</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;lda.components_.shape: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">lda</span><span class="o">.</span><span class="n">components_</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>lda.components_.shape: (3, 4696)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sorting</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">lda</span><span class="o">.</span><span class="n">components_</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)[:,</span> <span class="p">::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="n">feature_names</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">vec</span><span class="o">.</span><span class="n">get_feature_names</span><span class="p">())</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mglearn</span>

<span class="n">mglearn</span><span class="o">.</span><span class="n">tools</span><span class="o">.</span><span class="n">print_topics</span><span class="p">(</span>
    <span class="n">topics</span><span class="o">=</span><span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">),</span>
    <span class="n">feature_names</span><span class="o">=</span><span class="n">feature_names</span><span class="p">,</span>
    <span class="n">sorting</span><span class="o">=</span><span class="n">sorting</span><span class="p">,</span>
    <span class="n">topics_per_chunk</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
    <span class="n">n_words</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>topic 0       topic 1       topic 2       
--------      --------      --------      
hockey        intelligence  court         
ice           artificial    law           
player        original      provincial    
team          retrieve      government    
league        archive       power         
game          machine       federal       
play          human         canada        
puck          10            justice       
penalty       problem       supreme       
canada        doi           case          
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/Users/kvarada/opt/miniconda3/envs/cpsc330/lib/python3.9/site-packages/mglearn/plot_pca.py:7: DeprecationWarning: The &#39;cachedir&#39; parameter has been deprecated in version 0.12 and will be removed in version 0.14.
You provided &quot;cachedir=&#39;cache&#39;&quot;, use &quot;location=&#39;cache&#39;&quot; instead.
  memory = Memory(cachedir=&quot;cache&quot;)
/Users/kvarada/opt/miniconda3/envs/cpsc330/lib/python3.9/site-packages/mglearn/plot_nmf.py:7: DeprecationWarning: The &#39;cachedir&#39; parameter has been deprecated in version 0.12 and will be removed in version 0.14.
You provided &quot;cachedir=&#39;cache&#39;&quot;, use &quot;location=&#39;cache&#39;&quot; instead.
  memory = Memory(cachedir=&quot;cache&quot;)
</pre></div>
</div>
</div>
</div>
<p><br><br><br><br></p>
</div>
</div>
<div class="section" id="basic-text-preprocessing">
<h2>Basic text preprocessing<a class="headerlink" href="#basic-text-preprocessing" title="Permalink to this headline">¶</a></h2>
<div class="section" id="introduction">
<h3>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Why do we need preprocessing?</p>
<ul>
<li><p>Text data is unstructured and messy.</p></li>
<li><p>We need to “normalize” it before we do anything interesting with it.</p></li>
</ul>
</li>
<li><p>Example:</p>
<ul>
<li><p><strong>Lemma</strong>: Same stem, same part-of-speech, roughly the same meaning</p>
<ul>
<li><p>Vancouver’s → Vancouver</p></li>
<li><p>computers → computer</p></li>
<li><p>rising → rise, rose, rises</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
<div class="section" id="tokenization">
<h3>Tokenization<a class="headerlink" href="#tokenization" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Sentence segmentation</p>
<ul>
<li><p>Split text into sentences</p></li>
</ul>
</li>
<li><p>Word tokenization</p>
<ul>
<li><p>Split sentences into words</p></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="tokenization-sentence-segmentation">
<h3>Tokenization: sentence segmentation<a class="headerlink" href="#tokenization-sentence-segmentation" title="Permalink to this headline">¶</a></h3>
<blockquote>
MDS is a Master's program at UBC in British Columbia. MDS teaching team is truly multicultural!! Dr. George did his Ph.D. in Scotland. Dr. Timbers, Dr. Ostblom, Dr. Rodríguez-Arelis, and Dr. Kolhatkar did theirs in Canada. Dr. Gelbart did his PhD in the U.S.
</blockquote>
<ul class="simple">
<li><p>How many sentences are there in this text?</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">### Let&#39;s do sentence segmentation on &quot;.&quot;</span>
<span class="n">text</span> <span class="o">=</span> <span class="p">(</span>
    <span class="s2">&quot;MDS is a Master&#39;s program at UBC in British Columbia. &quot;</span>
    <span class="s2">&quot;MDS teaching team is truly multicultural!! &quot;</span>
    <span class="s2">&quot;Dr. George did his Ph.D. in Scotland. &quot;</span>
    <span class="s2">&quot;Dr. Timbers, Dr. Ostblom, Dr. Rodríguez-Arelis, and Dr. Kolhatkar did theirs in Canada. &quot;</span>
    <span class="s2">&quot;Dr. Gelbart did his PhD in the U.S.&quot;</span>
<span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">text</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&quot;MDS is a Master&#39;s program at UBC in British Columbia&quot;, &#39; MDS teaching team is truly multicultural!! Dr&#39;, &#39; George did his Ph&#39;, &#39;D&#39;, &#39; in Scotland&#39;, &#39; Dr&#39;, &#39; Timbers, Dr&#39;, &#39; Ostblom, Dr&#39;, &#39; Rodríguez-Arelis, and Dr&#39;, &#39; Kolhatkar did theirs in Canada&#39;, &#39; Dr&#39;, &#39; Gelbart did his PhD in the U&#39;, &#39;S&#39;, &#39;&#39;]
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="sentence-segmentation">
<h3>Sentence segmentation<a class="headerlink" href="#sentence-segmentation" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>In English, period (.) is quite ambiguous. (In Chinese, it is unambiguous.)</p>
<ul>
<li><p>Abbreviations like Dr., U.S., Inc.</p></li>
<li><p>Numbers like 60.44%, 0.98</p></li>
</ul>
</li>
<li><p>! and ? are relatively ambiguous.</p></li>
<li><p>How about writing regular expressions?</p></li>
<li><p>A common way is using off-the-shelf models for sentence segmentation.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">### Let&#39;s try to do sentence segmentation using nltk</span>
<span class="kn">from</span> <span class="nn">nltk.tokenize</span> <span class="kn">import</span> <span class="n">sent_tokenize</span>

<span class="n">sent_tokenized</span> <span class="o">=</span> <span class="n">sent_tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sent_tokenized</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&quot;MDS is a Master&#39;s program at UBC in British Columbia.&quot;, &#39;MDS teaching team is truly multicultural!!&#39;, &#39;Dr. George did his Ph.D. in Scotland.&#39;, &#39;Dr. Timbers, Dr. Ostblom, Dr. Rodríguez-Arelis, and Dr. Kolhatkar did theirs in Canada.&#39;, &#39;Dr. Gelbart did his PhD in the U.S.&#39;]
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="word-tokenization">
<h3>Word tokenization<a class="headerlink" href="#word-tokenization" title="Permalink to this headline">¶</a></h3>
<blockquote>
MDS is a Master's program at UBC in British Columbia. 
</blockquote>
<ul class="simple">
<li><p>How many words are there in this sentence?</p></li>
<li><p>Is whitespace a sufficient condition for a word boundary?</p></li>
</ul>
</div>
<div class="section" id="id9">
<h3>Word tokenization<a class="headerlink" href="#id9" title="Permalink to this headline">¶</a></h3>
<blockquote>
MDS is a Master's program at UBC in British Columbia. 
</blockquote>
<ul class="simple">
<li><p>What’s our definition of a word?</p>
<ul>
<li><p>Should British Columbia be one word or two words?</p></li>
<li><p>Should punctuation be considered a separate word?</p></li>
<li><p>What about the punctuations in <code class="docutils literal notranslate"><span class="pre">U.S.</span></code>?</p></li>
<li><p>What do we do with words like <code class="docutils literal notranslate"><span class="pre">Master's</span></code>?</p></li>
</ul>
</li>
<li><p>This process of identifying word boundaries is referred to as <strong>tokenization</strong>.</p></li>
<li><p>You can use regex but better to do it with off-the-shelf ML models.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">### Let&#39;s do word segmentation on white spaces</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Splitting on whitespace: &quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">sent</span><span class="o">.</span><span class="n">split</span><span class="p">()</span> <span class="k">for</span> <span class="n">sent</span> <span class="ow">in</span> <span class="n">sent_tokenized</span><span class="p">])</span>

<span class="c1">### Let&#39;s try to do word segmentation using nltk</span>
<span class="kn">from</span> <span class="nn">nltk.tokenize</span> <span class="kn">import</span> <span class="n">word_tokenize</span>

<span class="n">word_tokenized</span> <span class="o">=</span> <span class="p">[</span><span class="n">word_tokenize</span><span class="p">(</span><span class="n">sent</span><span class="p">)</span> <span class="k">for</span> <span class="n">sent</span> <span class="ow">in</span> <span class="n">sent_tokenized</span><span class="p">]</span>
<span class="c1"># This is similar to the input format of word2vec algorithm</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n\n\n</span><span class="s2">Tokenized: &quot;</span><span class="p">,</span> <span class="n">word_tokenized</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Splitting on whitespace:  [[&#39;MDS&#39;, &#39;is&#39;, &#39;a&#39;, &quot;Master&#39;s&quot;, &#39;program&#39;, &#39;at&#39;, &#39;UBC&#39;, &#39;in&#39;, &#39;British&#39;, &#39;Columbia.&#39;], [&#39;MDS&#39;, &#39;teaching&#39;, &#39;team&#39;, &#39;is&#39;, &#39;truly&#39;, &#39;multicultural!!&#39;], [&#39;Dr.&#39;, &#39;George&#39;, &#39;did&#39;, &#39;his&#39;, &#39;Ph.D.&#39;, &#39;in&#39;, &#39;Scotland.&#39;], [&#39;Dr.&#39;, &#39;Timbers,&#39;, &#39;Dr.&#39;, &#39;Ostblom,&#39;, &#39;Dr.&#39;, &#39;Rodríguez-Arelis,&#39;, &#39;and&#39;, &#39;Dr.&#39;, &#39;Kolhatkar&#39;, &#39;did&#39;, &#39;theirs&#39;, &#39;in&#39;, &#39;Canada.&#39;], [&#39;Dr.&#39;, &#39;Gelbart&#39;, &#39;did&#39;, &#39;his&#39;, &#39;PhD&#39;, &#39;in&#39;, &#39;the&#39;, &#39;U.S.&#39;]]



Tokenized:  [[&#39;MDS&#39;, &#39;is&#39;, &#39;a&#39;, &#39;Master&#39;, &quot;&#39;s&quot;, &#39;program&#39;, &#39;at&#39;, &#39;UBC&#39;, &#39;in&#39;, &#39;British&#39;, &#39;Columbia&#39;, &#39;.&#39;], [&#39;MDS&#39;, &#39;teaching&#39;, &#39;team&#39;, &#39;is&#39;, &#39;truly&#39;, &#39;multicultural&#39;, &#39;!&#39;, &#39;!&#39;], [&#39;Dr.&#39;, &#39;George&#39;, &#39;did&#39;, &#39;his&#39;, &#39;Ph.D.&#39;, &#39;in&#39;, &#39;Scotland&#39;, &#39;.&#39;], [&#39;Dr.&#39;, &#39;Timbers&#39;, &#39;,&#39;, &#39;Dr.&#39;, &#39;Ostblom&#39;, &#39;,&#39;, &#39;Dr.&#39;, &#39;Rodríguez-Arelis&#39;, &#39;,&#39;, &#39;and&#39;, &#39;Dr.&#39;, &#39;Kolhatkar&#39;, &#39;did&#39;, &#39;theirs&#39;, &#39;in&#39;, &#39;Canada&#39;, &#39;.&#39;], [&#39;Dr.&#39;, &#39;Gelbart&#39;, &#39;did&#39;, &#39;his&#39;, &#39;PhD&#39;, &#39;in&#39;, &#39;the&#39;, &#39;U.S&#39;, &#39;.&#39;]]
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="word-segmentation">
<h3>Word segmentation<a class="headerlink" href="#word-segmentation" title="Permalink to this headline">¶</a></h3>
<p>For some languages you need much more sophisticated tokenizers.</p>
<ul class="simple">
<li><p>For languages such as Chinese, there are no spaces between words.</p>
<ul>
<li><p><a class="reference external" href="https://github.com/fxsjy/jieba">jieba</a> is a popular tokenizer for Chinese.</p></li>
</ul>
</li>
<li><p>German doesn’t separate compound words.</p>
<ul>
<li><p>Example: <em>rindfleischetikettierungsüberwachungsaufgabenübertragungsgesetz</em></p></li>
<li><p>(the law for the delegation of monitoring beef labeling)</p></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="types-and-tokens">
<h3>Types and tokens<a class="headerlink" href="#types-and-tokens" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Usually in NLP, we talk about</p>
<ul>
<li><p><strong>Type</strong> an element in the vocabulary</p></li>
<li><p><strong>Token</strong> an instance of that type in running text</p></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="exercise-for-you">
<h3>Exercise for you<a class="headerlink" href="#exercise-for-you" title="Permalink to this headline">¶</a></h3>
<blockquote>    
UBC is located in the beautiful province of British Columbia. It's very close 
to the U.S. border. You'll get to the USA border in about 45 mins by car.     
</blockquote>  
<ul class="simple">
<li><p>Consider the example above.</p>
<ul>
<li><p>How many types? (task dependent)</p></li>
<li><p>How many tokens?</p></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="other-commonly-used-preprocessing-steps">
<h3>Other commonly used preprocessing steps<a class="headerlink" href="#other-commonly-used-preprocessing-steps" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Punctuation and stopword removal</p></li>
<li><p>Stemming and lemmatization</p></li>
</ul>
</div>
<div class="section" id="punctuation-and-stopword-removal">
<h3>Punctuation and stopword removal<a class="headerlink" href="#punctuation-and-stopword-removal" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>The most frequently occurring words in English are not very useful in many NLP tasks.</p>
<ul>
<li><p>Example: <em>the</em> , <em>is</em> , <em>a</em> , and punctuation</p></li>
</ul>
</li>
<li><p>Probably not very informative in many tasks</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Let&#39;s use `nltk.stopwords`.</span>
<span class="c1"># Add punctuations to the list.</span>
<span class="n">stop_words</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">stopwords</span><span class="o">.</span><span class="n">words</span><span class="p">(</span><span class="s2">&quot;english&quot;</span><span class="p">)))</span>
<span class="kn">import</span> <span class="nn">string</span>

<span class="n">punctuation</span> <span class="o">=</span> <span class="n">string</span><span class="o">.</span><span class="n">punctuation</span>
<span class="n">stop_words</span> <span class="o">+=</span> <span class="nb">list</span><span class="p">(</span><span class="n">punctuation</span><span class="p">)</span>
<span class="c1"># stop_words.extend([&#39;``&#39;,&#39;`&#39;,&#39;br&#39;,&#39;&quot;&#39;,&quot;”&quot;, &quot;&#39;&#39;&quot;, &quot;&#39;s&quot;])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">stop_words</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;than&#39;, &#39;these&#39;, &#39;didn&#39;, &#39;on&#39;, &#39;ours&#39;, &#39;most&#39;, &#39;they&#39;, &#39;more&#39;, &quot;it&#39;s&quot;, &#39;couldn&#39;, &#39;this&#39;, &#39;does&#39;, &#39;re&#39;, &#39;their&#39;, &#39;under&#39;, &#39;just&#39;, &#39;don&#39;, &#39;you&#39;, &#39;not&#39;, &quot;needn&#39;t&quot;, &#39;wasn&#39;, &#39;will&#39;, &#39;of&#39;, &#39;him&#39;, &quot;hadn&#39;t&quot;, &#39;ll&#39;, &#39;shouldn&#39;, &#39;own&#39;, &#39;with&#39;, &#39;she&#39;, &#39;was&#39;, &quot;you&#39;re&quot;, &#39;through&#39;, &#39;here&#39;, &quot;mustn&#39;t&quot;, &#39;when&#39;, &#39;or&#39;, &#39;as&#39;, &#39;further&#39;, &#39;very&#39;, &#39;hasn&#39;, &#39;what&#39;, &#39;but&#39;, &#39;me&#39;, &#39;we&#39;, &#39;should&#39;, &#39;mustn&#39;, &#39;such&#39;, &quot;aren&#39;t&quot;, &#39;same&#39;, &quot;weren&#39;t&quot;, &#39;then&#39;, &#39;so&#39;, &quot;don&#39;t&quot;, &quot;you&#39;d&quot;, &#39;both&#39;, &#39;myself&#39;, &#39;my&#39;, &#39;s&#39;, &#39;an&#39;, &#39;them&#39;, &#39;have&#39;, &quot;wouldn&#39;t&quot;, &quot;you&#39;ll&quot;, &#39;below&#39;, &#39;in&#39;, &#39;up&#39;, &#39;having&#39;, &quot;you&#39;ve&quot;, &#39;nor&#39;, &quot;didn&#39;t&quot;, &#39;had&#39;, &#39;off&#39;, &quot;should&#39;ve&quot;, &#39;until&#39;, &quot;couldn&#39;t&quot;, &#39;been&#39;, &#39;his&#39;, &#39;themselves&#39;, &quot;hasn&#39;t&quot;, &#39;while&#39;, &#39;down&#39;, &#39;are&#39;, &#39;be&#39;, &#39;a&#39;, &#39;o&#39;, &#39;any&#39;, &#39;all&#39;, &quot;doesn&#39;t&quot;, &#39;because&#39;, &#39;about&#39;, &#39;against&#39;, &#39;again&#39;, &quot;that&#39;ll&quot;, &#39;doing&#39;, &#39;to&#39;, &#39;won&#39;, &quot;won&#39;t&quot;, &#39;d&#39;, &#39;herself&#39;, &quot;mightn&#39;t&quot;, &#39;for&#39;, &#39;shan&#39;, &#39;doesn&#39;, &#39;by&#39;, &#39;m&#39;, &#39;wouldn&#39;, &#39;those&#39;, &#39;at&#39;, &#39;needn&#39;, &#39;yourselves&#39;, &#39;over&#39;, &#39;himself&#39;, &#39;too&#39;, &#39;it&#39;, &#39;why&#39;, &#39;yourself&#39;, &#39;is&#39;, &quot;shan&#39;t&quot;, &#39;ve&#39;, &#39;and&#39;, &#39;there&#39;, &#39;out&#39;, &#39;during&#39;, &#39;aren&#39;, &quot;wasn&#39;t&quot;, &#39;some&#39;, &#39;the&#39;, &#39;your&#39;, &#39;can&#39;, &quot;shouldn&#39;t&quot;, &#39;where&#39;, &#39;no&#39;, &#39;other&#39;, &#39;into&#39;, &#39;he&#39;, &#39;whom&#39;, &#39;weren&#39;, &quot;isn&#39;t&quot;, &#39;isn&#39;, &#39;who&#39;, &#39;yours&#39;, &#39;hadn&#39;, &#39;once&#39;, &#39;hers&#39;, &#39;y&#39;, &#39;how&#39;, &#39;ourselves&#39;, &#39;that&#39;, &#39;haven&#39;, &#39;which&#39;, &#39;were&#39;, &#39;each&#39;, &#39;being&#39;, &#39;now&#39;, &#39;i&#39;, &#39;ain&#39;, &#39;above&#39;, &#39;ma&#39;, &#39;our&#39;, &#39;did&#39;, &#39;am&#39;, &#39;after&#39;, &#39;do&#39;, &#39;has&#39;, &#39;theirs&#39;, &#39;if&#39;, &quot;haven&#39;t&quot;, &#39;few&#39;, &#39;her&#39;, &#39;t&#39;, &quot;she&#39;s&quot;, &#39;itself&#39;, &#39;mightn&#39;, &#39;its&#39;, &#39;between&#39;, &#39;from&#39;, &#39;only&#39;, &#39;before&#39;, &#39;!&#39;, &#39;&quot;&#39;, &#39;#&#39;, &#39;$&#39;, &#39;%&#39;, &#39;&amp;&#39;, &quot;&#39;&quot;, &#39;(&#39;, &#39;)&#39;, &#39;*&#39;, &#39;+&#39;, &#39;,&#39;, &#39;-&#39;, &#39;.&#39;, &#39;/&#39;, &#39;:&#39;, &#39;;&#39;, &#39;&lt;&#39;, &#39;=&#39;, &#39;&gt;&#39;, &#39;?&#39;, &#39;@&#39;, &#39;[&#39;, &#39;\\&#39;, &#39;]&#39;, &#39;^&#39;, &#39;_&#39;, &#39;`&#39;, &#39;{&#39;, &#39;|&#39;, &#39;}&#39;, &#39;~&#39;]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">### Get rid of stop words</span>
<span class="n">preprocessed</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">sent</span> <span class="ow">in</span> <span class="n">word_tokenized</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">sent</span><span class="p">:</span>
        <span class="n">token</span> <span class="o">=</span> <span class="n">token</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">token</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">stop_words</span><span class="p">:</span>
            <span class="n">preprocessed</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">token</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">preprocessed</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;mds&#39;, &#39;master&#39;, &quot;&#39;s&quot;, &#39;program&#39;, &#39;ubc&#39;, &#39;british&#39;, &#39;columbia&#39;, &#39;mds&#39;, &#39;teaching&#39;, &#39;team&#39;, &#39;truly&#39;, &#39;multicultural&#39;, &#39;dr.&#39;, &#39;george&#39;, &#39;ph.d.&#39;, &#39;scotland&#39;, &#39;dr.&#39;, &#39;timbers&#39;, &#39;dr.&#39;, &#39;ostblom&#39;, &#39;dr.&#39;, &#39;rodríguez-arelis&#39;, &#39;dr.&#39;, &#39;kolhatkar&#39;, &#39;canada&#39;, &#39;dr.&#39;, &#39;gelbart&#39;, &#39;phd&#39;, &#39;u.s&#39;]
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="lemmatization">
<h3>Lemmatization<a class="headerlink" href="#lemmatization" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>For many NLP tasks (e.g., web search) we want to ignore morphological differences between words</p>
<ul>
<li><p>Example: If your search term is “studying for ML quiz” you might want to include pages containing “tips to study for an ML quiz” or “here is how I studied for my ML quiz”</p></li>
</ul>
</li>
<li><p>Lemmatization converts inflected forms into the base form.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">nltk</span>

<span class="n">nltk</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="s2">&quot;wordnet&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[nltk_data] Downloading package wordnet to /Users/kvarada/nltk_data...
[nltk_data]   Package wordnet is already up-to-date!
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>True
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># nltk has a lemmatizer</span>
<span class="kn">from</span> <span class="nn">nltk.stem</span> <span class="kn">import</span> <span class="n">WordNetLemmatizer</span>

<span class="n">lemmatizer</span> <span class="o">=</span> <span class="n">WordNetLemmatizer</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Lemma of studying: &quot;</span><span class="p">,</span> <span class="n">lemmatizer</span><span class="o">.</span><span class="n">lemmatize</span><span class="p">(</span><span class="s2">&quot;studying&quot;</span><span class="p">,</span> <span class="s2">&quot;v&quot;</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Lemma of studied: &quot;</span><span class="p">,</span> <span class="n">lemmatizer</span><span class="o">.</span><span class="n">lemmatize</span><span class="p">(</span><span class="s2">&quot;studied&quot;</span><span class="p">,</span> <span class="s2">&quot;v&quot;</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Lemma of studying:  study
Lemma of studied:  study
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="stemming">
<h3>Stemming<a class="headerlink" href="#stemming" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Has a similar purpose but it is a crude chopping of affixes</p>
<ul>
<li><p><em>automates, automatic, automation</em> all reduced to <em>automat</em>.</p></li>
</ul>
</li>
<li><p>Usually these reduced forms (stems) are not actual words themselves.</p></li>
<li><p>A popular stemming algorithm for English is PorterStemmer.</p></li>
<li><p>Beware that it can be aggressive sometimes.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">nltk.stem.porter</span> <span class="kn">import</span> <span class="n">PorterStemmer</span>

<span class="n">text</span> <span class="o">=</span> <span class="p">(</span>
    <span class="s2">&quot;UBC is located in the beautiful province of British Columbia... &quot;</span>
    <span class="s2">&quot;It&#39;s very close to the U.S. border.&quot;</span>
<span class="p">)</span>
<span class="n">ps</span> <span class="o">=</span> <span class="n">PorterStemmer</span><span class="p">()</span>
<span class="n">tokenized</span> <span class="o">=</span> <span class="n">word_tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
<span class="n">stemmed</span> <span class="o">=</span> <span class="p">[</span><span class="n">ps</span><span class="o">.</span><span class="n">stem</span><span class="p">(</span><span class="n">token</span><span class="p">)</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">tokenized</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Before stemming: &quot;</span><span class="p">,</span> <span class="n">text</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n\n</span><span class="s2">After stemming: &quot;</span><span class="p">,</span> <span class="s2">&quot; &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">stemmed</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Before stemming:  UBC is located in the beautiful province of British Columbia... It&#39;s very close to the U.S. border.


After stemming:  ubc is locat in the beauti provinc of british columbia ... It &#39;s veri close to the u.s. border .
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="other-tools-for-preprocessing">
<h3>Other tools for preprocessing<a class="headerlink" href="#other-tools-for-preprocessing" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>We used <a class="reference external" href="https://www.nltk.org/">Natural Language Processing Toolkit (nltk)</a> above</p></li>
<li><p>Many available tools</p></li>
<li><p><a class="reference external" href="https://spacy.io/">spaCy</a></p></li>
</ul>
</div>
<div class="section" id="spacy">
<h3><a class="reference external" href="https://spacy.io/">spaCy</a><a class="headerlink" href="#spacy" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Industrial strength NLP library.</p></li>
<li><p>Lightweight, fast, and convenient to use.</p></li>
<li><p>spaCy does many things that we did above in one line of code!</p></li>
<li><p>Also has <a class="reference external" href="https://spacy.io/models/xx">multi-lingual</a> support.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">spacy</span>

<span class="c1"># Load the model</span>
<span class="n">nlp</span> <span class="o">=</span> <span class="n">spacy</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;en_core_web_md&quot;</span><span class="p">)</span>
<span class="n">text</span> <span class="o">=</span> <span class="p">(</span>
    <span class="s2">&quot;MDS is a Master&#39;s program at UBC in British Columbia. &quot;</span>
    <span class="s2">&quot;MDS teaching team is truly multicultural!! &quot;</span>
    <span class="s2">&quot;Dr. George did his Ph.D. in Scotland. &quot;</span>
    <span class="s2">&quot;Dr. Timbers, Dr. Ostblom, Dr. Rodríguez-Arelis, and Dr. Kolhatkar did theirs in Canada. &quot;</span>
    <span class="s2">&quot;Dr. Gelbart did his PhD in the U.S.&quot;</span>
<span class="p">)</span>

<span class="n">doc</span> <span class="o">=</span> <span class="n">nlp</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Accessing tokens</span>
<span class="n">tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">token</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">doc</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Tokens: &quot;</span><span class="p">,</span> <span class="n">tokens</span><span class="p">)</span>

<span class="c1"># Accessing lemma</span>
<span class="n">lemmas</span> <span class="o">=</span> <span class="p">[</span><span class="n">token</span><span class="o">.</span><span class="n">lemma_</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">doc</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Lemmas: &quot;</span><span class="p">,</span> <span class="n">lemmas</span><span class="p">)</span>

<span class="c1"># Accessing pos</span>
<span class="n">pos</span> <span class="o">=</span> <span class="p">[</span><span class="n">token</span><span class="o">.</span><span class="n">pos_</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">doc</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">POS: &quot;</span><span class="p">,</span> <span class="n">pos</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Tokens:  [MDS, is, a, Master, &#39;s, program, at, UBC, in, British, Columbia, ., MDS, teaching, team, is, truly, multicultural, !, !, Dr., George, did, his, Ph.D., in, Scotland, ., Dr., Timbers, ,, Dr., Ostblom, ,, Dr., Rodríguez, -, Arelis, ,, and, Dr., Kolhatkar, did, theirs, in, Canada, ., Dr., Gelbart, did, his, PhD, in, the, U.S.]

Lemmas:  [&#39;MDS&#39;, &#39;be&#39;, &#39;a&#39;, &#39;Master&#39;, &quot;&#39;s&quot;, &#39;program&#39;, &#39;at&#39;, &#39;UBC&#39;, &#39;in&#39;, &#39;British&#39;, &#39;Columbia&#39;, &#39;.&#39;, &#39;MDS&#39;, &#39;teaching&#39;, &#39;team&#39;, &#39;be&#39;, &#39;truly&#39;, &#39;multicultural&#39;, &#39;!&#39;, &#39;!&#39;, &#39;Dr.&#39;, &#39;George&#39;, &#39;do&#39;, &#39;his&#39;, &#39;ph.d.&#39;, &#39;in&#39;, &#39;Scotland&#39;, &#39;.&#39;, &#39;Dr.&#39;, &#39;Timbers&#39;, &#39;,&#39;, &#39;Dr.&#39;, &#39;Ostblom&#39;, &#39;,&#39;, &#39;Dr.&#39;, &#39;Rodríguez&#39;, &#39;-&#39;, &#39;Arelis&#39;, &#39;,&#39;, &#39;and&#39;, &#39;Dr.&#39;, &#39;Kolhatkar&#39;, &#39;do&#39;, &#39;theirs&#39;, &#39;in&#39;, &#39;Canada&#39;, &#39;.&#39;, &#39;Dr.&#39;, &#39;Gelbart&#39;, &#39;do&#39;, &#39;his&#39;, &#39;phd&#39;, &#39;in&#39;, &#39;the&#39;, &#39;U.S.&#39;]

POS:  [&#39;PROPN&#39;, &#39;AUX&#39;, &#39;DET&#39;, &#39;PROPN&#39;, &#39;PART&#39;, &#39;NOUN&#39;, &#39;ADP&#39;, &#39;PROPN&#39;, &#39;ADP&#39;, &#39;PROPN&#39;, &#39;PROPN&#39;, &#39;PUNCT&#39;, &#39;PROPN&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;AUX&#39;, &#39;ADV&#39;, &#39;ADJ&#39;, &#39;PUNCT&#39;, &#39;PUNCT&#39;, &#39;PROPN&#39;, &#39;PROPN&#39;, &#39;VERB&#39;, &#39;PRON&#39;, &#39;NOUN&#39;, &#39;ADP&#39;, &#39;PROPN&#39;, &#39;PUNCT&#39;, &#39;PROPN&#39;, &#39;PROPN&#39;, &#39;PUNCT&#39;, &#39;PROPN&#39;, &#39;PROPN&#39;, &#39;PUNCT&#39;, &#39;PROPN&#39;, &#39;PROPN&#39;, &#39;PUNCT&#39;, &#39;PROPN&#39;, &#39;PUNCT&#39;, &#39;CCONJ&#39;, &#39;PROPN&#39;, &#39;PROPN&#39;, &#39;VERB&#39;, &#39;PRON&#39;, &#39;ADP&#39;, &#39;PROPN&#39;, &#39;PUNCT&#39;, &#39;PROPN&#39;, &#39;PROPN&#39;, &#39;VERB&#39;, &#39;PRON&#39;, &#39;NOUN&#39;, &#39;ADP&#39;, &#39;DET&#39;, &#39;PROPN&#39;]
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="other-typical-nlp-tasks">
<h3>Other typical NLP tasks<a class="headerlink" href="#other-typical-nlp-tasks" title="Permalink to this headline">¶</a></h3>
<p>In order to understand text, we usually are interested in extracting information from text. Some common tasks in NLP pipeline are:</p>
<ul class="simple">
<li><p>Part of speech tagging</p>
<ul>
<li><p>Assigning part-of-speech tags to all words in a sentence.</p></li>
</ul>
</li>
<li><p>Named entity recognition</p>
<ul>
<li><p>Labelling named “real-world” objects, like persons, companies or locations.</p></li>
</ul>
</li>
<li><p>Coreference resolution</p>
<ul>
<li><p>Deciding whether two strings (e.g., UBC vs University of British Columbia) refer to the same entity</p></li>
</ul>
</li>
<li><p>Dependency parsing</p>
<ul>
<li><p>Representing grammatical structure of a sentence</p></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="extracting-named-entities-using-spacy">
<h3>Extracting named-entities using spaCy<a class="headerlink" href="#extracting-named-entities-using-spacy" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">spacy</span> <span class="kn">import</span> <span class="n">displacy</span>

<span class="n">doc</span> <span class="o">=</span> <span class="n">nlp</span><span class="p">(</span>
    <span class="s2">&quot;University of British Columbia &quot;</span>
    <span class="s2">&quot;is located in the beautiful &quot;</span>
    <span class="s2">&quot;province of British Columbia.&quot;</span>
<span class="p">)</span>
<span class="n">displacy</span><span class="o">.</span><span class="n">render</span><span class="p">(</span><span class="n">doc</span><span class="p">,</span> <span class="n">style</span><span class="o">=</span><span class="s2">&quot;ent&quot;</span><span class="p">)</span>
<span class="c1"># Text and label of named entity span</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Named entities:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="p">[(</span><span class="n">ent</span><span class="o">.</span><span class="n">text</span><span class="p">,</span> <span class="n">ent</span><span class="o">.</span><span class="n">label_</span><span class="p">)</span> <span class="k">for</span> <span class="n">ent</span> <span class="ow">in</span> <span class="n">doc</span><span class="o">.</span><span class="n">ents</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">ORG means: &quot;</span><span class="p">,</span> <span class="n">spacy</span><span class="o">.</span><span class="n">explain</span><span class="p">(</span><span class="s2">&quot;ORG&quot;</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;GPE means: &quot;</span><span class="p">,</span> <span class="n">spacy</span><span class="o">.</span><span class="n">explain</span><span class="p">(</span><span class="s2">&quot;GPE&quot;</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><span class="tex2jax_ignore"><div class="entities" style="line-height: 2.5; direction: ltr">
<mark class="entity" style="background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    University of British Columbia
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">ORG</span>
</mark>
 is located in the beautiful province of 
<mark class="entity" style="background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    British Columbia
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">GPE</span>
</mark>
.</div></span></div><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Named entities:
 [(&#39;University of British Columbia&#39;, &#39;ORG&#39;), (&#39;British Columbia&#39;, &#39;GPE&#39;)]

ORG means:  Companies, agencies, institutions, etc.
GPE means:  Countries, cities, states
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="dependency-parsing-using-spacy">
<h3>Dependency parsing using spaCy<a class="headerlink" href="#dependency-parsing-using-spacy" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">doc</span> <span class="o">=</span> <span class="n">nlp</span><span class="p">(</span><span class="s2">&quot;I like cats&quot;</span><span class="p">)</span>
<span class="n">displacy</span><span class="o">.</span><span class="n">render</span><span class="p">(</span><span class="n">doc</span><span class="p">,</span> <span class="n">style</span><span class="o">=</span><span class="s2">&quot;dep&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><span class="tex2jax_ignore"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" xml:lang="en" id="b85896da282b428c9522d2619ae1a70a-0" class="displacy" width="575" height="224.5" direction="ltr" style="max-width: none; height: 224.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr">
<text class="displacy-token" fill="currentColor" text-anchor="middle" y="134.5">
    <tspan class="displacy-word" fill="currentColor" x="50">I</tspan>
    <tspan class="displacy-tag" dy="2em" fill="currentColor" x="50">PRON</tspan>
</text>

<text class="displacy-token" fill="currentColor" text-anchor="middle" y="134.5">
    <tspan class="displacy-word" fill="currentColor" x="225">like</tspan>
    <tspan class="displacy-tag" dy="2em" fill="currentColor" x="225">VERB</tspan>
</text>

<text class="displacy-token" fill="currentColor" text-anchor="middle" y="134.5">
    <tspan class="displacy-word" fill="currentColor" x="400">cats</tspan>
    <tspan class="displacy-tag" dy="2em" fill="currentColor" x="400">NOUN</tspan>
</text>

<g class="displacy-arrow">
    <path class="displacy-arc" id="arrow-b85896da282b428c9522d2619ae1a70a-0-0" stroke-width="2px" d="M70,89.5 C70,2.0 225.0,2.0 225.0,89.5" fill="none" stroke="currentColor"/>
    <text dy="1.25em" style="font-size: 0.8em; letter-spacing: 1px">
        <textPath xlink:href="#arrow-b85896da282b428c9522d2619ae1a70a-0-0" class="displacy-label" startOffset="50%" side="left" fill="currentColor" text-anchor="middle">nsubj</textPath>
    </text>
    <path class="displacy-arrowhead" d="M70,91.5 L62,79.5 78,79.5" fill="currentColor"/>
</g>

<g class="displacy-arrow">
    <path class="displacy-arc" id="arrow-b85896da282b428c9522d2619ae1a70a-0-1" stroke-width="2px" d="M245,89.5 C245,2.0 400.0,2.0 400.0,89.5" fill="none" stroke="currentColor"/>
    <text dy="1.25em" style="font-size: 0.8em; letter-spacing: 1px">
        <textPath xlink:href="#arrow-b85896da282b428c9522d2619ae1a70a-0-1" class="displacy-label" startOffset="50%" side="left" fill="currentColor" text-anchor="middle">dobj</textPath>
    </text>
    <path class="displacy-arrowhead" d="M400.0,91.5 L408.0,79.5 392.0,79.5" fill="currentColor"/>
</g>
</svg></span></div></div>
</div>
</div>
<div class="section" id="many-other-things-possible">
<h3>Many other things possible<a class="headerlink" href="#many-other-things-possible" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>A powerful tool</p></li>
<li><p>All my Capstone groups last year used this tool.</p></li>
<li><p>You can build your own rule-based searches.</p></li>
<li><p>You can also access word vectors using spaCy with bigger models. (Currently we are using <code class="docutils literal notranslate"><span class="pre">en_core_web_md</span></code> model.)</p></li>
</ul>
<p><br><br><br><br></p>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "conda-env-cpsc330-py"
        },
        kernelOptions: {
            kernelName: "conda-env-cpsc330-py",
            path: "./lectures"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'conda-env-cpsc330-py'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="15_recommender-systems.html" title="previous page">Lecture 15: DBSCAN and Recommender Systems</a>
    <a class='right-next' id="next-link" href="17_intro_to_computer-vision.html" title="next page">Lecture 17: Multi-class classification and introduction to computer vision</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Varada Kolhatkar<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>